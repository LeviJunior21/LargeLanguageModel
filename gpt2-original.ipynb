{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Modelos de Linguagem de Larga Escala\n\n## GPT2 - Original\n\n### Estudantes\n- Levi de Lima Pereira J√∫nior: 121210472\n- Lucas de Sousa Pereira: 121210538\n- Matheus Hensley de Figueiredo e Silva: 120210164","metadata":{"id":"s6QO2NXHUJOV"}},{"cell_type":"markdown","source":"# **Bibliotecas üìö**","metadata":{"id":"f-VRH0oEIrDZ"}},{"cell_type":"code","source":"import torch, time\nimport tiktoken\nimport urllib.request\nfrom torch.utils.data import Dataset, DataLoader\nimport wandb\n\n# Importa√ß√µes necess√°rias para criar uma API para coletar dados do Project Gutemberg\nimport os, csv, time, json, re, sys\nfrom datetime import datetime\nimport requests\nfrom urllib.parse import urlencode\n\n# Plotagem de Gr√°ficos\nimport matplotlib.pyplot as plt\nimport numpy as np","metadata":{"id":"5k-pxQP2ItV5","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T02:11:09.872815Z","iopub.execute_input":"2025-09-03T02:11:09.872987Z","iopub.status.idle":"2025-09-03T02:11:16.089733Z","shell.execute_reply.started":"2025-09-03T02:11:09.872970Z","shell.execute_reply":"2025-09-03T02:11:16.089166Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# **Introdu√ß√£o ‚≠ê**\n\nModelos de linguagem baseados em arquiteturas de Transformers revolucionaram o campo de Processamento de Linguagem Natural (PLN), permitindo avan√ßos expressivos em tarefas como tradu√ß√£o autom√°tica, sumariza√ß√£o, resposta a perguntas e gera√ß√£o de texto. Entre esses modelos, o GPT-2 (Generative Pretrained Transformer 2), proposto pela OpenAI em 2019, destacou-se como um dos primeiros a demonstrar a capacidade de gerar texto coerente e contextualizado em larga escala.\n\nA arquitetura do GPT-2 √© composta por blocos de transformers decoders, que utilizam mecanismos de aten√ß√£o para modelar depend√™ncias de longo alcance em sequ√™ncias textuais. Dois elementos fundamentais para o funcionamento desse tipo de modelo s√£o os embeddings posicionais, que permitem incorporar a no√ß√£o de ordem das palavras, e o mecanismo de aten√ß√£o multi-cabe√ßas (Multi-Head Attention), respons√°vel por capturar rela√ß√µes contextuais em diferentes subespa√ßos de representa√ß√£o.\n\nNos √∫ltimos anos, diversas varia√ß√µes da arquitetura original foram propostas, buscando melhorar a efici√™ncia computacional e a qualidade do modelo. Uma dessas varia√ß√µes √© o Grouped-Query Attention (GQA), que reduz o custo da aten√ß√£o agrupando consultas (queries), mantendo boa capacidade representacional ao mesmo tempo em que economiza mem√≥ria e acelera a infer√™ncia.\n\nEste trabalho tem como objetivo implementar, treinar e avaliar um modelo inspirado no GPT-2 do zero, utilizando um conjunto de textos em portugu√™s de dom√≠nio p√∫blico. Al√©m da implementa√ß√£o do baseline, exploramos modifica√ß√µes arquiteturais, em particular:\n\nO uso do embedding posicional da arquitetura original do GPT-2;\n\nA implementa√ß√£o de Grouped-Query Attention (GQA).\n\nA avalia√ß√£o do modelo ser√° realizada por meio de m√©tricas quantitativas, como a perplexidade no conjunto de teste, al√©m da an√°lise de desempenho (tokens/s e consumo de mem√≥ria). Tamb√©m conduziremos uma an√°lise qualitativa das gera√ß√µes de texto, comparando o baseline com as variantes propostas.\n\nCom isso, buscamos n√£o apenas reproduzir parte do funcionamento do GPT-2, mas tamb√©m compreender o impacto de modifica√ß√µes arquiteturais em termos de qualidade, efici√™ncia e capacidade de generaliza√ß√£o em l√≠ngua portuguesa.","metadata":{"id":"7yUYLMlwHqFi"}},{"cell_type":"markdown","source":"# **Dados üé≤**","metadata":{"id":"gOSJSbHCAdtS"}},{"cell_type":"code","source":"# Configura√ß√µes\nOUTPUT_DIR = \"data/ptbr\"\nMANIFEST = os.path.join(OUTPUT_DIR, \"manifest.csv\")\nCONCAT_FILE = os.path.join(OUTPUT_DIR, \"subset.txt\")\nMAX_BOOKS = 120\nTARGET_COUNT = 100\nSLEEP = 0.5","metadata":{"id":"DVfI_UzLh39_","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T02:11:16.090432Z","iopub.execute_input":"2025-09-03T02:11:16.090696Z","iopub.status.idle":"2025-09-03T02:11:16.095221Z","shell.execute_reply.started":"2025-09-03T02:11:16.090668Z","shell.execute_reply":"2025-09-03T02:11:16.094391Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Lista de autores br\nAUTORES_BRASILEIROS =  [\n    \"Machado de Assis\",\n    \"Raul Pomp√©ia\"\n]","metadata":{"id":"1G1ld5hXiCab","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T02:11:16.096257Z","iopub.execute_input":"2025-09-03T02:11:16.096730Z","iopub.status.idle":"2025-09-03T02:11:16.114024Z","shell.execute_reply.started":"2025-09-03T02:11:16.096698Z","shell.execute_reply":"2025-09-03T02:11:16.113371Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Fun√ß√µes para auxiliar na coleta de livros\ndef norm_tokens(name: str):\n    import unicodedata\n    name = unicodedata.normalize(\"NFKD\", name)\n    name = \"\".join([c for c in name if not unicodedata.combining(c)])\n    name = re.sub(r\"[^a-zA-Z\\s]\", \" \", name).lower()\n    tokens = [t for t in name.split() if t not in {\"de\",\"da\",\"do\",\"dos\",\"das\"}]\n    return set(tokens)\n\ndef is_brazilian_author(name: str) -> bool:\n    name_tokens = norm_tokens(name)\n    for a in AUTORES_BRASILEIROS:\n        if norm_tokens(a).issubset(name_tokens):\n            return True\n    return False\n\ndef choose_txt_url(formats: dict) -> str | None:\n    # Escolhe link de texto\n    prefs = [\n        \"text/plain; charset=utf-8\",\n        \"text/plain; charset=us-ascii\",\n        \"text/plain\",\n    ]\n    for k in prefs:\n        if k in formats:\n            return formats[k]\n    return None\n\ndef gutendex_search(query_params: dict) -> dict:\n    url = \"https://gutendex.com/books?\" + urlencode(query_params)\n    r = requests.get(url, timeout=30)\n    r.raise_for_status()\n    return r.json()\n\ndef fetch_books_by_author(author_name: str, hard_limit=200) -> list[dict]:\n    results = []\n    page = 1\n    while True:\n        params = {\n            \"search\": author_name,\n            \"languages\": \"pt\",\n            \"page\": page,\n        }\n        data = gutendex_search(params)\n        results.extend(data.get(\"results\", []))\n        if not data.get(\"next\") or len(results) >= hard_limit:\n            break\n        page += 1\n        time.sleep(SLEEP)\n    return results\n\ndef looks_brazil_related(book: dict) -> bool:\n    subjects = \" \".join(book.get(\"subjects\") or [])\n    shelves = \" \".join(book.get(\"bookshelves\") or [])\n    blob = f\"{subjects} {shelves}\".lower()\n    return (\"brazil\" in blob) or (\"brasil\" in blob) or (\"brazilian\" in blob)","metadata":{"id":"dXzo3FpOiTpx","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T02:11:16.114713Z","iopub.execute_input":"2025-09-03T02:11:16.114967Z","iopub.status.idle":"2025-09-03T02:11:16.132178Z","shell.execute_reply.started":"2025-09-03T02:11:16.114944Z","shell.execute_reply":"2025-09-03T02:11:16.131708Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Coleta\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\ncollected = {}\nfor author in AUTORES_BRASILEIROS:\n    try:\n        books = fetch_books_by_author(author, hard_limit=300)\n    except Exception as e:\n        print(f\"[WARN] Falha buscando {author}: {e}\", file=sys.stderr)\n        continue\n\n    print(f\"[INFO] {author}: {len(books)} livros encontrados na API\")\n\n    for b in books:\n        # Verifica se √© Portugu√™s Brasileiro\n        if \"pt\" not in b.get(\"languages\", []):\n            print(f\"[DEBUG] Ignorado idioma: {b.get('title')} - {b.get('languages')}\")\n            continue\n\n        # Autor\n        authors = [a.get(\"name\", \"\") for a in (b.get(\"authors\") or [])]\n        if not any(is_brazilian_author(a) for a in authors):\n            print(f\"Ignorado autor: {b.get('title')} - {authors}\")\n            continue\n\n        _ = looks_brazil_related(b)\n\n        # URL TXT\n        txt_url = choose_txt_url(b.get(\"formats\", {}))\n        if not txt_url:\n            print(f\"Ignorado sem txt: {b.get('title')} - {list(b.get('formats', {}).keys())}\")\n            continue\n\n        collected[b[\"id\"]] = {\n            \"id\": b[\"id\"],\n            \"title\": b.get(\"title\", \"\").strip(),\n            \"authors\": \"; \".join(authors),\n            \"download_url\": txt_url,\n        }\n\n    print(f\"Acumulado ap√≥s {author}: {len(collected)}\")\n    time.sleep(SLEEP)\n    if len(collected) >= MAX_BOOKS:\n        break\n\nselected = list(collected.values())[:TARGET_COUNT]\nprint(f\"\\nTotal selecionado: {len(selected)} livros\")","metadata":{"id":"kp1eSamLjPql","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d1a990c6-625e-456a-8ea0-76d8615087ac","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T02:11:16.132919Z","iopub.execute_input":"2025-09-03T02:11:16.133151Z","iopub.status.idle":"2025-09-03T02:11:20.926291Z","shell.execute_reply.started":"2025-09-03T02:11:16.133129Z","shell.execute_reply":"2025-09-03T02:11:20.925716Z"}},"outputs":[{"name":"stdout","text":"[INFO] Machado de Assis: 12 livros encontrados na API\nAcumulado ap√≥s Machado de Assis: 12\n[INFO] Raul Pomp√©ia: 1 livros encontrados na API\nAcumulado ap√≥s Raul Pomp√©ia: 13\n\nTotal selecionado: 13 livros\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Download\ndef safe_filename(s: str) -> str:\n    s = re.sub(r\"[^\\w\\s.-]\", \"\", s, flags=re.UNICODE)\n    s = re.sub(r\"\\s+\", \"_\", s.strip())\n    return s[:140]\n\ndownloaded = []\n\nfor rec in selected:\n    bid = rec[\"id\"]\n    title = rec[\"title\"] or f\"book_{bid}\"\n    author_tag = safe_filename(rec[\"authors\"].split(\";\")[0] if rec[\"authors\"] else \"autor_desconhecido\")\n    fname = f\"{safe_filename(title)}__{author_tag}__pg{bid}.txt\"\n    fpath = os.path.join(OUTPUT_DIR, fname)\n\n    try:\n        resp = requests.get(rec[\"download_url\"], timeout=60)\n        resp.raise_for_status()\n        text = resp.text\n\n        # Remove cabe√ßalhos/rodap√©s Gutenberg\n        text = re.sub(\n          r\"\\*\\*\\* START OF THE PROJECT GUTENBERG EBOOK .*?\\*\\*\\*\",\n          \"\",\n          text,\n          flags=re.IGNORECASE|re.DOTALL\n        )\n        text = re.sub(\n            r\"\\*\\*\\* END OF THE PROJECT GUTENBERG EBOOK .*?\\*\\*\\*\",\n            \"\",\n            text,\n            flags=re.IGNORECASE|re.DOTALL\n        )\n\n\n        with open(fpath, \"w\", encoding=\"utf-8\") as f:\n            f.write(text)\n\n        downloaded.append({\n            \"id\": bid,\n            \"title\": rec[\"title\"],\n            \"authors\": rec[\"authors\"],\n            \"path\": fpath,\n            \"url\": rec[\"download_url\"],\n        })\n        print(f\"[OK] {fname}\")\n    except Exception as e:\n        print(f\"[ERRO] pg{bid}: {e}\", file=sys.stderr)\n    time.sleep(SLEEP)\n\ndownloaded = downloaded[:7]","metadata":{"id":"no90vaxxpcgB","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"outputId":"705a8436-b56d-45ca-9c10-f93d81c9a8c9","execution":{"iopub.status.busy":"2025-09-03T02:11:20.928438Z","iopub.execute_input":"2025-09-03T02:11:20.928636Z","iopub.status.idle":"2025-09-03T02:11:53.948123Z","shell.execute_reply.started":"2025-09-03T02:11:20.928620Z","shell.execute_reply":"2025-09-03T02:11:53.947379Z"}},"outputs":[{"name":"stdout","text":"[OK] Dom_Casmurro__Machado_de_Assis__pg55752.txt\n[OK] Memorias_Posthumas_de_Braz_Cubas__Machado_de_Assis__pg54829.txt\n[OK] Poesias_Completas__Machado_de_Assis__pg61653.txt\n[OK] Quincas_Borba__Machado_de_Assis__pg55682.txt\n[OK] Esau_e_Jacob__Machado_de_Assis__pg56737.txt\n[OK] Papeis_Avulsos__Machado_de_Assis__pg57001.txt\n[OK] A_Mao_e_A_Luva__Machado_de_Assis__pg53101.txt\n[OK] Helena__Machado_de_Assis__pg67162.txt\n[OK] Reliquias_de_Casa_Velha__Machado_de_Assis__pg67935.txt\n[OK] Historias_Sem_Data__Machado_de_Assis__pg33056.txt\n[OK] Yay√°_Garcia__Machado_de_Assis__pg67780.txt\n[OK] Memorial_de_Ayres__Machado_de_Assis__pg55797.txt\n[OK] O_Atheneu_chronica_de_saudades__Pomp√©ia_Raul__pg68541.txt\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# L√™ todos os livros baixados e escreve o conte√∫do em um √∫nico arquivo cahamdo 'manifest.csv'\nwith open(MANIFEST, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n    w = csv.writer(f)\n    w.writerow([\"id\", \"title\", \"authors\", \"path\", \"url\"])\n    for row in downloaded:\n        w.writerow([row[\"id\"], row[\"title\"], row[\"authors\"], row[\"path\"], row[\"url\"]])\n","metadata":{"id":"jQ8xs2eA9Xzs","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T02:11:53.948949Z","iopub.execute_input":"2025-09-03T02:11:53.949203Z","iopub.status.idle":"2025-09-03T02:11:53.954104Z","shell.execute_reply.started":"2025-09-03T02:11:53.949179Z","shell.execute_reply":"2025-09-03T02:11:53.953381Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# L√™ todos livros e cocatena todos livros salvando em um √∫nico arquivo chamado 'subset.txt'\nwith open(CONCAT_FILE, \"w\", encoding=\"utf-8\") as out:\n    for row in downloaded:\n        try:\n            with open(row[\"path\"], \"r\", encoding=\"utf-8\") as fin:\n                out.write(fin.read().strip() + \"\\n\\n\")\n        except Exception as e:\n            print(f\"[WARN] concat {row['path']}: {e}\", file=sys.stderr)\n","metadata":{"id":"-v6yzAUW9v3L","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T02:11:53.954848Z","iopub.execute_input":"2025-09-03T02:11:53.955100Z","iopub.status.idle":"2025-09-03T02:11:53.983707Z","shell.execute_reply.started":"2025-09-03T02:11:53.955076Z","shell.execute_reply":"2025-09-03T02:11:53.983227Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Caminho do arquivo concatenado\nsubset_path = \"data/ptbr/subset.txt\"\n\n# Ler todo o conte√∫do em uma vari√°vel\nwith open(subset_path, \"r\", encoding=\"utf-8\") as f:\n    corpus = f.read()\n\n# An√°lise do arquivo para treinar o nosso GPT2\nnum_chars = len(corpus)\nnum_words = len(corpus.split())\nnum_lines = corpus.count(\"\\n\") + 1\n\nprint(f\"N√∫mero de Caracteres: {num_chars}\")\nprint(f\"N√∫mero de Palavras: {num_words}\")\nprint(f\"N√∫mero de Linhas: {num_lines}\")","metadata":{"id":"T9ACROvs-Tav","trusted":true,"outputId":"e79688ed-b966-4a77-a8b4-23cfaf094d12","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-09-03T02:11:53.984275Z","iopub.execute_input":"2025-09-03T02:11:53.984535Z","iopub.status.idle":"2025-09-03T02:11:54.045672Z","shell.execute_reply.started":"2025-09-03T02:11:53.984512Z","shell.execute_reply":"2025-09-03T02:11:54.045086Z"}},"outputs":[{"name":"stdout","text":"N√∫mero de Caracteres: 2535277\nN√∫mero de Palavras: 428455\nN√∫mero de Linhas: 60957\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Divis√£o: Treino, Teste e Valida√ß√£o","metadata":{"id":"jzXGfFy2H4Ph"}},{"cell_type":"code","source":"data = corpus","metadata":{"id":"m6iHk62nIp6e","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T02:11:54.046288Z","iopub.execute_input":"2025-09-03T02:11:54.046565Z","iopub.status.idle":"2025-09-03T02:11:54.050060Z","shell.execute_reply.started":"2025-09-03T02:11:54.046543Z","shell.execute_reply":"2025-09-03T02:11:54.049391Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"num_chars = len(data)\nnum_words = len(data.split())\nnum_lines = data.count(\"\\n\") + 1\n\nprint(f\"N√∫mero de Caracteres: {num_chars}\")\nprint(f\"N√∫mero de Palavras: {num_words}\")\nprint(f\"N√∫mero de Linhas: {num_lines}\")","metadata":{"id":"vuy1IHPESpYs","trusted":true,"outputId":"0b4a0b67-07b4-496f-8a7f-f5ff052dda41","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-09-03T02:11:54.050843Z","iopub.execute_input":"2025-09-03T02:11:54.051511Z","iopub.status.idle":"2025-09-03T02:11:54.108784Z","shell.execute_reply.started":"2025-09-03T02:11:54.051493Z","shell.execute_reply":"2025-09-03T02:11:54.108215Z"}},"outputs":[{"name":"stdout","text":"N√∫mero de Caracteres: 2535277\nN√∫mero de Palavras: 428455\nN√∫mero de Linhas: 60957\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Calcular √≠ndices de corte\ndata = data[0: len(data) // 2]\nn = len(data)\ntrain_end = int(0.8 * n)\ntest_end = int(0.9 * n)\n\n# Divis√£o\ntrain_data = data[:train_end]\ntest_data = data[train_end:test_end]\nval_data = data[test_end:]\n\n# Salvar os arquivos\nwith open(\"train.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(train_data)\n\nwith open(\"test.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(test_data)\n\n\nwith open(\"val.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(val_data)\n\nprint(f\"Treino: {len(train_data)} docs\")\nprint(f\"Teste: {len(test_data)} docs\")\nprint(f\"Valida√ß√£o: {len(val_data)} docs\")","metadata":{"id":"vrZBtxJUIfOo","trusted":true,"outputId":"81069028-ece3-4f4a-83dc-b078f7246879","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-09-03T02:11:54.109423Z","iopub.execute_input":"2025-09-03T02:11:54.109584Z","iopub.status.idle":"2025-09-03T02:11:54.126779Z","shell.execute_reply.started":"2025-09-03T02:11:54.109571Z","shell.execute_reply":"2025-09-03T02:11:54.126226Z"}},"outputs":[{"name":"stdout","text":"Treino: 1014110 docs\nTeste: 126764 docs\nValida√ß√£o: 126764 docs\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# **M√©todos ‚öôÔ∏è**","metadata":{"id":"duLEhAFRKSiK"}},{"cell_type":"markdown","source":"## TikToken","metadata":{"id":"HeqvBJhnT9Ep"}},{"cell_type":"code","source":"# Verifica√ß√£o do uso do TikToken\ndef text_to_token_ids(text, tokenizer, device):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n    return encoded_tensor.to(device)\n\ndef token_ids_to_text(token_ids, tokenizer, device):\n    flat = token_ids.squeeze(0) # remove batch dimension\n    return tokenizer.decode(flat.tolist())\n\ntokenizer = tiktoken.get_encoding(\"o200k_base\")\nencode = tokenizer.encode(\"Ol√° mundo!\")\ndecode = tokenizer.decode(encode)\nprint(f\"{encode}: {decode}\")","metadata":{"id":"qbnBKeq2RbwG","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b6eafc67-19e8-4f3d-e298-7e5abcd7b5cf","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T02:11:54.127495Z","iopub.execute_input":"2025-09-03T02:11:54.127651Z","iopub.status.idle":"2025-09-03T02:11:57.562257Z","shell.execute_reply.started":"2025-09-03T02:11:54.127638Z","shell.execute_reply":"2025-09-03T02:11:57.561524Z"}},"outputs":[{"name":"stdout","text":"[89191, 10225, 0]: Ol√° mundo!\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## Criando Dataset\n\n- O texto √© passado para o DatasetGPT e em seguida todo o texto √© codificado em ID de tokens.\n- Depois √© criado v√°rios arrays onde cada um comporta max_length tokens, ou seja, ser√° criado N linhas versus max_length colunas.\n","metadata":{"id":"FDUDLWDKmGns"}},{"cell_type":"code","source":"# Defini√ß√£o de lotes de dados de treino\nclass Dataset_GPT(Dataset):\n    def __init__(self, text, tokenizer, stride, max_length):\n        self.input_ids = []\n        self.target_ids = []\n\n        allowed_special = {\n            '<|endoftext|>'\n        }\n        tokens = tokenizer.encode(text, allowed_special=allowed_special) # Codifica o texto recebido\n\n        for i in range(0, len(tokens) - max_length, stride):\n            self.input_ids.append(torch.tensor(tokens[i: i + max_length])) # Seleciona os tokens do tamanho i at√© i + max_length\n            self.target_ids.append(torch.tensor(tokens[i+1: i+max_length + 1])) # Seleciona os tokens do tamanho i + 1 at√© i + max_length + 1\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.target_ids[idx]\n\n    def __len__(self):\n        return len(self.input_ids)","metadata":{"id":"OJ6gBo1bazYJ","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T02:11:57.563109Z","iopub.execute_input":"2025-09-03T02:11:57.563397Z","iopub.status.idle":"2025-09-03T02:11:57.568718Z","shell.execute_reply.started":"2025-09-03T02:11:57.563373Z","shell.execute_reply":"2025-09-03T02:11:57.568019Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Criando valida√ß√£o cruzada, separando em dados de treino e teste","metadata":{"id":"a4ShoVM-efOy"}},{"cell_type":"markdown","source":"- O shape do loader ser√° batch_size, N, max_length","metadata":{"id":"FXCUstMBOUbY"}},{"cell_type":"code","source":"# Cria√ß√£o do DataLoader do PyTorch\ndef create_dataset(text, stride, max_length, shuffle, drop_last, num_workers, batch_size):\n    tokenizer = tiktoken.get_encoding(\"o200k_base\")\n\n    dataset = Dataset_GPT(\n        text=text,\n        tokenizer=tokenizer,\n        stride=stride,\n        max_length=max_length\n    )\n\n    return DataLoader(\n        dataset=dataset,  # define o dataset processado na classe Dataset_GPT\n        batch_size=batch_size, # quantidade de amostras por lote\n        shuffle=shuffle, # embaralhar os dados - evitar overfitting\n        drop_last=drop_last, # discartar os dados, caso n√£o complete o tamanho da sequ√™ncia de tokens\n        num_workers=num_workers # paraleliza√ß√£o\n    )","metadata":{"id":"YU_LMOH6hR0W","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T02:11:57.569367Z","iopub.execute_input":"2025-09-03T02:11:57.569688Z","iopub.status.idle":"2025-09-03T02:11:57.582023Z","shell.execute_reply.started":"2025-09-03T02:11:57.569663Z","shell.execute_reply":"2025-09-03T02:11:57.581383Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"max_length = 250\nbatch_sz = 10\n\ntrain_loader = create_dataset(\n    text=train_data,\n    max_length=max_length,\n    stride=1,\n    batch_size=batch_sz,\n    num_workers=4,\n    drop_last=True,\n    shuffle=True\n)\n\ntest_loader = create_dataset(\n    text=test_data,\n    max_length=max_length,\n    stride=1,\n    batch_size=batch_sz,\n    num_workers=4,\n    drop_last=True,\n    shuffle=True\n)\n\nval_loader = create_dataset(\n    text=val_data,\n    max_length=max_length,\n    stride=1,\n    batch_size=batch_sz,\n    num_workers=4,\n    drop_last=True,\n    shuffle=True\n)","metadata":{"id":"4lHZbA1pi2g1","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9573c599-a589-4158-bf12-462dbb1a8ccc","execution":{"iopub.status.busy":"2025-09-03T02:11:57.582676Z","iopub.execute_input":"2025-09-03T02:11:57.582841Z","iopub.status.idle":"2025-09-03T02:12:30.076974Z","shell.execute_reply.started":"2025-09-03T02:11:57.582828Z","shell.execute_reply":"2025-09-03T02:12:30.076369Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def print_loader_info(title, loader, init=False):\n    print(title)\n    print(f\"\\tTotal de amostras: {loader.dataset.__len__()}\")\n    print(f\"\\tTokens em cada amostra: {len(loader.dataset.__getitem__(0)[0])}\")\n    print(f\"\\tN√∫mero de batches: {len(loader)}\")\n    print(f\"\\tN√∫mero de amostras por batch: {loader.batch_size}\")\n\nprint_loader_info(\"Treino\", train_loader, init=False)\nprint_loader_info(\"Teste\", test_loader, init=True)\nprint_loader_info(\"Valida√ß√£o\", val_loader, init=True)","metadata":{"id":"4FHfOx95OxJt","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c4a0f214-1dd5-4be4-9e30-afe061725fc7","execution":{"iopub.status.busy":"2025-09-03T02:12:30.077682Z","iopub.execute_input":"2025-09-03T02:12:30.077861Z","iopub.status.idle":"2025-09-03T02:12:30.084040Z","shell.execute_reply.started":"2025-09-03T02:12:30.077847Z","shell.execute_reply":"2025-09-03T02:12:30.083324Z"}},"outputs":[{"name":"stdout","text":"Treino\n\tTotal de amostras: 287947\n\tTokens em cada amostra: 250\n\tN√∫mero de batches: 28794\n\tN√∫mero de amostras por batch: 10\nTeste\n\tTotal de amostras: 34711\n\tTokens em cada amostra: 250\n\tN√∫mero de batches: 3471\n\tN√∫mero de amostras por batch: 10\nValida√ß√£o\n\tTotal de amostras: 35353\n\tTokens em cada amostra: 250\n\tN√∫mero de batches: 3535\n\tN√∫mero de amostras por batch: 10\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# **- M√∫ltiplas Cabe√ßas de Aten√ß√£o:**","metadata":{"id":"duKYNqvFK3Sr"}},{"cell_type":"code","source":"class MultiHeadAttention(torch.nn.Module):\n    def __init__(self, dim_in, dim_out, context_length, num_heads, bias=False):\n        super().__init__()\n\n        self.dim_out = dim_out\n        self.num_heads = num_heads\n        self.head_dim = dim_out // num_heads\n        assert dim_out % num_heads == 0\n\n        self.wq = torch.nn.Linear(dim_in, dim_out, bias)\n        self.wk = torch.nn.Linear(dim_in, dim_out, bias)\n        self.wv = torch.nn.Linear(dim_in, dim_out, bias)\n        self.wo_proj = torch.nn.Linear(dim_out, dim_out, bias)\n\n        self.register_buffer(\n            \"mask\",\n            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n        )\n\n    def forward(self, x):\n        batch, num_tokens, d_in = x.shape\n        queries = self.wq(x)\n        keys = self.wk(x)\n        values = self.wv(x)\n\n        queries = queries.view(batch, num_tokens, self.num_heads, self.head_dim)\n        keys = keys.view(batch, num_tokens, self.num_heads, self.head_dim)\n        values = values.view(batch, num_tokens, self.num_heads, self.head_dim)\n\n        queries = queries.transpose(1, 2)\n        keys = keys.transpose(1, 2)\n        values = values.transpose(1, 2)\n\n        attention = queries @ keys.transpose(2, 3)\n        attention.masked_fill_(\n            self.mask.bool()[:num_tokens, :num_tokens],\n            -torch.inf\n        )\n        causal_attention = torch.softmax(attention / keys.shape[-1]**0.5,dim=-1)\n\n        context_vec = causal_attention @ values\n        context_vec = context_vec.transpose(1, 2)\n\n        concat = context_vec.contiguous().view(batch, num_tokens, self.dim_out)\n        concat = self.wo_proj(concat)\n\n        return concat","metadata":{"id":"UNo3a5D5Lzwn","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T02:12:30.084833Z","iopub.execute_input":"2025-09-03T02:12:30.085061Z","iopub.status.idle":"2025-09-03T02:12:30.097964Z","shell.execute_reply.started":"2025-09-03T02:12:30.085046Z","shell.execute_reply":"2025-09-03T02:12:30.097365Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"### Teste do Multi-Head Attention","metadata":{"id":"jCBgCQoGqTVm"}},{"cell_type":"code","source":"# Teste de dimens√µes de sa√≠da do Multi-Head Attention\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntests = [\n    {\n        \"hiperparams\": {\n            \"context_length\": 256,\n            \"embedding_dim\": 512,\n            \"batch_size\": 2,\n            \"bias\": False,\n            \"num_heads\": 8\n        },\n        \"result\": {\n            \"accept\": True\n        }\n    },\n\n    {\n        \"hiperparams\": {\n            \"context_length\": 256,\n            \"embedding_dim\": 1024,\n            \"batch_size\": 2,\n            \"bias\": False,\n            \"num_heads\": 8\n        },\n        \"result\": {\n            \"accept\": True\n        }\n    }\n]\n\nfor test in tests:\n    hiperparams = test[\"hiperparams\"]\n    sample = torch.rand(\n        hiperparams[\"batch_size\"],\n        hiperparams[\"context_length\"],\n        hiperparams[\"embedding_dim\"],\n        device=device\n    )\n\n    batch, tokens, embed_dim = sample.shape\n\n    multi_head_attention = MultiHeadAttention(\n        dim_in=embed_dim,\n        dim_out=embed_dim,\n        num_heads=8,\n        context_length=tokens,\n        bias=False\n    ).to(device)\n\n    multi_head_attention_forward = multi_head_attention(sample)\n\n    if test[\"result\"][\"accept\"]:\n        assert sample.shape == multi_head_attention_forward.shape","metadata":{"id":"rQtRepATh6tG","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T02:12:30.098712Z","iopub.execute_input":"2025-09-03T02:12:30.098950Z","iopub.status.idle":"2025-09-03T02:12:30.703068Z","shell.execute_reply.started":"2025-09-03T02:12:30.098928Z","shell.execute_reply":"2025-09-03T02:12:30.702517Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## Normaliza√ß√£o","metadata":{"id":"gUnbqyd3vi1-"}},{"cell_type":"markdown","source":"### Layer Norm","metadata":{"id":"pjniPi3KhnUc"}},{"cell_type":"code","source":"# Queremos a m√©dia 0 e vari√¢ncia 1 para os embeddings de cada token\n\nclass LayerNorm(torch.nn.Module):\n    def __init__(self, embedding_dim, epsilon = 1e-5):\n        super().__init__()\n        self.gama = torch.nn.Parameter(torch.ones(embedding_dim))\n        self.beta = torch.nn.Parameter(torch.zeros(embedding_dim))\n        self.epsilon = epsilon\n\n    def forward(self, x):\n        mean = x.mean(dim=-1, keepdim=True)\n        var = x.var(dim=-1, keepdim=True, unbiased=False)\n        x = (x - mean) / torch.sqrt(var + self.epsilon)\n        return x * self.gama + self.beta","metadata":{"id":"VRb1L_kKhX1c","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T02:12:30.703756Z","iopub.execute_input":"2025-09-03T02:12:30.703947Z","iopub.status.idle":"2025-09-03T02:12:30.709327Z","shell.execute_reply.started":"2025-09-03T02:12:30.703932Z","shell.execute_reply":"2025-09-03T02:12:30.708702Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"### Teste do Layer Norm","metadata":{"id":"c8ENJlSMq0sD"}},{"cell_type":"code","source":"# Testando dimens√µes de sa√≠da da normaliza√ß√£o z-score LayerNorm\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nbatch, tokens, embedding_dim = 2, 256, 512\nsample = torch.rand(2, 256, 512, device=device)\n\nnorm = LayerNorm(embedding_dim=embedding_dim).to(device)\nresult = norm(sample)\n\nassert sample.shape == result.shape\n\n\nsample_2 = torch.tensor([[[1.0, 2.0, 3.0]]])\nnorm_2 = LayerNorm(embedding_dim=sample_2.shape[2], epsilon=1e-5)\nresult_2 = norm_2(sample_2)\n\nmean = sample_2.mean(dim=-1, keepdim=True)\nvar = sample_2.var(dim=-1, unbiased=False, keepdim=True)\nmanual = (sample_2 - mean) / torch.sqrt(var + norm_2.epsilon)\n\nassert sample_2.shape == result_2.shape\nassert torch.equal(result_2, manual * norm_2.gama + norm_2.beta)","metadata":{"id":"bQh4d1n5q3QH","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T02:12:30.712685Z","iopub.execute_input":"2025-09-03T02:12:30.712888Z","iopub.status.idle":"2025-09-03T02:12:30.849260Z","shell.execute_reply.started":"2025-09-03T02:12:30.712874Z","shell.execute_reply":"2025-09-03T02:12:30.848682Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## Fun√ß√£o de Ativa√ß√£o","metadata":{"id":"xIzFdT9-yz8Q"}},{"cell_type":"code","source":"class GELU(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return 0.5 * x * (1 + torch.tanh(\n            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n            (x + 0.044715 * torch.pow(x, 3))\n        ))","metadata":{"id":"HtNDwYkDy3Pf","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T02:12:30.934970Z","iopub.execute_input":"2025-09-03T02:12:30.935163Z","iopub.status.idle":"2025-09-03T02:12:30.939474Z","shell.execute_reply.started":"2025-09-03T02:12:30.935147Z","shell.execute_reply":"2025-09-03T02:12:30.938796Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"### Teste do GELU","metadata":{"id":"hsvTZh_Tx6pB"}},{"cell_type":"code","source":"# Testando dimens√µes de sa√≠da da fun√ß√£o de ativa√ß√£o GeLU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ngelu_test = torch.rand(2, 256, 512, device=device)\nbatch, context_length, embedding_dim = gelu_test.shape\n\ngelu = GELU().to(device)\nresult = gelu(gelu_test)\n\nassert gelu_test.shape == result.shape","metadata":{"id":"skmLfg97hhOz","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T02:12:30.940113Z","iopub.execute_input":"2025-09-03T02:12:30.940354Z","iopub.status.idle":"2025-09-03T02:12:30.959685Z","shell.execute_reply.started":"2025-09-03T02:12:30.940314Z","shell.execute_reply":"2025-09-03T02:12:30.959155Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"## FeedForward","metadata":{"id":"M8fIsNYlx6yV"}},{"cell_type":"code","source":"# Usando autoencoder com overcomplete (dimens√£o da camada oculta maior que\n# o tamanho da entrada) for√ßando a camada latente a ser esparsa para gerar\n# representa√ß√µes esparsas interpret√°veis, capturar rela√ß√µes n√£o lineares\n# (que o undercomplete n√£o consegue) e descobrir estrutura latente em dados\n# complexos.\n\nclass FeedForward(torch.nn.Module):\n    def __init__(self, embedding_dim, bias=False):\n        super().__init__()\n        self.layers = torch.nn.Sequential(\n            torch.nn.Linear(\n                embedding_dim,\n                4 * embedding_dim,\n                bias=bias\n            ),\n            GELU(),\n            torch.nn.Linear(\n                4 * embedding_dim,\n                embedding_dim,\n                bias=bias\n            )\n        )\n\n    def forward(self, x):\n        return self.layers(x)","metadata":{"id":"zg5jh2Qqx6KQ","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T02:12:30.960433Z","iopub.execute_input":"2025-09-03T02:12:30.961186Z","iopub.status.idle":"2025-09-03T02:12:30.965608Z","shell.execute_reply.started":"2025-09-03T02:12:30.961159Z","shell.execute_reply":"2025-09-03T02:12:30.965053Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"### Teste do FeedForward","metadata":{"id":"jcUghcKAyEte"}},{"cell_type":"code","source":"# Testando dimens√µes de sa√≠da do FeedForward\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nfeed_forward_test = torch.rand(2, 256, 512, device=device)\nbatch, context_length, embedding_dim = feed_forward_test.shape\n\nfeed_forward = FeedForward(embedding_dim=embedding_dim, bias=False).to(device)\nresult = feed_forward(feed_forward_test)\n\nassert feed_forward_test.shape == result.shape","metadata":{"id":"ls4_4FcfeCXO","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T02:12:30.966195Z","iopub.execute_input":"2025-09-03T02:12:30.966432Z","iopub.status.idle":"2025-09-03T02:12:31.002914Z","shell.execute_reply.started":"2025-09-03T02:12:30.966415Z","shell.execute_reply":"2025-09-03T02:12:31.002192Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"## Bloco Transformer","metadata":{"id":"YW9loqiT0-HQ"}},{"cell_type":"code","source":"class TransformerBlock(torch.nn.Module):\n    def __init__(self, context_length, dim_in, dim_out, num_heads, bias=False):\n        super().__init__()\n\n        # Usando a classe do Multi-Head Attention criada\n        self.multi_head_attention = MultiHeadAttention(\n            dim_in=dim_in,\n            dim_out=dim_out,\n            num_heads=num_heads,\n            context_length=context_length,\n            bias=bias\n        )\n\n        # Usando a classe do FeedForward criada\n        self.feed_forward = FeedForward(\n            embedding_dim=dim_in,\n            bias=bias\n        )\n\n        self.norm1 = LayerNorm(\n            embedding_dim=dim_in\n        )\n\n        self.norm2 = LayerNorm(\n            embedding_dim=dim_in\n        )\n\n\n    def forward(self, x):\n        residual_connection_attention = x\n        norm1 = self.norm1(x)\n        attention_masked = self.multi_head_attention(norm1)\n        add1 = attention_masked + residual_connection_attention\n\n        residual_connetion_forward = add1\n        norm2 = self.norm2(add1)\n        forward = self.feed_forward(norm2)\n        add2 = forward + residual_connetion_forward\n\n        return add2","metadata":{"id":"puvdwPp4FBgv","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T02:12:31.003724Z","iopub.execute_input":"2025-09-03T02:12:31.003962Z","iopub.status.idle":"2025-09-03T02:12:31.009282Z","shell.execute_reply.started":"2025-09-03T02:12:31.003941Z","shell.execute_reply":"2025-09-03T02:12:31.008669Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"### Teste do bloco Transformer","metadata":{"id":"cQ9HUMQjyMbK"}},{"cell_type":"code","source":"# Testando dimens√µes de sa√≠da do bloco transformer\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntransformer_block_test = torch.rand(2, 256, 512, device=device)\nbatch, context_length, embedding_dim = transformer_block_test.shape\n\ntransformer_block = TransformerBlock(\n    context_length=context_length,\n    dim_in=embedding_dim,\n    dim_out=embedding_dim,\n    num_heads=8,\n    bias=False\n).to(device)\n\nresult = transformer_block(feed_forward_test)\n\nassert transformer_block_test.shape == result.shape","metadata":{"id":"O42Ncx0Th8lU","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T02:12:31.010192Z","iopub.execute_input":"2025-09-03T02:12:31.010443Z","iopub.status.idle":"2025-09-03T02:12:31.054235Z","shell.execute_reply.started":"2025-09-03T02:12:31.010423Z","shell.execute_reply":"2025-09-03T02:12:31.053750Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"- Depois de passar pelos blocos transformers, como o tamamnho da sa√≠da do FeedForward √© igual ao tamanho da entrada;\n- Normalizamos os valores da sa√≠da dos blocos tansformers;\n- Sabemos que as linhas de sa√≠da dos blocos transformers s√£o os tokens e as colunas tem dimens√£o do embedding;\n- Precisamos que cada token tenha dimens√£o do tamanho do vocabul√°rio para saber a qual √© o pr√≥ximo token;\n- Multiplica-se a sa√≠da por uma matriz de pesos, onde as linhas tem o tamanho do embedding e a sa√≠da igual o tamanho do vocabul√°rio;\n- Normalizamos a sa√≠da dos tokens com dimens√£o do vocab_size, pois multiplicamos por uma matriz de pesos que alterou a dimens√£o;\n- Normalizamos usando o softmax;\n- Ap√≥s os tokens ter dimens√£o do vocab_size, o token 1 ter√° que ter maior score para o ind√≠ce do segundo token mais prov√°vel (token 2);\n- O token 2 ter√° que ter maior score para o ind√≠ce do segundo token mais prov√°vel a ser o token 3, e assim em diante;\n- Para gerar o pr√≥ximo token, recuperamos o √∫ltimo token e buscamos qual √© o √≠ndice do token com maior score (mais prov√°vel);\n- Para recuperar o mais prov√°vel, como buscamos o √≠ndice da coluna com o maior score, usamos a fun√ß√£o argmax.","metadata":{"id":"rwdinswfNGAb"}},{"cell_type":"markdown","source":"### GPT2: Original","metadata":{"id":"FgLR39YKM0tN"}},{"cell_type":"code","source":"class GPT2ModelOriginal(torch.nn.Module):\n    def __init__(self, config, device):\n        super().__init__()\n\n        self.device = device\n\n        self.embeddings = torch.nn.Embedding(\n            num_embeddings=config[\"vocab_size\"],\n            embedding_dim=config[\"embedding_dim\"]\n        )\n\n        self.pos_embeddings = torch.nn.Embedding(\n            num_embeddings=config[\"context_length\"],\n            embedding_dim=config[\"embedding_dim\"]\n        )\n\n        self.transformer_blocks = torch.nn.Sequential(*[\n            TransformerBlock(\n                context_length=config[\"context_length\"],\n                dim_in=config[\"embedding_dim\"],\n                dim_out=config[\"embedding_dim\"],\n                num_heads=config[\"num_heads\"],\n                bias=config[\"bias\"]\n            )\n            for _ in range(config[\"num_layers\"])\n        ])\n\n        self.final_norm = LayerNorm(\n            embedding_dim=config[\"embedding_dim\"],\n            epsilon=1e-5\n        )\n\n        self.out_head = torch.nn.Linear(\n            config[\"embedding_dim\"],\n            config[\"vocab_size\"],\n            bias=config[\"bias\"]\n        )\n\n\n    def forward(self, x):\n        batch_size, context_length = x.shape\n        tok_emb = self.embeddings(x)\n        pos_emb = self.pos_embeddings(\n            torch.arange(context_length, device=self.device)\n        )\n        input_emb = tok_emb + pos_emb\n\n        result_transformer_blocks = self.transformer_blocks(input_emb)\n        norm = self.final_norm(result_transformer_blocks)\n        logits = self.out_head(norm)\n\n        return logits","metadata":{"id":"83_NRl_2hgSA","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T02:12:31.054882Z","iopub.execute_input":"2025-09-03T02:12:31.055112Z","iopub.status.idle":"2025-09-03T02:12:31.061445Z","shell.execute_reply.started":"2025-09-03T02:12:31.055091Z","shell.execute_reply":"2025-09-03T02:12:31.060893Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"#### Teste do GPT2 Original","metadata":{"id":"-F7dRfjBYFu4"}},{"cell_type":"code","source":"# Testando dimens√µes de sa√≠da do modelo GPT\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nconfig_test = {\n    \"vocab_size\": 100,\n    \"embedding_dim\": 512,\n    \"context_length\": 256,\n    \"num_layers\": 8,\n    \"num_heads\": 8,\n    \"bias\": False\n}\n\ntest_gpt_model = torch.randint(\n    high=config_test[\"vocab_size\"],\n    size=(2, config_test[\"context_length\"]),\n    device=device\n)\n\nmodel_test = GPT2ModelOriginal(config=config_test, device=device).to(device)\nmodel_test_forward = model_test(test_gpt_model)\n\nassert model_test_forward.shape == (2, 256, config_test[\"vocab_size\"])","metadata":{"id":"zjILklbuYMWY","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T02:12:31.062282Z","iopub.execute_input":"2025-09-03T02:12:31.062558Z","iopub.status.idle":"2025-09-03T02:12:31.378956Z","shell.execute_reply.started":"2025-09-03T02:12:31.062543Z","shell.execute_reply":"2025-09-03T02:12:31.378197Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"# **Experimentos üß™**","metadata":{"id":"X_1Cww7KO_DF"}},{"cell_type":"markdown","source":"- A dimens√£o da sa√≠da do GPT s√£o os tokens da senten√ßa, vocab_size;\n\n> Adicionar aspas\n\n\n- Extrai-se o √∫ltimo token, pois ele carrega a informa√ß√£o de qual palavra vem a seguir;\n- Normaliza-se os valores com softmax;\n- Extrai-se o √≠ndice do token do vocabul√°rio com a maior probabilidade;\n- O ID token selecionado ser√° concatenado a lista de tokens da senten√ßa.","metadata":{"id":"wP4efNliYdYK"}},{"cell_type":"markdown","source":"## Gera√ß√£o de Texto","metadata":{"id":"mfeoSoNOYlGS"}},{"cell_type":"code","source":"def generate_text(model, idx, max_new_tokens, context_size=50, reset=False):\n    model.eval()\n    reset = False\n\n    try:\n        for i in range((max_new_tokens + 1) if reset else max_new_tokens):\n            idx_cond = idx[:, -context_size:]\n            with torch.no_grad():\n                logits = model(idx_cond, reset) if reset else model(idx_cond)\n                if logits == None: continue\n\n            reset = False\n\n            logits = logits[:, -1, :]\n            probas = torch.nn.functional.softmax(logits, dim=-1)\n            idx_next_token = torch.argmax(probas, dim=-1, keepdim=True)\n            idx = torch.cat((idx, idx_next_token), dim=1)\n\n        return idx\n\n    except Exception as e:\n        print(e)\n        return idx","metadata":{"id":"XPH8mLXDQkDq","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T02:12:31.379785Z","iopub.execute_input":"2025-09-03T02:12:31.379986Z","iopub.status.idle":"2025-09-03T02:12:31.385288Z","shell.execute_reply.started":"2025-09-03T02:12:31.379971Z","shell.execute_reply":"2025-09-03T02:12:31.384717Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"### GPT Original","metadata":{"id":"wDz92zMOEyeH"}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = tiktoken.get_encoding(\"o200k_base\")\ntorch.manual_seed(123)\n\nconfig = {\n    \"vocab_size\": tokenizer.n_vocab,\n    \"embedding_dim\": 512,\n    \"context_length\": 256,\n    \"num_layers\": 8,\n    \"num_heads\": 8,\n    \"bias\": False\n}\n\ntext = \"Ol√° mundo!\"\ntext_encoded = tokenizer.encode(text)\ntext_encoded_tensor = torch.tensor(text_encoded, device=device).unsqueeze(0)\n\nmodel_test = GPT2ModelOriginal(config, device).to(device)\n\nresult_encoded = generate_text(\n    idx=text_encoded_tensor,\n    model=model_test,\n    max_new_tokens=10,\n    context_size=config[\"context_length\"]\n)\n\nresult_encoded_list = result_encoded.squeeze(0).tolist()\nresult = tokenizer.decode(result_encoded_list)\nprint(result)","metadata":{"id":"ZRMfVwf2YtUY","colab":{"base_uri":"https://localhost:8080/"},"outputId":"59e19cee-d20a-4581-e4aa-30a34427e6d4","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T02:12:31.386105Z","iopub.execute_input":"2025-09-03T02:12:31.386808Z","iopub.status.idle":"2025-09-03T02:12:33.708185Z","shell.execute_reply.started":"2025-09-03T02:12:31.386780Z","shell.execute_reply":"2025-09-03T02:12:33.707473Z"}},"outputs":[{"name":"stdout","text":"Ol√° mundo!(Font ka ª kuri‚Äô√©coute√∫striasparty ŸÖÿ∞ uj::_('Î∞ñ\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"## Conjunto de Fun√ß√µes para Salvar os pesos do Modelo","metadata":{"id":"QW0lZL69ti3G"}},{"cell_type":"code","source":"def fetch_weights_and_bias(user, project, name, version, file_name):\n    \"\"\"\n    Busca um artifact de pesos salvo no W&B.\n    Exemplo de version: 'v1', 'v2', etc (n√£o o run id).\n    \"\"\"\n    try:\n        api = wandb.Api()\n        artifact = api.artifact(f\"{user}/{project}/{name}:{version}\", type=\"model\")\n        artifact_dir = artifact.download()\n        file_path = os.path.join(artifact_dir, file_name)\n        print(f\"Fetch success -> {file_path}\")\n        return True\n\n    except Exception as e:\n        print(f\"Fetch Error: {e}\")\n        return False\n\ndef load_weights_and_bias(file_name):\n    try:\n        checkpoint = torch.load(file_name, map_location=\"cpu\")\n        return True, checkpoint\n    except Exception as e:\n        print(f\"Load error: {e}\")\n        return False, {}\n\n# def load_weights_and_bias(file_name):\n#     try:\n#         return True, torch.load(file_name)\n\n#     except Exception as e:\n#         print(\"Load error\")\n#         return False, {}","metadata":{"id":"uihMe0CNCvQ8","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T02:12:33.709004Z","iopub.execute_input":"2025-09-03T02:12:33.709255Z","iopub.status.idle":"2025-09-03T02:12:33.714716Z","shell.execute_reply.started":"2025-09-03T02:12:33.709220Z","shell.execute_reply":"2025-09-03T02:12:33.713981Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"os.environ[\"WANDB_API_KEY\"] = \"5ca14c4352d864370bad3199b26c5dab929ba976\"\n\ndef train_model(model, optimizer, config):\n    run = wandb.init(project=config[\"project\"], name=config[\"name\"], id=config[\"run_id\"], resume=\"allow\")\n    res_fetch = fetch_weights_and_bias(\n        #wandb=run,\n        user=config[\"user\"],\n        project=config[\"project\"],\n        name=config[\"name\"],\n        version=config[\"version\"],\n        file_name=config[\"file_name\"]\n    )\n\n    state_dict = {}\n    if res_fetch:\n        loaded, checkpoint = load_weights_and_bias(\n            file_name=config[\"file_name\"]\n        )\n        if loaded:\n            model.load_state_dict(checkpoint[\"model_state\"])\n            optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n            state_dict[\"epoch\"] = checkpoint.get(\"epoch\", 0)\n            state_dict[\"batch\"] = checkpoint.get(\"batch\", 0)\n            state_dict[\"train_time\"] = checkpoint.get(\"train_time\", 0.0)\n            print(\"Pesos carregados com sucesso!\")\n\n    print(\"\\nEPOCHS/BATCHS RECUPERADOS: \", state_dict)\n    num_epochs = config[\"max_epochs\"]\n\n    start_time = time.time()\n    train_losses, val_losses, tokens_seen, total_train_time = train_model_simple(\n        run, model, train_loader, val_loader, optimizer, device,\n        num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n        start_context=\"Bom dia!\", tokenizer=tokenizer,\n        save_freq_wdb=config[\"save_freq_wdb\"], file_name=config[\"file_name\"],\n        save_wdb=config[\"save_wdb\"], state_dict=state_dict,\n        project=config[\"project\"], name=config[\"name\"], start_time=start_time\n    )\n    print(\"\\nGR√ÅFICO DE PERDA DURANTE O TREINO:\")\n    epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n    plot_graph(epochs_tensor, tokens_seen, train_losses, val_losses)\n    # Retorna o n√∫mero de tokens processados. Isso ir√° ajuda na quantidade de tokens/s\n    return tokens_seen[-1], total_train_time","metadata":{"id":"QM-Yie6vnV0E","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T02:12:33.715357Z","iopub.execute_input":"2025-09-03T02:12:33.715593Z","iopub.status.idle":"2025-09-03T02:12:33.728912Z","shell.execute_reply.started":"2025-09-03T02:12:33.715577Z","shell.execute_reply":"2025-09-03T02:12:33.728217Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"## Conjunto de Fun√ß√µes para Treinamento e Avalia√ß√£o do Modelo","metadata":{"id":"Cu6UKDU3YzVk"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\n\n\ndef plot_graph(epochs_seen, tokens_seen, train_losses, val_losses):\n    fig, ax1 = plt.subplots(figsize=(5, 3))\n\n    ax1.plot(epochs_seen, train_losses, label=\"Perda no Treino\")\n    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Perda na Valida√ß√£o\")\n    ax1.set_xlabel(\"√âpocas\")\n    ax1.set_ylabel(\"Perda\")\n    ax1.legend(loc=\"upper right\")\n    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n\n    ax2 = ax1.twiny()\n    ax2.plot(tokens_seen, train_losses, alpha=0)\n    ax2.set_xlabel(\"Tokens Vistos\")\n\n    fig.tight_layout()  # Adjust layout to make room\n    plt.savefig(\"loss-plot.pdf\")\n    plt.show()","metadata":{"id":"FpMLH66jn1-r","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T02:12:33.729725Z","iopub.execute_input":"2025-09-03T02:12:33.730286Z","iopub.status.idle":"2025-09-03T02:12:33.745658Z","shell.execute_reply.started":"2025-09-03T02:12:33.730263Z","shell.execute_reply":"2025-09-03T02:12:33.745131Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def calc_loss_batch_by_cross_entropy(model, input_batch, target_batch, device):\n    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n    logits = model(input_batch)\n    loss = torch.nn.functional.cross_entropy(\n        logits.flatten(0, 1),\n        target_batch.flatten()\n    )\n\n    return loss\n\n\ndef calc_loss_loader(data_loader, model, device, num_batches=None):\n    total_loss = 0\n    if len(data_loader) == 0:\n        return float('nan')\n    elif num_batches is None:\n        num_batches = len(data_loader)\n    else:\n        num_batches = min(num_batches, len(data_loader))\n\n    for i, (input_batch, target_batch) in enumerate(data_loader):\n        if i < num_batches:\n            loss = calc_loss_batch_by_cross_entropy(\n                input_batch=input_batch,\n                target_batch=target_batch,\n                model=model,\n                device=device\n            )\n\n            total_loss += loss.item()\n        else:\n            break\n\n    return total_loss","metadata":{"id":"l_TI26Q4Y150","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T02:12:33.746384Z","iopub.execute_input":"2025-09-03T02:12:33.746624Z","iopub.status.idle":"2025-09-03T02:12:33.761288Z","shell.execute_reply.started":"2025-09-03T02:12:33.746602Z","shell.execute_reply":"2025-09-03T02:12:33.760590Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n    model.eval()\n    with torch.no_grad():\n        train_loss = calc_loss_loader(\n            train_loader, model, device, num_batches=eval_iter\n        )\n        val_loss = calc_loss_loader(\n            val_loader, model, device, num_batches=eval_iter\n        )\n\n    model.train()\n    return train_loss, val_loss\n\n\ndef generate_and_print_sample(model, tokenizer, device, start_context):\n    model.eval()\n    if hasattr(model, 'pos_embeddings'):\n      context_size = model.pos_embeddings.weight.shape[0]\n    elif hasattr(model, 'pos_encoding'):\n      context_size = model.pos_encoding.pe.size(1)\n    else:\n      # Fallback padr√£o\n      context_size = 256\n      print(f\"Usando context_size padr√£o: {context_size}\")\n    encoded = text_to_token_ids(start_context, tokenizer, device).to(device)\n    with torch.no_grad():\n        token_ids = generate_text(\n            model=model,\n            idx=encoded,\n            max_new_tokens=50,\n            context_size=context_size\n        )\n\n    decoded_text = token_ids_to_text(token_ids, tokenizer, device)\n    print(decoded_text.replace(\"\\n\", \" \"))\n    model.train()\n\n\ndef train_model_simple(\n        wandb_run, model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer,\n        save_freq_wdb, file_name, save_wdb, state_dict, project, name, start_time\n):\n    train_losses, val_losses, track_tokens_seen = [], [], []\n    tokens_seen, global_step = 0, -1\n\n    epochs_complete = state_dict.get(\"epoch\", 0)\n    batchs_complete = state_dict.get(\"batch\", 0)\n    accumulated_time = state_dict.get(\"train_time\", 0)\n\n    for epoch in range(epochs_complete, num_epochs):\n        model.train()\n\n        for batch_idx, (input_batch, target_batch) in enumerate(train_loader):\n            if epoch == epochs_complete and batch_idx < batchs_complete:\n                continue\n\n            optimizer.zero_grad()\n            loss = calc_loss_batch_by_cross_entropy(\n                model,\n                input_batch,\n                target_batch,\n                device\n            )\n            loss.backward()\n            optimizer.step()\n            tokens_seen += input_batch.numel()\n            global_step += 1\n\n            if global_step % eval_freq == 0:\n                train_loss, val_loss = evaluate_model(\n                    model,\n                    train_loader,\n                    val_loader,\n                    device,\n                    eval_iter\n                )\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n\n                wandb_run.log({\n                    \"train_loss\": train_loss,\n                    \"val_loss\": val_loss,\n                    \"tokens_seen\": tokens_seen,\n                    \"epoch\": epoch + 1,\n                    \"global_step\": global_step,\n                })\n\n                print(\n                    f\"Ep {epoch+1} (Step {global_step:06d}): \"\n                    f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\"\n                )\n\n            # salva peso s√≥ ap√≥s o primeiro batch\n            if save_wdb and global_step > 0 and global_step % save_freq_wdb == 0:\n                elapsed_time = time.time() - start_time + accumulated_time\n                torch.save({\n                    \"epoch\": epoch,\n                    \"model_state\": model.state_dict(),\n                    \"optimizer_state\": optimizer.state_dict(),\n                    \"batch\": batch_idx,\n                    \"train_time\": elapsed_time\n                },  file_name)\n                # wandb.init(project=project, name=name)\n                # wandb.save(file_name)\n                #artifact = wandb.Artifact(file_name.split(\".\")[0], type='model')\n                artifact = wandb.Artifact(name, type=\"model\")\n                artifact.add_file(file_name)\n                wandb_run.log_artifact(artifact)\n                #wandb.finish()\n        print(\"\\nEXEMPLO DE GERA√á√ÉO:\")\n        generate_and_print_sample(model, tokenizer, device, start_context)\n\n        # salva ap√≥s finalizar o treino\n        elapsed_time = time.time() - start_time + accumulated_time\n        if save_wdb:\n          torch.save({\n              \"epoch\": epoch,\n              \"model_state\": model.state_dict(),\n              \"optimizer_state\": optimizer.state_dict(),\n              \"batch\": batch_idx,\n              \"train_time\": elapsed_time\n          },  file_name)\n          artifact = wandb.Artifact(file_name.split(\".\")[0] + \"_final\", type=\"model\")\n          artifact.add_file(file_name)\n          wandb_run.log_artifact(artifact)\n\n    return train_losses, val_losses, track_tokens_seen, elapsed_time","metadata":{"id":"2dBoLM36ycCT","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T02:12:33.762063Z","iopub.execute_input":"2025-09-03T02:12:33.762397Z","iopub.status.idle":"2025-09-03T02:12:33.776118Z","shell.execute_reply.started":"2025-09-03T02:12:33.762369Z","shell.execute_reply":"2025-09-03T02:12:33.775442Z"}},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"## Treinamento dos Modelos + Prova de Converg√™ncia","metadata":{"id":"7GVucSbmiwBA"}},{"cell_type":"markdown","source":"### GPT2: Original","metadata":{"id":"5Y39TV3Ni1i5"}},{"cell_type":"code","source":"torch.manual_seed(123)\n\nconfig = {\n    \"vocab_size\": tokenizer.n_vocab,\n    \"embedding_dim\": 512,\n    \"context_length\": 250,\n    \"num_layers\": 8,\n    \"num_heads\": 8,\n    \"bias\": False,\n    \"max_epochs\": 1,\n    \"dtype\": torch.float32,\n    \"save_wdb\": True,\n    \"save_freq_wdb\": 8000,\n    \"user\": \"levi-pereira-junior-ufcg\",\n    \"project\": \"gpt2\",\n    \"name\": \"gpt2original\",\n    \"run_id\": \"gpt2original-run1\",\n    \"version\": \"v0\",\n    \"file_name\": \"mini_mlp.pth\"\n}\n\n# Inicializa modelo\nmodel = GPT2ModelOriginal(config, device).to(device=device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n\nparams = sum(p.numel() for p in model.parameters())\nparams_gpt2 = params - sum(p.numel() for p in model.out_head.parameters())\nprint(f\"N√∫mero de par√¢metros (sem head): {params_gpt2:,}\")\n\n# Medir tokens/s\ntorch.cuda.reset_peak_memory_stats(device)\n\nstart_time = time.time()\ntokens_processed, total_train_time = train_model(model=model, optimizer=optimizer, config=config)\nend_time = time.time()\n\nelapsed = end_time - start_time\ntokens_per_sec = tokens_processed / elapsed\nmax_memory = torch.cuda.max_memory_allocated(device) / (1024**2)  # MB\n\nprint(\"\\nDESEMPENHO:\")\nprint(f\"Tempo total: {elapsed:.2f} s\")\nprint(f\"Tokens/s: {tokens_per_sec:.2f}\")\nprint(f\"Mem√≥ria m√°xima: {max_memory:.2f} MB\")\n\nprint(f\"\\nTempo total de treino: {total_train_time:.2f} s ({total_train_time/60:.2f} min)\")","metadata":{"id":"rVzexXBPzs1C","trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"2f7446f1-9614-4572-ccf4-33a5e4ddcc98","execution":{"iopub.status.busy":"2025-09-03T02:12:33.776851Z","iopub.execute_input":"2025-09-03T02:12:33.777070Z","iopub.status.idle":"2025-09-03T07:14:50.343266Z","shell.execute_reply.started":"2025-09-03T02:12:33.777046Z","shell.execute_reply":"2025-09-03T07:14:50.342548Z"}},"outputs":[{"name":"stdout","text":"N√∫mero de par√¢metros (sem head): 127,720,960\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlevi-pereira-junior\u001b[0m (\u001b[33mlevi-pereira-junior-ufcg\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250903_021246-gpt2original-run1</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/levi-pereira-junior-ufcg/gpt2/runs/gpt2original-run1' target=\"_blank\">gpt2original</a></strong> to <a href='https://wandb.ai/levi-pereira-junior-ufcg/gpt2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/levi-pereira-junior-ufcg/gpt2' target=\"_blank\">https://wandb.ai/levi-pereira-junior-ufcg/gpt2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/levi-pereira-junior-ufcg/gpt2/runs/gpt2original-run1' target=\"_blank\">https://wandb.ai/levi-pereira-junior-ufcg/gpt2/runs/gpt2original-run1</a>"},"metadata":{}},{"name":"stdout","text":"Fetch Error: artifact 'gpt2original:v0' not found in 'levi-pereira-junior-ufcg/gpt2'\n\nEPOCHS/BATCHS RECUPERADOS:  {}\nEp 1 (Step 000000): Train loss 59.219, Val loss 59.127\nEp 1 (Step 000005): Train loss 51.988, Val loss 51.653\nEp 1 (Step 000010): Train loss 46.162, Val loss 45.797\nEp 1 (Step 000015): Train loss 41.094, Val loss 40.791\nEp 1 (Step 000020): Train loss 38.673, Val loss 38.270\nEp 1 (Step 000025): Train loss 37.304, Val loss 37.257\nEp 1 (Step 000030): Train loss 37.075, Val loss 37.263\nEp 1 (Step 000035): Train loss 36.622, Val loss 37.366\nEp 1 (Step 000040): Train loss 37.558, Val loss 36.906\nEp 1 (Step 000045): Train loss 36.666, Val loss 36.708\nEp 1 (Step 000050): Train loss 36.341, Val loss 36.862\nEp 1 (Step 000055): Train loss 36.009, Val loss 36.540\nEp 1 (Step 000060): Train loss 36.708, Val loss 36.038\nEp 1 (Step 000065): Train loss 35.080, Val loss 35.611\nEp 1 (Step 000070): Train loss 36.031, Val loss 35.914\nEp 1 (Step 000075): Train loss 35.444, Val loss 35.373\nEp 1 (Step 000080): Train loss 34.850, Val loss 34.859\nEp 1 (Step 000085): Train loss 34.947, Val loss 35.095\nEp 1 (Step 000090): Train loss 34.723, Val loss 34.887\nEp 1 (Step 000095): Train loss 34.484, Val loss 34.411\nEp 1 (Step 000100): Train loss 33.403, Val loss 33.868\nEp 1 (Step 000105): Train loss 33.332, Val loss 34.078\nEp 1 (Step 000110): Train loss 32.866, Val loss 33.413\nEp 1 (Step 000115): Train loss 32.966, Val loss 33.226\nEp 1 (Step 000120): Train loss 32.497, Val loss 33.084\nEp 1 (Step 000125): Train loss 32.278, Val loss 33.002\nEp 1 (Step 000130): Train loss 32.821, Val loss 32.588\nEp 1 (Step 000135): Train loss 31.503, Val loss 32.943\nEp 1 (Step 000140): Train loss 31.318, Val loss 32.257\nEp 1 (Step 000145): Train loss 31.654, Val loss 32.281\nEp 1 (Step 000150): Train loss 30.825, Val loss 32.311\nEp 1 (Step 000155): Train loss 30.932, Val loss 32.000\nEp 1 (Step 000160): Train loss 30.899, Val loss 31.958\nEp 1 (Step 000165): Train loss 30.138, Val loss 31.319\nEp 1 (Step 000170): Train loss 30.114, Val loss 31.275\nEp 1 (Step 000175): Train loss 29.971, Val loss 31.425\nEp 1 (Step 000180): Train loss 30.013, Val loss 30.840\nEp 1 (Step 000185): Train loss 29.677, Val loss 31.552\nEp 1 (Step 000190): Train loss 29.473, Val loss 31.090\nEp 1 (Step 000195): Train loss 29.147, Val loss 31.166\nEp 1 (Step 000200): Train loss 29.275, Val loss 30.723\nEp 1 (Step 000205): Train loss 29.401, Val loss 31.042\nEp 1 (Step 000210): Train loss 28.474, Val loss 30.509\nEp 1 (Step 000215): Train loss 28.482, Val loss 30.111\nEp 1 (Step 000220): Train loss 28.946, Val loss 30.418\nEp 1 (Step 000225): Train loss 28.088, Val loss 30.099\nEp 1 (Step 000230): Train loss 28.203, Val loss 29.703\nEp 1 (Step 000235): Train loss 27.722, Val loss 30.226\nEp 1 (Step 000240): Train loss 28.165, Val loss 29.952\nEp 1 (Step 000245): Train loss 28.418, Val loss 29.759\nEp 1 (Step 000250): Train loss 27.423, Val loss 30.028\nEp 1 (Step 000255): Train loss 27.106, Val loss 29.482\nEp 1 (Step 000260): Train loss 27.015, Val loss 29.684\nEp 1 (Step 000265): Train loss 27.158, Val loss 30.049\nEp 1 (Step 000270): Train loss 27.105, Val loss 29.463\nEp 1 (Step 000275): Train loss 27.684, Val loss 29.282\nEp 1 (Step 000280): Train loss 27.034, Val loss 28.865\nEp 1 (Step 000285): Train loss 27.039, Val loss 29.186\nEp 1 (Step 000290): Train loss 26.953, Val loss 29.448\nEp 1 (Step 000295): Train loss 26.849, Val loss 29.143\nEp 1 (Step 000300): Train loss 26.384, Val loss 28.939\nEp 1 (Step 000305): Train loss 25.794, Val loss 29.239\nEp 1 (Step 000310): Train loss 26.673, Val loss 28.929\nEp 1 (Step 000315): Train loss 26.218, Val loss 28.715\nEp 1 (Step 000320): Train loss 25.717, Val loss 28.825\nEp 1 (Step 000325): Train loss 25.603, Val loss 29.057\nEp 1 (Step 000330): Train loss 25.824, Val loss 28.994\nEp 1 (Step 000335): Train loss 26.135, Val loss 28.456\nEp 1 (Step 000340): Train loss 25.653, Val loss 29.091\nEp 1 (Step 000345): Train loss 25.173, Val loss 28.593\nEp 1 (Step 000350): Train loss 25.896, Val loss 28.591\nEp 1 (Step 000355): Train loss 25.130, Val loss 28.432\nEp 1 (Step 000360): Train loss 25.583, Val loss 28.701\nEp 1 (Step 000365): Train loss 25.519, Val loss 28.459\nEp 1 (Step 000370): Train loss 24.677, Val loss 28.421\nEp 1 (Step 000375): Train loss 24.680, Val loss 28.396\nEp 1 (Step 000380): Train loss 25.089, Val loss 28.644\nEp 1 (Step 000385): Train loss 25.290, Val loss 28.582\nEp 1 (Step 000390): Train loss 25.127, Val loss 28.662\nEp 1 (Step 000395): Train loss 24.838, Val loss 28.077\nEp 1 (Step 000400): Train loss 24.472, Val loss 27.748\nEp 1 (Step 000405): Train loss 24.347, Val loss 28.417\nEp 1 (Step 000410): Train loss 25.012, Val loss 28.147\nEp 1 (Step 000415): Train loss 24.039, Val loss 28.438\nEp 1 (Step 000420): Train loss 24.142, Val loss 28.252\nEp 1 (Step 000425): Train loss 24.434, Val loss 28.176\nEp 1 (Step 000430): Train loss 24.100, Val loss 28.024\nEp 1 (Step 000435): Train loss 23.665, Val loss 28.150\nEp 1 (Step 000440): Train loss 24.622, Val loss 28.295\nEp 1 (Step 000445): Train loss 24.148, Val loss 28.318\nEp 1 (Step 000450): Train loss 23.940, Val loss 27.958\nEp 1 (Step 000455): Train loss 24.131, Val loss 28.169\nEp 1 (Step 000460): Train loss 24.153, Val loss 28.287\nEp 1 (Step 000465): Train loss 23.564, Val loss 28.102\nEp 1 (Step 000470): Train loss 23.858, Val loss 27.607\nEp 1 (Step 000475): Train loss 23.804, Val loss 28.400\nEp 1 (Step 000480): Train loss 23.370, Val loss 27.981\nEp 1 (Step 000485): Train loss 23.396, Val loss 28.075\nEp 1 (Step 000490): Train loss 23.297, Val loss 27.805\nEp 1 (Step 000495): Train loss 23.899, Val loss 27.893\nEp 1 (Step 000500): Train loss 22.936, Val loss 28.442\nEp 1 (Step 000505): Train loss 23.467, Val loss 27.731\nEp 1 (Step 000510): Train loss 23.340, Val loss 28.198\nEp 1 (Step 000515): Train loss 23.308, Val loss 27.877\nEp 1 (Step 000520): Train loss 22.765, Val loss 27.724\nEp 1 (Step 000525): Train loss 23.254, Val loss 28.031\nEp 1 (Step 000530): Train loss 22.878, Val loss 27.783\nEp 1 (Step 000535): Train loss 23.165, Val loss 28.038\nEp 1 (Step 000540): Train loss 22.600, Val loss 28.304\nEp 1 (Step 000545): Train loss 23.083, Val loss 27.525\nEp 1 (Step 000550): Train loss 22.794, Val loss 27.579\nEp 1 (Step 000555): Train loss 22.511, Val loss 27.757\nEp 1 (Step 000560): Train loss 22.359, Val loss 27.774\nEp 1 (Step 000565): Train loss 22.539, Val loss 27.819\nEp 1 (Step 000570): Train loss 22.769, Val loss 27.788\nEp 1 (Step 000575): Train loss 22.999, Val loss 28.094\nEp 1 (Step 000580): Train loss 22.568, Val loss 27.535\nEp 1 (Step 000585): Train loss 22.209, Val loss 28.061\nEp 1 (Step 000590): Train loss 22.307, Val loss 27.768\nEp 1 (Step 000595): Train loss 22.797, Val loss 27.680\nEp 1 (Step 000600): Train loss 22.097, Val loss 27.909\nEp 1 (Step 000605): Train loss 22.067, Val loss 28.170\nEp 1 (Step 000610): Train loss 22.203, Val loss 27.540\nEp 1 (Step 000615): Train loss 22.254, Val loss 27.482\nEp 1 (Step 000620): Train loss 21.871, Val loss 27.454\nEp 1 (Step 000625): Train loss 21.684, Val loss 27.617\nEp 1 (Step 000630): Train loss 21.448, Val loss 27.850\nEp 1 (Step 000635): Train loss 21.804, Val loss 27.153\nEp 1 (Step 000640): Train loss 22.043, Val loss 27.887\nEp 1 (Step 000645): Train loss 21.811, Val loss 27.943\nEp 1 (Step 000650): Train loss 22.060, Val loss 27.214\nEp 1 (Step 000655): Train loss 22.196, Val loss 27.714\nEp 1 (Step 000660): Train loss 21.815, Val loss 28.017\nEp 1 (Step 000665): Train loss 21.231, Val loss 27.213\nEp 1 (Step 000670): Train loss 21.958, Val loss 27.695\nEp 1 (Step 000675): Train loss 21.876, Val loss 27.588\nEp 1 (Step 000680): Train loss 21.342, Val loss 27.308\nEp 1 (Step 000685): Train loss 21.649, Val loss 27.452\nEp 1 (Step 000690): Train loss 21.380, Val loss 27.564\nEp 1 (Step 000695): Train loss 21.647, Val loss 28.004\nEp 1 (Step 000700): Train loss 21.855, Val loss 27.505\nEp 1 (Step 000705): Train loss 22.092, Val loss 27.712\nEp 1 (Step 000710): Train loss 21.623, Val loss 27.550\nEp 1 (Step 000715): Train loss 21.555, Val loss 27.168\nEp 1 (Step 000720): Train loss 21.001, Val loss 27.786\nEp 1 (Step 000725): Train loss 20.169, Val loss 27.424\nEp 1 (Step 000730): Train loss 21.355, Val loss 27.618\nEp 1 (Step 000735): Train loss 20.781, Val loss 27.422\nEp 1 (Step 000740): Train loss 21.068, Val loss 27.342\nEp 1 (Step 000745): Train loss 21.070, Val loss 27.489\nEp 1 (Step 000750): Train loss 21.223, Val loss 27.508\nEp 1 (Step 000755): Train loss 20.438, Val loss 27.455\nEp 1 (Step 000760): Train loss 20.062, Val loss 27.396\nEp 1 (Step 000765): Train loss 20.586, Val loss 27.881\nEp 1 (Step 000770): Train loss 20.815, Val loss 26.932\nEp 1 (Step 000775): Train loss 21.170, Val loss 27.673\nEp 1 (Step 000780): Train loss 20.553, Val loss 27.493\nEp 1 (Step 000785): Train loss 20.342, Val loss 27.450\nEp 1 (Step 000790): Train loss 21.236, Val loss 27.844\nEp 1 (Step 000795): Train loss 20.075, Val loss 27.811\nEp 1 (Step 000800): Train loss 20.160, Val loss 27.453\nEp 1 (Step 000805): Train loss 20.715, Val loss 27.864\nEp 1 (Step 000810): Train loss 20.114, Val loss 27.709\nEp 1 (Step 000815): Train loss 20.295, Val loss 27.272\nEp 1 (Step 000820): Train loss 20.529, Val loss 27.615\nEp 1 (Step 000825): Train loss 20.401, Val loss 27.426\nEp 1 (Step 000830): Train loss 20.802, Val loss 27.747\nEp 1 (Step 000835): Train loss 21.034, Val loss 27.839\nEp 1 (Step 000840): Train loss 19.216, Val loss 27.214\nEp 1 (Step 000845): Train loss 20.444, Val loss 27.223\nEp 1 (Step 000850): Train loss 20.479, Val loss 27.451\nEp 1 (Step 000855): Train loss 20.175, Val loss 27.831\nEp 1 (Step 000860): Train loss 19.894, Val loss 26.881\nEp 1 (Step 000865): Train loss 19.945, Val loss 27.517\nEp 1 (Step 000870): Train loss 19.089, Val loss 27.346\nEp 1 (Step 000875): Train loss 19.848, Val loss 27.320\nEp 1 (Step 000880): Train loss 20.143, Val loss 27.520\nEp 1 (Step 000885): Train loss 19.888, Val loss 27.654\nEp 1 (Step 000890): Train loss 19.461, Val loss 27.357\nEp 1 (Step 000895): Train loss 20.041, Val loss 27.608\nEp 1 (Step 000900): Train loss 19.211, Val loss 27.953\nEp 1 (Step 000905): Train loss 19.299, Val loss 27.457\nEp 1 (Step 000910): Train loss 20.263, Val loss 27.778\nEp 1 (Step 000915): Train loss 20.068, Val loss 27.227\nEp 1 (Step 000920): Train loss 19.722, Val loss 27.291\nEp 1 (Step 000925): Train loss 19.367, Val loss 27.196\nEp 1 (Step 000930): Train loss 19.563, Val loss 28.011\nEp 1 (Step 000935): Train loss 19.639, Val loss 26.822\nEp 1 (Step 000940): Train loss 19.402, Val loss 27.037\nEp 1 (Step 000945): Train loss 19.267, Val loss 27.304\nEp 1 (Step 000950): Train loss 19.037, Val loss 27.270\nEp 1 (Step 000955): Train loss 19.766, Val loss 27.780\nEp 1 (Step 000960): Train loss 19.734, Val loss 27.105\nEp 1 (Step 000965): Train loss 18.981, Val loss 27.723\nEp 1 (Step 000970): Train loss 18.690, Val loss 27.468\nEp 1 (Step 000975): Train loss 19.114, Val loss 27.804\nEp 1 (Step 000980): Train loss 18.655, Val loss 27.531\nEp 1 (Step 000985): Train loss 19.109, Val loss 27.686\nEp 1 (Step 000990): Train loss 18.200, Val loss 27.474\nEp 1 (Step 000995): Train loss 19.194, Val loss 28.039\nEp 1 (Step 001000): Train loss 18.928, Val loss 27.933\nEp 1 (Step 001005): Train loss 18.604, Val loss 27.240\nEp 1 (Step 001010): Train loss 18.766, Val loss 27.570\nEp 1 (Step 001015): Train loss 18.020, Val loss 27.958\nEp 1 (Step 001020): Train loss 18.587, Val loss 27.772\nEp 1 (Step 001025): Train loss 18.692, Val loss 27.748\nEp 1 (Step 001030): Train loss 18.395, Val loss 27.625\nEp 1 (Step 001035): Train loss 18.229, Val loss 27.918\nEp 1 (Step 001040): Train loss 18.459, Val loss 27.278\nEp 1 (Step 001045): Train loss 18.205, Val loss 27.548\nEp 1 (Step 001050): Train loss 18.694, Val loss 27.592\nEp 1 (Step 001055): Train loss 18.028, Val loss 27.955\nEp 1 (Step 001060): Train loss 18.558, Val loss 27.867\nEp 1 (Step 001065): Train loss 18.112, Val loss 27.665\nEp 1 (Step 001070): Train loss 17.334, Val loss 27.841\nEp 1 (Step 001075): Train loss 18.623, Val loss 27.373\nEp 1 (Step 001080): Train loss 18.269, Val loss 27.532\nEp 1 (Step 001085): Train loss 17.657, Val loss 27.676\nEp 1 (Step 001090): Train loss 17.887, Val loss 27.712\nEp 1 (Step 001095): Train loss 18.580, Val loss 27.507\nEp 1 (Step 001100): Train loss 18.482, Val loss 27.851\nEp 1 (Step 001105): Train loss 18.347, Val loss 27.591\nEp 1 (Step 001110): Train loss 17.742, Val loss 27.650\nEp 1 (Step 001115): Train loss 18.218, Val loss 27.329\nEp 1 (Step 001120): Train loss 17.619, Val loss 27.873\nEp 1 (Step 001125): Train loss 17.792, Val loss 27.771\nEp 1 (Step 001130): Train loss 18.124, Val loss 27.432\nEp 1 (Step 001135): Train loss 17.763, Val loss 27.709\nEp 1 (Step 001140): Train loss 17.544, Val loss 28.125\nEp 1 (Step 001145): Train loss 18.372, Val loss 28.092\nEp 1 (Step 001150): Train loss 17.794, Val loss 27.499\nEp 1 (Step 001155): Train loss 18.558, Val loss 27.808\nEp 1 (Step 001160): Train loss 17.408, Val loss 27.864\nEp 1 (Step 001165): Train loss 17.464, Val loss 27.930\nEp 1 (Step 001170): Train loss 17.887, Val loss 27.598\nEp 1 (Step 001175): Train loss 17.992, Val loss 28.198\nEp 1 (Step 001180): Train loss 17.406, Val loss 27.516\nEp 1 (Step 001185): Train loss 16.929, Val loss 27.510\nEp 1 (Step 001190): Train loss 16.694, Val loss 27.591\nEp 1 (Step 001195): Train loss 17.276, Val loss 28.028\nEp 1 (Step 001200): Train loss 16.563, Val loss 28.322\nEp 1 (Step 001205): Train loss 17.086, Val loss 27.819\nEp 1 (Step 001210): Train loss 16.221, Val loss 27.775\nEp 1 (Step 001215): Train loss 17.436, Val loss 28.104\nEp 1 (Step 001220): Train loss 16.907, Val loss 27.962\nEp 1 (Step 001225): Train loss 17.219, Val loss 27.944\nEp 1 (Step 001230): Train loss 16.312, Val loss 27.253\nEp 1 (Step 001235): Train loss 16.397, Val loss 28.125\nEp 1 (Step 001240): Train loss 17.122, Val loss 28.312\nEp 1 (Step 001245): Train loss 17.604, Val loss 27.948\nEp 1 (Step 001250): Train loss 16.082, Val loss 27.869\nEp 1 (Step 001255): Train loss 17.272, Val loss 28.170\nEp 1 (Step 001260): Train loss 16.734, Val loss 27.815\nEp 1 (Step 001265): Train loss 16.478, Val loss 28.158\nEp 1 (Step 001270): Train loss 16.280, Val loss 28.231\nEp 1 (Step 001275): Train loss 16.660, Val loss 28.186\nEp 1 (Step 001280): Train loss 16.312, Val loss 28.475\nEp 1 (Step 001285): Train loss 16.768, Val loss 27.998\nEp 1 (Step 001290): Train loss 16.503, Val loss 27.929\nEp 1 (Step 001295): Train loss 16.669, Val loss 28.021\nEp 1 (Step 001300): Train loss 15.421, Val loss 28.390\nEp 1 (Step 001305): Train loss 16.541, Val loss 28.519\nEp 1 (Step 001310): Train loss 16.985, Val loss 28.300\nEp 1 (Step 001315): Train loss 15.535, Val loss 28.273\nEp 1 (Step 001320): Train loss 15.451, Val loss 27.802\nEp 1 (Step 001325): Train loss 16.358, Val loss 28.245\nEp 1 (Step 001330): Train loss 15.645, Val loss 28.438\nEp 1 (Step 001335): Train loss 15.586, Val loss 28.271\nEp 1 (Step 001340): Train loss 16.325, Val loss 28.548\nEp 1 (Step 001345): Train loss 16.074, Val loss 28.570\nEp 1 (Step 001350): Train loss 15.982, Val loss 28.052\nEp 1 (Step 001355): Train loss 15.442, Val loss 28.667\nEp 1 (Step 001360): Train loss 15.599, Val loss 28.134\nEp 1 (Step 001365): Train loss 15.329, Val loss 28.721\nEp 1 (Step 001370): Train loss 16.191, Val loss 28.622\nEp 1 (Step 001375): Train loss 15.058, Val loss 28.643\nEp 1 (Step 001380): Train loss 14.295, Val loss 28.594\nEp 1 (Step 001385): Train loss 15.759, Val loss 28.416\nEp 1 (Step 001390): Train loss 14.922, Val loss 28.408\nEp 1 (Step 001395): Train loss 15.006, Val loss 28.978\nEp 1 (Step 001400): Train loss 15.370, Val loss 28.444\nEp 1 (Step 001405): Train loss 15.001, Val loss 28.789\nEp 1 (Step 001410): Train loss 14.402, Val loss 28.961\nEp 1 (Step 001415): Train loss 15.259, Val loss 28.198\nEp 1 (Step 001420): Train loss 15.042, Val loss 28.152\nEp 1 (Step 001425): Train loss 15.181, Val loss 28.256\nEp 1 (Step 001430): Train loss 14.706, Val loss 28.451\nEp 1 (Step 001435): Train loss 14.074, Val loss 28.908\nEp 1 (Step 001440): Train loss 14.419, Val loss 28.893\nEp 1 (Step 001445): Train loss 13.686, Val loss 28.713\nEp 1 (Step 001450): Train loss 14.963, Val loss 28.911\nEp 1 (Step 001455): Train loss 14.755, Val loss 29.066\nEp 1 (Step 001460): Train loss 14.495, Val loss 29.084\nEp 1 (Step 001465): Train loss 14.183, Val loss 29.080\nEp 1 (Step 001470): Train loss 13.646, Val loss 28.709\nEp 1 (Step 001475): Train loss 13.430, Val loss 28.565\nEp 1 (Step 001480): Train loss 13.970, Val loss 28.804\nEp 1 (Step 001485): Train loss 13.671, Val loss 28.919\nEp 1 (Step 001490): Train loss 14.206, Val loss 29.266\nEp 1 (Step 001495): Train loss 14.611, Val loss 29.161\nEp 1 (Step 001500): Train loss 13.466, Val loss 28.836\nEp 1 (Step 001505): Train loss 14.264, Val loss 29.292\nEp 1 (Step 001510): Train loss 13.455, Val loss 29.418\nEp 1 (Step 001515): Train loss 13.389, Val loss 29.199\nEp 1 (Step 001520): Train loss 13.241, Val loss 29.447\nEp 1 (Step 001525): Train loss 13.624, Val loss 29.123\nEp 1 (Step 001530): Train loss 13.424, Val loss 29.054\nEp 1 (Step 001535): Train loss 14.031, Val loss 28.925\nEp 1 (Step 001540): Train loss 13.734, Val loss 29.027\nEp 1 (Step 001545): Train loss 13.744, Val loss 28.836\nEp 1 (Step 001550): Train loss 13.402, Val loss 29.446\nEp 1 (Step 001555): Train loss 12.885, Val loss 29.006\nEp 1 (Step 001560): Train loss 12.918, Val loss 28.898\nEp 1 (Step 001565): Train loss 12.447, Val loss 28.859\nEp 1 (Step 001570): Train loss 13.179, Val loss 29.066\nEp 1 (Step 001575): Train loss 13.211, Val loss 29.416\nEp 1 (Step 001580): Train loss 11.795, Val loss 29.447\nEp 1 (Step 001585): Train loss 12.355, Val loss 29.689\nEp 1 (Step 001590): Train loss 12.519, Val loss 28.720\nEp 1 (Step 001595): Train loss 12.430, Val loss 29.574\nEp 1 (Step 001600): Train loss 13.054, Val loss 29.723\nEp 1 (Step 001605): Train loss 13.131, Val loss 29.828\nEp 1 (Step 001610): Train loss 12.004, Val loss 29.416\nEp 1 (Step 001615): Train loss 12.849, Val loss 29.571\nEp 1 (Step 001620): Train loss 12.208, Val loss 29.593\nEp 1 (Step 001625): Train loss 12.559, Val loss 29.593\nEp 1 (Step 001630): Train loss 12.736, Val loss 29.317\nEp 1 (Step 001635): Train loss 13.240, Val loss 29.501\nEp 1 (Step 001640): Train loss 11.372, Val loss 29.364\nEp 1 (Step 001645): Train loss 12.477, Val loss 29.804\nEp 1 (Step 001650): Train loss 11.866, Val loss 29.500\nEp 1 (Step 001655): Train loss 11.998, Val loss 29.600\nEp 1 (Step 001660): Train loss 12.231, Val loss 29.664\nEp 1 (Step 001665): Train loss 11.872, Val loss 29.860\nEp 1 (Step 001670): Train loss 11.429, Val loss 29.536\nEp 1 (Step 001675): Train loss 11.807, Val loss 29.753\nEp 1 (Step 001680): Train loss 11.090, Val loss 29.759\nEp 1 (Step 001685): Train loss 10.971, Val loss 30.015\nEp 1 (Step 001690): Train loss 10.749, Val loss 29.680\nEp 1 (Step 001695): Train loss 11.870, Val loss 29.509\nEp 1 (Step 001700): Train loss 11.454, Val loss 30.292\nEp 1 (Step 001705): Train loss 11.268, Val loss 29.753\nEp 1 (Step 001710): Train loss 11.634, Val loss 29.907\nEp 1 (Step 001715): Train loss 11.386, Val loss 29.760\nEp 1 (Step 001720): Train loss 10.672, Val loss 30.542\nEp 1 (Step 001725): Train loss 11.609, Val loss 29.759\nEp 1 (Step 001730): Train loss 10.416, Val loss 30.415\nEp 1 (Step 001735): Train loss 10.229, Val loss 30.640\nEp 1 (Step 001740): Train loss 10.795, Val loss 30.085\nEp 1 (Step 001745): Train loss 10.822, Val loss 30.206\nEp 1 (Step 001750): Train loss 10.095, Val loss 30.550\nEp 1 (Step 001755): Train loss 10.937, Val loss 30.817\nEp 1 (Step 001760): Train loss 11.060, Val loss 30.459\nEp 1 (Step 001765): Train loss 10.347, Val loss 31.157\nEp 1 (Step 001770): Train loss 10.493, Val loss 30.598\nEp 1 (Step 001775): Train loss 10.455, Val loss 30.578\nEp 1 (Step 001780): Train loss 10.580, Val loss 30.545\nEp 1 (Step 001785): Train loss 10.244, Val loss 30.350\nEp 1 (Step 001790): Train loss 10.329, Val loss 30.678\nEp 1 (Step 001795): Train loss 10.521, Val loss 30.760\nEp 1 (Step 001800): Train loss 10.452, Val loss 30.249\nEp 1 (Step 001805): Train loss 9.710, Val loss 30.678\nEp 1 (Step 001810): Train loss 9.321, Val loss 30.444\nEp 1 (Step 001815): Train loss 9.929, Val loss 31.090\nEp 1 (Step 001820): Train loss 10.176, Val loss 31.268\nEp 1 (Step 001825): Train loss 8.934, Val loss 30.256\nEp 1 (Step 001830): Train loss 10.140, Val loss 30.882\nEp 1 (Step 001835): Train loss 10.055, Val loss 30.937\nEp 1 (Step 001840): Train loss 9.549, Val loss 31.123\nEp 1 (Step 001845): Train loss 9.765, Val loss 30.632\nEp 1 (Step 001850): Train loss 9.404, Val loss 30.951\nEp 1 (Step 001855): Train loss 8.883, Val loss 31.125\nEp 1 (Step 001860): Train loss 9.833, Val loss 31.095\nEp 1 (Step 001865): Train loss 8.885, Val loss 31.411\nEp 1 (Step 001870): Train loss 8.942, Val loss 31.472\nEp 1 (Step 001875): Train loss 9.547, Val loss 31.634\nEp 1 (Step 001880): Train loss 9.469, Val loss 31.382\nEp 1 (Step 001885): Train loss 9.401, Val loss 31.676\nEp 1 (Step 001890): Train loss 8.495, Val loss 31.785\nEp 1 (Step 001895): Train loss 8.559, Val loss 31.215\nEp 1 (Step 001900): Train loss 8.028, Val loss 31.336\nEp 1 (Step 001905): Train loss 8.867, Val loss 31.203\nEp 1 (Step 001910): Train loss 8.918, Val loss 31.610\nEp 1 (Step 001915): Train loss 8.614, Val loss 31.900\nEp 1 (Step 001920): Train loss 9.123, Val loss 31.406\nEp 1 (Step 001925): Train loss 9.120, Val loss 31.450\nEp 1 (Step 001930): Train loss 7.771, Val loss 32.102\nEp 1 (Step 001935): Train loss 8.332, Val loss 32.036\nEp 1 (Step 001940): Train loss 8.690, Val loss 31.702\nEp 1 (Step 001945): Train loss 8.805, Val loss 31.735\nEp 1 (Step 001950): Train loss 7.556, Val loss 31.717\nEp 1 (Step 001955): Train loss 8.275, Val loss 32.140\nEp 1 (Step 001960): Train loss 7.794, Val loss 32.012\nEp 1 (Step 001965): Train loss 7.604, Val loss 31.795\nEp 1 (Step 001970): Train loss 8.012, Val loss 32.333\nEp 1 (Step 001975): Train loss 7.176, Val loss 32.337\nEp 1 (Step 001980): Train loss 7.547, Val loss 31.807\nEp 1 (Step 001985): Train loss 8.591, Val loss 32.309\nEp 1 (Step 001990): Train loss 7.605, Val loss 32.140\nEp 1 (Step 001995): Train loss 7.834, Val loss 32.470\nEp 1 (Step 002000): Train loss 7.889, Val loss 31.964\nEp 1 (Step 002005): Train loss 7.516, Val loss 32.693\nEp 1 (Step 002010): Train loss 8.030, Val loss 32.687\nEp 1 (Step 002015): Train loss 8.413, Val loss 32.767\nEp 1 (Step 002020): Train loss 7.085, Val loss 32.472\nEp 1 (Step 002025): Train loss 7.323, Val loss 31.739\nEp 1 (Step 002030): Train loss 7.918, Val loss 33.120\nEp 1 (Step 002035): Train loss 7.129, Val loss 32.272\nEp 1 (Step 002040): Train loss 7.647, Val loss 32.673\nEp 1 (Step 002045): Train loss 6.943, Val loss 32.848\nEp 1 (Step 002050): Train loss 7.526, Val loss 32.828\nEp 1 (Step 002055): Train loss 7.658, Val loss 33.027\nEp 1 (Step 002060): Train loss 7.083, Val loss 32.152\nEp 1 (Step 002065): Train loss 7.203, Val loss 32.733\nEp 1 (Step 002070): Train loss 6.910, Val loss 31.881\nEp 1 (Step 002075): Train loss 6.816, Val loss 32.987\nEp 1 (Step 002080): Train loss 6.535, Val loss 33.023\nEp 1 (Step 002085): Train loss 7.133, Val loss 33.042\nEp 1 (Step 002090): Train loss 6.438, Val loss 32.706\nEp 1 (Step 002095): Train loss 6.933, Val loss 32.555\nEp 1 (Step 002100): Train loss 6.285, Val loss 33.119\nEp 1 (Step 002105): Train loss 5.949, Val loss 33.400\nEp 1 (Step 002110): Train loss 6.729, Val loss 32.991\nEp 1 (Step 002115): Train loss 6.128, Val loss 32.903\nEp 1 (Step 002120): Train loss 5.787, Val loss 33.446\nEp 1 (Step 002125): Train loss 6.190, Val loss 33.415\nEp 1 (Step 002130): Train loss 6.140, Val loss 33.012\nEp 1 (Step 002135): Train loss 6.551, Val loss 33.590\nEp 1 (Step 002140): Train loss 6.240, Val loss 33.397\nEp 1 (Step 002145): Train loss 6.660, Val loss 33.382\nEp 1 (Step 002150): Train loss 6.118, Val loss 33.367\nEp 1 (Step 002155): Train loss 5.715, Val loss 33.660\nEp 1 (Step 002160): Train loss 6.253, Val loss 32.832\nEp 1 (Step 002165): Train loss 6.299, Val loss 33.419\nEp 1 (Step 002170): Train loss 6.040, Val loss 33.331\nEp 1 (Step 002175): Train loss 6.142, Val loss 33.168\nEp 1 (Step 002180): Train loss 5.571, Val loss 34.045\nEp 1 (Step 002185): Train loss 5.235, Val loss 33.554\nEp 1 (Step 002190): Train loss 5.217, Val loss 33.399\nEp 1 (Step 002195): Train loss 5.684, Val loss 33.456\nEp 1 (Step 002200): Train loss 5.771, Val loss 33.838\nEp 1 (Step 002205): Train loss 5.198, Val loss 33.536\nEp 1 (Step 002210): Train loss 5.284, Val loss 33.890\nEp 1 (Step 002215): Train loss 5.613, Val loss 33.948\nEp 1 (Step 002220): Train loss 5.265, Val loss 34.004\nEp 1 (Step 002225): Train loss 4.883, Val loss 34.195\nEp 1 (Step 002230): Train loss 5.347, Val loss 34.424\nEp 1 (Step 002235): Train loss 5.310, Val loss 34.149\nEp 1 (Step 002240): Train loss 5.028, Val loss 34.164\nEp 1 (Step 002245): Train loss 5.399, Val loss 34.233\nEp 1 (Step 002250): Train loss 4.841, Val loss 34.380\nEp 1 (Step 002255): Train loss 5.291, Val loss 33.882\nEp 1 (Step 002260): Train loss 4.944, Val loss 34.616\nEp 1 (Step 002265): Train loss 4.785, Val loss 34.033\nEp 1 (Step 002270): Train loss 5.249, Val loss 34.827\nEp 1 (Step 002275): Train loss 5.140, Val loss 34.478\nEp 1 (Step 002280): Train loss 4.583, Val loss 34.956\nEp 1 (Step 002285): Train loss 5.079, Val loss 34.601\nEp 1 (Step 002290): Train loss 5.520, Val loss 34.441\nEp 1 (Step 002295): Train loss 5.217, Val loss 34.306\nEp 1 (Step 002300): Train loss 4.546, Val loss 34.325\nEp 1 (Step 002305): Train loss 4.472, Val loss 34.362\nEp 1 (Step 002310): Train loss 4.645, Val loss 34.593\nEp 1 (Step 002315): Train loss 4.509, Val loss 34.290\nEp 1 (Step 002320): Train loss 4.500, Val loss 34.713\nEp 1 (Step 002325): Train loss 4.368, Val loss 35.015\nEp 1 (Step 002330): Train loss 4.489, Val loss 34.716\nEp 1 (Step 002335): Train loss 4.131, Val loss 35.104\nEp 1 (Step 002340): Train loss 4.727, Val loss 34.491\nEp 1 (Step 002345): Train loss 4.437, Val loss 34.595\nEp 1 (Step 002350): Train loss 4.112, Val loss 34.938\nEp 1 (Step 002355): Train loss 4.483, Val loss 34.452\nEp 1 (Step 002360): Train loss 3.819, Val loss 34.573\nEp 1 (Step 002365): Train loss 4.662, Val loss 34.957\nEp 1 (Step 002370): Train loss 4.288, Val loss 34.852\nEp 1 (Step 002375): Train loss 4.693, Val loss 34.498\nEp 1 (Step 002380): Train loss 3.876, Val loss 35.230\nEp 1 (Step 002385): Train loss 4.471, Val loss 34.942\nEp 1 (Step 002390): Train loss 4.759, Val loss 35.012\nEp 1 (Step 002395): Train loss 3.893, Val loss 34.697\nEp 1 (Step 002400): Train loss 3.544, Val loss 35.373\nEp 1 (Step 002405): Train loss 3.636, Val loss 35.599\nEp 1 (Step 002410): Train loss 4.196, Val loss 35.594\nEp 1 (Step 002415): Train loss 4.346, Val loss 35.570\nEp 1 (Step 002420): Train loss 3.661, Val loss 35.412\nEp 1 (Step 002425): Train loss 3.496, Val loss 36.031\nEp 1 (Step 002430): Train loss 3.932, Val loss 35.797\nEp 1 (Step 002435): Train loss 3.630, Val loss 35.506\nEp 1 (Step 002440): Train loss 3.523, Val loss 35.824\nEp 1 (Step 002445): Train loss 3.438, Val loss 35.485\nEp 1 (Step 002450): Train loss 3.477, Val loss 35.433\nEp 1 (Step 002455): Train loss 3.430, Val loss 35.693\nEp 1 (Step 002460): Train loss 3.374, Val loss 35.653\nEp 1 (Step 002465): Train loss 4.022, Val loss 35.314\nEp 1 (Step 002470): Train loss 3.081, Val loss 35.774\nEp 1 (Step 002475): Train loss 3.145, Val loss 35.841\nEp 1 (Step 002480): Train loss 3.239, Val loss 35.719\nEp 1 (Step 002485): Train loss 3.556, Val loss 35.888\nEp 1 (Step 002490): Train loss 3.499, Val loss 35.485\nEp 1 (Step 002495): Train loss 3.693, Val loss 35.774\nEp 1 (Step 002500): Train loss 3.142, Val loss 35.792\nEp 1 (Step 002505): Train loss 2.959, Val loss 35.568\nEp 1 (Step 002510): Train loss 3.475, Val loss 35.622\nEp 1 (Step 002515): Train loss 3.260, Val loss 35.449\nEp 1 (Step 002520): Train loss 2.922, Val loss 35.097\nEp 1 (Step 002525): Train loss 3.442, Val loss 36.415\nEp 1 (Step 002530): Train loss 2.904, Val loss 36.308\nEp 1 (Step 002535): Train loss 3.347, Val loss 36.282\nEp 1 (Step 002540): Train loss 3.206, Val loss 35.619\nEp 1 (Step 002545): Train loss 3.566, Val loss 36.191\nEp 1 (Step 002550): Train loss 3.122, Val loss 35.464\nEp 1 (Step 002555): Train loss 2.891, Val loss 36.566\nEp 1 (Step 002560): Train loss 3.053, Val loss 35.846\nEp 1 (Step 002565): Train loss 2.870, Val loss 36.121\nEp 1 (Step 002570): Train loss 3.005, Val loss 36.430\nEp 1 (Step 002575): Train loss 2.846, Val loss 36.327\nEp 1 (Step 002580): Train loss 3.243, Val loss 35.783\nEp 1 (Step 002585): Train loss 3.257, Val loss 36.450\nEp 1 (Step 002590): Train loss 2.649, Val loss 36.081\nEp 1 (Step 002595): Train loss 2.810, Val loss 36.240\nEp 1 (Step 002600): Train loss 2.646, Val loss 36.094\nEp 1 (Step 002605): Train loss 2.961, Val loss 35.808\nEp 1 (Step 002610): Train loss 3.037, Val loss 36.718\nEp 1 (Step 002615): Train loss 2.984, Val loss 36.787\nEp 1 (Step 002620): Train loss 3.135, Val loss 36.464\nEp 1 (Step 002625): Train loss 2.464, Val loss 36.870\nEp 1 (Step 002630): Train loss 2.836, Val loss 36.222\nEp 1 (Step 002635): Train loss 2.766, Val loss 36.689\nEp 1 (Step 002640): Train loss 2.433, Val loss 35.962\nEp 1 (Step 002645): Train loss 2.662, Val loss 36.131\nEp 1 (Step 002650): Train loss 2.644, Val loss 37.243\nEp 1 (Step 002655): Train loss 2.787, Val loss 36.770\nEp 1 (Step 002660): Train loss 2.618, Val loss 36.528\nEp 1 (Step 002665): Train loss 2.353, Val loss 36.788\nEp 1 (Step 002670): Train loss 2.478, Val loss 36.957\nEp 1 (Step 002675): Train loss 2.534, Val loss 36.536\nEp 1 (Step 002680): Train loss 2.488, Val loss 37.027\nEp 1 (Step 002685): Train loss 2.337, Val loss 36.775\nEp 1 (Step 002690): Train loss 2.278, Val loss 37.195\nEp 1 (Step 002695): Train loss 2.598, Val loss 36.427\nEp 1 (Step 002700): Train loss 2.762, Val loss 37.046\nEp 1 (Step 002705): Train loss 2.586, Val loss 36.848\nEp 1 (Step 002710): Train loss 2.654, Val loss 37.086\nEp 1 (Step 002715): Train loss 2.543, Val loss 36.701\nEp 1 (Step 002720): Train loss 2.522, Val loss 37.422\nEp 1 (Step 002725): Train loss 2.322, Val loss 37.056\nEp 1 (Step 002730): Train loss 2.539, Val loss 37.042\nEp 1 (Step 002735): Train loss 2.352, Val loss 36.636\nEp 1 (Step 002740): Train loss 2.397, Val loss 37.286\nEp 1 (Step 002745): Train loss 2.221, Val loss 36.793\nEp 1 (Step 002750): Train loss 2.354, Val loss 37.576\nEp 1 (Step 002755): Train loss 2.609, Val loss 36.494\nEp 1 (Step 002760): Train loss 2.193, Val loss 37.163\nEp 1 (Step 002765): Train loss 2.317, Val loss 36.777\nEp 1 (Step 002770): Train loss 2.385, Val loss 37.320\nEp 1 (Step 002775): Train loss 2.340, Val loss 37.743\nEp 1 (Step 002780): Train loss 2.318, Val loss 37.496\nEp 1 (Step 002785): Train loss 2.250, Val loss 37.385\nEp 1 (Step 002790): Train loss 2.440, Val loss 37.672\nEp 1 (Step 002795): Train loss 2.103, Val loss 36.475\nEp 1 (Step 002800): Train loss 2.304, Val loss 37.555\nEp 1 (Step 002805): Train loss 2.238, Val loss 36.977\nEp 1 (Step 002810): Train loss 2.200, Val loss 37.320\nEp 1 (Step 002815): Train loss 2.199, Val loss 37.131\nEp 1 (Step 002820): Train loss 2.293, Val loss 36.860\nEp 1 (Step 002825): Train loss 2.119, Val loss 36.858\nEp 1 (Step 002830): Train loss 2.168, Val loss 36.947\nEp 1 (Step 002835): Train loss 2.043, Val loss 37.054\nEp 1 (Step 002840): Train loss 1.986, Val loss 37.402\nEp 1 (Step 002845): Train loss 2.251, Val loss 37.716\nEp 1 (Step 002850): Train loss 2.329, Val loss 37.361\nEp 1 (Step 002855): Train loss 2.449, Val loss 37.938\nEp 1 (Step 002860): Train loss 2.109, Val loss 37.374\nEp 1 (Step 002865): Train loss 1.949, Val loss 37.200\nEp 1 (Step 002870): Train loss 2.038, Val loss 37.940\nEp 1 (Step 002875): Train loss 1.953, Val loss 37.669\nEp 1 (Step 002880): Train loss 2.121, Val loss 37.563\nEp 1 (Step 002885): Train loss 2.303, Val loss 37.526\nEp 1 (Step 002890): Train loss 1.954, Val loss 37.798\nEp 1 (Step 002895): Train loss 2.016, Val loss 37.517\nEp 1 (Step 002900): Train loss 2.020, Val loss 38.025\nEp 1 (Step 002905): Train loss 1.984, Val loss 38.076\nEp 1 (Step 002910): Train loss 1.805, Val loss 37.888\nEp 1 (Step 002915): Train loss 1.990, Val loss 37.031\nEp 1 (Step 002920): Train loss 1.937, Val loss 36.761\nEp 1 (Step 002925): Train loss 2.007, Val loss 38.091\nEp 1 (Step 002930): Train loss 1.921, Val loss 38.275\nEp 1 (Step 002935): Train loss 2.050, Val loss 37.667\nEp 1 (Step 002940): Train loss 2.029, Val loss 38.648\nEp 1 (Step 002945): Train loss 1.861, Val loss 37.601\nEp 1 (Step 002950): Train loss 1.694, Val loss 37.188\nEp 1 (Step 002955): Train loss 2.003, Val loss 37.775\nEp 1 (Step 002960): Train loss 1.789, Val loss 37.293\nEp 1 (Step 002965): Train loss 2.013, Val loss 38.265\nEp 1 (Step 002970): Train loss 1.789, Val loss 37.768\nEp 1 (Step 002975): Train loss 1.933, Val loss 37.214\nEp 1 (Step 002980): Train loss 1.815, Val loss 37.792\nEp 1 (Step 002985): Train loss 1.901, Val loss 38.142\nEp 1 (Step 002990): Train loss 1.893, Val loss 37.282\nEp 1 (Step 002995): Train loss 1.943, Val loss 37.909\nEp 1 (Step 003000): Train loss 2.012, Val loss 38.091\nEp 1 (Step 003005): Train loss 2.194, Val loss 37.747\nEp 1 (Step 003010): Train loss 1.825, Val loss 38.019\nEp 1 (Step 003015): Train loss 1.766, Val loss 38.481\nEp 1 (Step 003020): Train loss 1.807, Val loss 37.896\nEp 1 (Step 003025): Train loss 1.957, Val loss 38.093\nEp 1 (Step 003030): Train loss 1.946, Val loss 37.790\nEp 1 (Step 003035): Train loss 1.666, Val loss 38.434\nEp 1 (Step 003040): Train loss 1.837, Val loss 38.326\nEp 1 (Step 003045): Train loss 1.991, Val loss 37.965\nEp 1 (Step 003050): Train loss 2.224, Val loss 38.167\nEp 1 (Step 003055): Train loss 1.840, Val loss 37.823\nEp 1 (Step 003060): Train loss 1.923, Val loss 38.217\nEp 1 (Step 003065): Train loss 1.935, Val loss 38.090\nEp 1 (Step 003070): Train loss 1.754, Val loss 37.874\nEp 1 (Step 003075): Train loss 1.851, Val loss 37.619\nEp 1 (Step 003080): Train loss 1.802, Val loss 37.860\nEp 1 (Step 003085): Train loss 1.916, Val loss 38.694\nEp 1 (Step 003090): Train loss 1.983, Val loss 37.864\nEp 1 (Step 003095): Train loss 1.884, Val loss 37.790\nEp 1 (Step 003100): Train loss 1.715, Val loss 37.802\nEp 1 (Step 003105): Train loss 1.792, Val loss 38.208\nEp 1 (Step 003110): Train loss 1.846, Val loss 38.436\nEp 1 (Step 003115): Train loss 1.715, Val loss 38.829\nEp 1 (Step 003120): Train loss 1.629, Val loss 38.617\nEp 1 (Step 003125): Train loss 1.831, Val loss 38.130\nEp 1 (Step 003130): Train loss 1.658, Val loss 37.999\nEp 1 (Step 003135): Train loss 1.548, Val loss 38.249\nEp 1 (Step 003140): Train loss 1.893, Val loss 37.935\nEp 1 (Step 003145): Train loss 1.781, Val loss 38.554\nEp 1 (Step 003150): Train loss 1.463, Val loss 38.001\nEp 1 (Step 003155): Train loss 1.637, Val loss 38.008\nEp 1 (Step 003160): Train loss 1.491, Val loss 38.569\nEp 1 (Step 003165): Train loss 1.580, Val loss 38.107\nEp 1 (Step 003170): Train loss 1.684, Val loss 38.428\nEp 1 (Step 003175): Train loss 1.805, Val loss 38.474\nEp 1 (Step 003180): Train loss 1.793, Val loss 38.240\nEp 1 (Step 003185): Train loss 1.698, Val loss 38.351\nEp 1 (Step 003190): Train loss 1.676, Val loss 38.582\nEp 1 (Step 003195): Train loss 1.600, Val loss 38.131\nEp 1 (Step 003200): Train loss 1.624, Val loss 38.124\nEp 1 (Step 003205): Train loss 1.814, Val loss 38.720\nEp 1 (Step 003210): Train loss 1.921, Val loss 39.023\nEp 1 (Step 003215): Train loss 1.703, Val loss 38.660\nEp 1 (Step 003220): Train loss 1.584, Val loss 38.218\nEp 1 (Step 003225): Train loss 1.459, Val loss 38.182\nEp 1 (Step 003230): Train loss 1.541, Val loss 38.271\nEp 1 (Step 003235): Train loss 1.617, Val loss 38.534\nEp 1 (Step 003240): Train loss 1.701, Val loss 38.082\nEp 1 (Step 003245): Train loss 1.550, Val loss 38.567\nEp 1 (Step 003250): Train loss 1.739, Val loss 38.276\nEp 1 (Step 003255): Train loss 1.677, Val loss 38.845\nEp 1 (Step 003260): Train loss 1.661, Val loss 39.255\nEp 1 (Step 003265): Train loss 1.697, Val loss 39.092\nEp 1 (Step 003270): Train loss 1.610, Val loss 38.792\nEp 1 (Step 003275): Train loss 1.753, Val loss 39.104\nEp 1 (Step 003280): Train loss 1.599, Val loss 38.676\nEp 1 (Step 003285): Train loss 1.631, Val loss 38.684\nEp 1 (Step 003290): Train loss 1.590, Val loss 37.981\nEp 1 (Step 003295): Train loss 1.500, Val loss 38.639\nEp 1 (Step 003300): Train loss 1.580, Val loss 38.287\nEp 1 (Step 003305): Train loss 1.619, Val loss 38.915\nEp 1 (Step 003310): Train loss 1.616, Val loss 39.051\nEp 1 (Step 003315): Train loss 1.619, Val loss 38.673\nEp 1 (Step 003320): Train loss 1.605, Val loss 38.615\nEp 1 (Step 003325): Train loss 1.516, Val loss 38.698\nEp 1 (Step 003330): Train loss 1.565, Val loss 38.627\nEp 1 (Step 003335): Train loss 1.458, Val loss 39.148\nEp 1 (Step 003340): Train loss 1.551, Val loss 38.838\nEp 1 (Step 003345): Train loss 1.550, Val loss 38.951\nEp 1 (Step 003350): Train loss 1.450, Val loss 39.072\nEp 1 (Step 003355): Train loss 1.687, Val loss 39.185\nEp 1 (Step 003360): Train loss 1.393, Val loss 39.430\nEp 1 (Step 003365): Train loss 1.554, Val loss 39.268\nEp 1 (Step 003370): Train loss 1.545, Val loss 38.663\nEp 1 (Step 003375): Train loss 1.603, Val loss 38.628\nEp 1 (Step 003380): Train loss 1.524, Val loss 38.172\nEp 1 (Step 003385): Train loss 1.586, Val loss 38.873\nEp 1 (Step 003390): Train loss 1.536, Val loss 38.642\nEp 1 (Step 003395): Train loss 1.587, Val loss 39.181\nEp 1 (Step 003400): Train loss 1.552, Val loss 39.407\nEp 1 (Step 003405): Train loss 1.506, Val loss 38.745\nEp 1 (Step 003410): Train loss 1.544, Val loss 38.698\nEp 1 (Step 003415): Train loss 1.525, Val loss 38.937\nEp 1 (Step 003420): Train loss 1.564, Val loss 38.851\nEp 1 (Step 003425): Train loss 1.447, Val loss 39.153\nEp 1 (Step 003430): Train loss 1.630, Val loss 39.396\nEp 1 (Step 003435): Train loss 1.546, Val loss 38.884\nEp 1 (Step 003440): Train loss 1.345, Val loss 39.236\nEp 1 (Step 003445): Train loss 1.463, Val loss 39.268\nEp 1 (Step 003450): Train loss 1.544, Val loss 39.332\nEp 1 (Step 003455): Train loss 1.688, Val loss 39.076\nEp 1 (Step 003460): Train loss 1.632, Val loss 38.647\nEp 1 (Step 003465): Train loss 1.555, Val loss 39.106\nEp 1 (Step 003470): Train loss 1.441, Val loss 39.714\nEp 1 (Step 003475): Train loss 1.489, Val loss 39.129\nEp 1 (Step 003480): Train loss 1.524, Val loss 38.984\nEp 1 (Step 003485): Train loss 1.446, Val loss 39.340\nEp 1 (Step 003490): Train loss 1.452, Val loss 39.497\nEp 1 (Step 003495): Train loss 1.656, Val loss 38.695\nEp 1 (Step 003500): Train loss 1.322, Val loss 38.880\nEp 1 (Step 003505): Train loss 1.511, Val loss 39.291\nEp 1 (Step 003510): Train loss 1.504, Val loss 38.940\nEp 1 (Step 003515): Train loss 1.453, Val loss 39.663\nEp 1 (Step 003520): Train loss 1.418, Val loss 39.336\nEp 1 (Step 003525): Train loss 1.385, Val loss 38.932\nEp 1 (Step 003530): Train loss 1.485, Val loss 39.268\nEp 1 (Step 003535): Train loss 1.454, Val loss 39.058\nEp 1 (Step 003540): Train loss 1.365, Val loss 39.251\nEp 1 (Step 003545): Train loss 1.327, Val loss 38.484\nEp 1 (Step 003550): Train loss 1.480, Val loss 39.047\nEp 1 (Step 003555): Train loss 1.502, Val loss 39.176\nEp 1 (Step 003560): Train loss 1.463, Val loss 39.120\nEp 1 (Step 003565): Train loss 1.432, Val loss 39.156\nEp 1 (Step 003570): Train loss 1.375, Val loss 39.454\nEp 1 (Step 003575): Train loss 1.300, Val loss 39.494\nEp 1 (Step 003580): Train loss 1.440, Val loss 39.275\nEp 1 (Step 003585): Train loss 1.444, Val loss 39.007\nEp 1 (Step 003590): Train loss 1.284, Val loss 39.368\nEp 1 (Step 003595): Train loss 1.279, Val loss 38.952\nEp 1 (Step 003600): Train loss 1.457, Val loss 39.513\nEp 1 (Step 003605): Train loss 1.471, Val loss 39.100\nEp 1 (Step 003610): Train loss 1.486, Val loss 39.345\nEp 1 (Step 003615): Train loss 1.329, Val loss 39.072\nEp 1 (Step 003620): Train loss 1.222, Val loss 39.601\nEp 1 (Step 003625): Train loss 1.428, Val loss 39.479\nEp 1 (Step 003630): Train loss 1.390, Val loss 39.430\nEp 1 (Step 003635): Train loss 1.294, Val loss 39.054\nEp 1 (Step 003640): Train loss 1.413, Val loss 39.018\nEp 1 (Step 003645): Train loss 1.290, Val loss 39.778\nEp 1 (Step 003650): Train loss 1.379, Val loss 39.951\nEp 1 (Step 003655): Train loss 1.416, Val loss 39.063\nEp 1 (Step 003660): Train loss 1.361, Val loss 39.152\nEp 1 (Step 003665): Train loss 1.481, Val loss 39.531\nEp 1 (Step 003670): Train loss 1.313, Val loss 39.348\nEp 1 (Step 003675): Train loss 1.347, Val loss 39.469\nEp 1 (Step 003680): Train loss 1.284, Val loss 39.092\nEp 1 (Step 003685): Train loss 1.355, Val loss 39.570\nEp 1 (Step 003690): Train loss 1.408, Val loss 39.858\nEp 1 (Step 003695): Train loss 1.293, Val loss 39.489\nEp 1 (Step 003700): Train loss 1.313, Val loss 39.031\nEp 1 (Step 003705): Train loss 1.388, Val loss 39.170\nEp 1 (Step 003710): Train loss 1.366, Val loss 39.090\nEp 1 (Step 003715): Train loss 1.387, Val loss 39.244\nEp 1 (Step 003720): Train loss 1.385, Val loss 39.553\nEp 1 (Step 003725): Train loss 1.301, Val loss 39.559\nEp 1 (Step 003730): Train loss 1.309, Val loss 39.776\nEp 1 (Step 003735): Train loss 1.313, Val loss 39.431\nEp 1 (Step 003740): Train loss 1.295, Val loss 39.596\nEp 1 (Step 003745): Train loss 1.363, Val loss 39.620\nEp 1 (Step 003750): Train loss 1.244, Val loss 39.119\nEp 1 (Step 003755): Train loss 1.447, Val loss 39.540\nEp 1 (Step 003760): Train loss 1.267, Val loss 39.606\nEp 1 (Step 003765): Train loss 1.274, Val loss 39.747\nEp 1 (Step 003770): Train loss 1.435, Val loss 38.936\nEp 1 (Step 003775): Train loss 1.493, Val loss 39.947\nEp 1 (Step 003780): Train loss 1.383, Val loss 39.702\nEp 1 (Step 003785): Train loss 1.398, Val loss 39.942\nEp 1 (Step 003790): Train loss 1.406, Val loss 40.227\nEp 1 (Step 003795): Train loss 1.356, Val loss 39.747\nEp 1 (Step 003800): Train loss 1.309, Val loss 39.461\nEp 1 (Step 003805): Train loss 1.287, Val loss 40.094\nEp 1 (Step 003810): Train loss 1.347, Val loss 39.545\nEp 1 (Step 003815): Train loss 1.314, Val loss 39.444\nEp 1 (Step 003820): Train loss 1.339, Val loss 40.121\nEp 1 (Step 003825): Train loss 1.337, Val loss 39.783\nEp 1 (Step 003830): Train loss 1.242, Val loss 39.820\nEp 1 (Step 003835): Train loss 1.368, Val loss 39.757\nEp 1 (Step 003840): Train loss 1.407, Val loss 39.440\nEp 1 (Step 003845): Train loss 1.325, Val loss 39.791\nEp 1 (Step 003850): Train loss 1.352, Val loss 39.399\nEp 1 (Step 003855): Train loss 1.150, Val loss 39.415\nEp 1 (Step 003860): Train loss 1.232, Val loss 40.272\nEp 1 (Step 003865): Train loss 1.367, Val loss 39.576\nEp 1 (Step 003870): Train loss 1.363, Val loss 40.209\nEp 1 (Step 003875): Train loss 1.190, Val loss 39.734\nEp 1 (Step 003880): Train loss 1.227, Val loss 40.136\nEp 1 (Step 003885): Train loss 1.205, Val loss 39.838\nEp 1 (Step 003890): Train loss 1.265, Val loss 40.022\nEp 1 (Step 003895): Train loss 1.338, Val loss 39.807\nEp 1 (Step 003900): Train loss 1.253, Val loss 39.667\nEp 1 (Step 003905): Train loss 1.287, Val loss 39.584\nEp 1 (Step 003910): Train loss 1.282, Val loss 39.593\nEp 1 (Step 003915): Train loss 1.282, Val loss 39.410\nEp 1 (Step 003920): Train loss 1.258, Val loss 40.088\nEp 1 (Step 003925): Train loss 1.264, Val loss 39.681\nEp 1 (Step 003930): Train loss 1.207, Val loss 39.889\nEp 1 (Step 003935): Train loss 1.339, Val loss 39.875\nEp 1 (Step 003940): Train loss 1.177, Val loss 40.470\nEp 1 (Step 003945): Train loss 1.266, Val loss 39.688\nEp 1 (Step 003950): Train loss 1.228, Val loss 40.019\nEp 1 (Step 003955): Train loss 1.196, Val loss 40.107\nEp 1 (Step 003960): Train loss 1.239, Val loss 39.975\nEp 1 (Step 003965): Train loss 1.280, Val loss 40.382\nEp 1 (Step 003970): Train loss 1.263, Val loss 40.113\nEp 1 (Step 003975): Train loss 1.345, Val loss 39.513\nEp 1 (Step 003980): Train loss 1.239, Val loss 39.876\nEp 1 (Step 003985): Train loss 1.258, Val loss 40.314\nEp 1 (Step 003990): Train loss 1.123, Val loss 40.062\nEp 1 (Step 003995): Train loss 1.312, Val loss 39.387\nEp 1 (Step 004000): Train loss 1.264, Val loss 40.031\nEp 1 (Step 004005): Train loss 1.283, Val loss 39.618\nEp 1 (Step 004010): Train loss 1.277, Val loss 40.438\nEp 1 (Step 004015): Train loss 1.261, Val loss 40.332\nEp 1 (Step 004020): Train loss 1.320, Val loss 39.658\nEp 1 (Step 004025): Train loss 1.321, Val loss 39.808\nEp 1 (Step 004030): Train loss 1.291, Val loss 40.587\nEp 1 (Step 004035): Train loss 1.251, Val loss 40.223\nEp 1 (Step 004040): Train loss 1.197, Val loss 40.931\nEp 1 (Step 004045): Train loss 1.177, Val loss 40.439\nEp 1 (Step 004050): Train loss 1.254, Val loss 40.594\nEp 1 (Step 004055): Train loss 1.224, Val loss 40.357\nEp 1 (Step 004060): Train loss 1.213, Val loss 40.971\nEp 1 (Step 004065): Train loss 1.308, Val loss 40.884\nEp 1 (Step 004070): Train loss 1.351, Val loss 40.112\nEp 1 (Step 004075): Train loss 1.201, Val loss 39.895\nEp 1 (Step 004080): Train loss 1.244, Val loss 40.718\nEp 1 (Step 004085): Train loss 1.211, Val loss 40.558\nEp 1 (Step 004090): Train loss 1.172, Val loss 39.351\nEp 1 (Step 004095): Train loss 1.231, Val loss 40.717\nEp 1 (Step 004100): Train loss 1.211, Val loss 40.316\nEp 1 (Step 004105): Train loss 1.317, Val loss 40.006\nEp 1 (Step 004110): Train loss 1.237, Val loss 40.094\nEp 1 (Step 004115): Train loss 1.258, Val loss 39.760\nEp 1 (Step 004120): Train loss 1.213, Val loss 39.997\nEp 1 (Step 004125): Train loss 1.212, Val loss 39.452\nEp 1 (Step 004130): Train loss 1.236, Val loss 39.606\nEp 1 (Step 004135): Train loss 1.223, Val loss 40.229\nEp 1 (Step 004140): Train loss 1.157, Val loss 39.689\nEp 1 (Step 004145): Train loss 1.244, Val loss 40.508\nEp 1 (Step 004150): Train loss 1.483, Val loss 39.990\nEp 1 (Step 004155): Train loss 1.192, Val loss 40.223\nEp 1 (Step 004160): Train loss 1.187, Val loss 40.244\nEp 1 (Step 004165): Train loss 1.259, Val loss 40.523\nEp 1 (Step 004170): Train loss 1.300, Val loss 40.210\nEp 1 (Step 004175): Train loss 1.310, Val loss 40.723\nEp 1 (Step 004180): Train loss 1.237, Val loss 40.688\nEp 1 (Step 004185): Train loss 1.266, Val loss 40.266\nEp 1 (Step 004190): Train loss 1.126, Val loss 40.334\nEp 1 (Step 004195): Train loss 1.235, Val loss 40.767\nEp 1 (Step 004200): Train loss 1.164, Val loss 39.966\nEp 1 (Step 004205): Train loss 1.186, Val loss 40.392\nEp 1 (Step 004210): Train loss 1.213, Val loss 40.606\nEp 1 (Step 004215): Train loss 1.372, Val loss 40.295\nEp 1 (Step 004220): Train loss 1.232, Val loss 40.146\nEp 1 (Step 004225): Train loss 1.165, Val loss 39.559\nEp 1 (Step 004230): Train loss 1.311, Val loss 40.943\nEp 1 (Step 004235): Train loss 1.253, Val loss 39.984\nEp 1 (Step 004240): Train loss 1.167, Val loss 41.157\nEp 1 (Step 004245): Train loss 1.167, Val loss 40.640\nEp 1 (Step 004250): Train loss 1.157, Val loss 40.597\nEp 1 (Step 004255): Train loss 1.318, Val loss 41.206\nEp 1 (Step 004260): Train loss 1.328, Val loss 39.668\nEp 1 (Step 004265): Train loss 1.319, Val loss 40.262\nEp 1 (Step 004270): Train loss 1.201, Val loss 40.310\nEp 1 (Step 004275): Train loss 1.219, Val loss 40.588\nEp 1 (Step 004280): Train loss 1.261, Val loss 40.510\nEp 1 (Step 004285): Train loss 1.211, Val loss 40.673\nEp 1 (Step 004290): Train loss 1.215, Val loss 40.180\nEp 1 (Step 004295): Train loss 1.157, Val loss 40.279\nEp 1 (Step 004300): Train loss 1.092, Val loss 40.175\nEp 1 (Step 004305): Train loss 1.121, Val loss 40.407\nEp 1 (Step 004310): Train loss 1.236, Val loss 40.400\nEp 1 (Step 004315): Train loss 1.348, Val loss 40.792\nEp 1 (Step 004320): Train loss 1.175, Val loss 40.506\nEp 1 (Step 004325): Train loss 1.121, Val loss 40.544\nEp 1 (Step 004330): Train loss 1.221, Val loss 40.862\nEp 1 (Step 004335): Train loss 1.197, Val loss 40.266\nEp 1 (Step 004340): Train loss 1.223, Val loss 40.599\nEp 1 (Step 004345): Train loss 1.064, Val loss 41.004\nEp 1 (Step 004350): Train loss 1.218, Val loss 40.819\nEp 1 (Step 004355): Train loss 1.100, Val loss 40.427\nEp 1 (Step 004360): Train loss 1.169, Val loss 40.684\nEp 1 (Step 004365): Train loss 1.192, Val loss 41.038\nEp 1 (Step 004370): Train loss 1.288, Val loss 40.733\nEp 1 (Step 004375): Train loss 1.176, Val loss 40.579\nEp 1 (Step 004380): Train loss 1.174, Val loss 40.610\nEp 1 (Step 004385): Train loss 1.218, Val loss 40.285\nEp 1 (Step 004390): Train loss 1.101, Val loss 40.380\nEp 1 (Step 004395): Train loss 1.240, Val loss 40.546\nEp 1 (Step 004400): Train loss 1.225, Val loss 40.429\nEp 1 (Step 004405): Train loss 1.201, Val loss 40.615\nEp 1 (Step 004410): Train loss 1.076, Val loss 40.212\nEp 1 (Step 004415): Train loss 1.288, Val loss 40.228\nEp 1 (Step 004420): Train loss 1.215, Val loss 41.001\nEp 1 (Step 004425): Train loss 1.139, Val loss 40.400\nEp 1 (Step 004430): Train loss 1.257, Val loss 40.714\nEp 1 (Step 004435): Train loss 1.182, Val loss 40.632\nEp 1 (Step 004440): Train loss 1.208, Val loss 40.376\nEp 1 (Step 004445): Train loss 1.267, Val loss 40.787\nEp 1 (Step 004450): Train loss 1.147, Val loss 40.337\nEp 1 (Step 004455): Train loss 1.298, Val loss 40.465\nEp 1 (Step 004460): Train loss 1.098, Val loss 40.858\nEp 1 (Step 004465): Train loss 1.112, Val loss 40.532\nEp 1 (Step 004470): Train loss 1.165, Val loss 40.687\nEp 1 (Step 004475): Train loss 1.182, Val loss 41.023\nEp 1 (Step 004480): Train loss 1.219, Val loss 40.545\nEp 1 (Step 004485): Train loss 1.152, Val loss 41.042\nEp 1 (Step 004490): Train loss 1.061, Val loss 40.317\nEp 1 (Step 004495): Train loss 1.286, Val loss 40.060\nEp 1 (Step 004500): Train loss 1.169, Val loss 40.985\nEp 1 (Step 004505): Train loss 1.188, Val loss 40.953\nEp 1 (Step 004510): Train loss 1.108, Val loss 40.457\nEp 1 (Step 004515): Train loss 1.215, Val loss 40.198\nEp 1 (Step 004520): Train loss 1.165, Val loss 41.168\nEp 1 (Step 004525): Train loss 1.107, Val loss 41.123\nEp 1 (Step 004530): Train loss 1.103, Val loss 40.882\nEp 1 (Step 004535): Train loss 1.136, Val loss 41.139\nEp 1 (Step 004540): Train loss 1.151, Val loss 40.781\nEp 1 (Step 004545): Train loss 1.170, Val loss 41.501\nEp 1 (Step 004550): Train loss 1.240, Val loss 41.026\nEp 1 (Step 004555): Train loss 1.198, Val loss 41.210\nEp 1 (Step 004560): Train loss 1.042, Val loss 40.669\nEp 1 (Step 004565): Train loss 1.162, Val loss 40.970\nEp 1 (Step 004570): Train loss 1.164, Val loss 41.170\nEp 1 (Step 004575): Train loss 1.300, Val loss 41.342\nEp 1 (Step 004580): Train loss 1.215, Val loss 40.885\nEp 1 (Step 004585): Train loss 1.225, Val loss 41.348\nEp 1 (Step 004590): Train loss 1.167, Val loss 40.814\nEp 1 (Step 004595): Train loss 1.204, Val loss 40.593\nEp 1 (Step 004600): Train loss 1.162, Val loss 40.539\nEp 1 (Step 004605): Train loss 1.319, Val loss 40.466\nEp 1 (Step 004610): Train loss 1.193, Val loss 40.519\nEp 1 (Step 004615): Train loss 1.220, Val loss 41.098\nEp 1 (Step 004620): Train loss 1.121, Val loss 40.640\nEp 1 (Step 004625): Train loss 1.146, Val loss 41.221\nEp 1 (Step 004630): Train loss 1.087, Val loss 41.123\nEp 1 (Step 004635): Train loss 1.019, Val loss 40.803\nEp 1 (Step 004640): Train loss 1.087, Val loss 40.564\nEp 1 (Step 004645): Train loss 1.098, Val loss 40.603\nEp 1 (Step 004650): Train loss 1.158, Val loss 40.792\nEp 1 (Step 004655): Train loss 1.249, Val loss 41.009\nEp 1 (Step 004660): Train loss 1.215, Val loss 41.010\nEp 1 (Step 004665): Train loss 1.001, Val loss 41.374\nEp 1 (Step 004670): Train loss 1.094, Val loss 41.211\nEp 1 (Step 004675): Train loss 1.093, Val loss 40.912\nEp 1 (Step 004680): Train loss 1.052, Val loss 40.228\nEp 1 (Step 004685): Train loss 1.137, Val loss 42.392\nEp 1 (Step 004690): Train loss 1.109, Val loss 41.002\nEp 1 (Step 004695): Train loss 1.040, Val loss 40.370\nEp 1 (Step 004700): Train loss 1.172, Val loss 40.589\nEp 1 (Step 004705): Train loss 1.221, Val loss 40.061\nEp 1 (Step 004710): Train loss 1.104, Val loss 41.043\nEp 1 (Step 004715): Train loss 1.200, Val loss 41.098\nEp 1 (Step 004720): Train loss 1.032, Val loss 40.759\nEp 1 (Step 004725): Train loss 1.082, Val loss 40.172\nEp 1 (Step 004730): Train loss 1.128, Val loss 40.434\nEp 1 (Step 004735): Train loss 1.119, Val loss 40.808\nEp 1 (Step 004740): Train loss 1.031, Val loss 41.354\nEp 1 (Step 004745): Train loss 1.154, Val loss 41.307\nEp 1 (Step 004750): Train loss 1.098, Val loss 40.816\nEp 1 (Step 004755): Train loss 1.208, Val loss 40.977\nEp 1 (Step 004760): Train loss 1.113, Val loss 40.666\nEp 1 (Step 004765): Train loss 1.166, Val loss 40.560\nEp 1 (Step 004770): Train loss 1.106, Val loss 40.985\nEp 1 (Step 004775): Train loss 1.116, Val loss 40.986\nEp 1 (Step 004780): Train loss 1.131, Val loss 41.534\nEp 1 (Step 004785): Train loss 1.066, Val loss 40.617\nEp 1 (Step 004790): Train loss 1.155, Val loss 41.143\nEp 1 (Step 004795): Train loss 1.117, Val loss 40.983\nEp 1 (Step 004800): Train loss 1.115, Val loss 40.799\nEp 1 (Step 004805): Train loss 1.112, Val loss 41.326\nEp 1 (Step 004810): Train loss 1.079, Val loss 40.796\nEp 1 (Step 004815): Train loss 1.039, Val loss 41.060\nEp 1 (Step 004820): Train loss 1.185, Val loss 40.645\nEp 1 (Step 004825): Train loss 1.102, Val loss 41.037\nEp 1 (Step 004830): Train loss 1.142, Val loss 41.190\nEp 1 (Step 004835): Train loss 1.171, Val loss 41.030\nEp 1 (Step 004840): Train loss 1.102, Val loss 40.888\nEp 1 (Step 004845): Train loss 1.164, Val loss 41.203\nEp 1 (Step 004850): Train loss 1.190, Val loss 41.443\nEp 1 (Step 004855): Train loss 1.005, Val loss 41.314\nEp 1 (Step 004860): Train loss 1.025, Val loss 40.529\nEp 1 (Step 004865): Train loss 1.050, Val loss 40.918\nEp 1 (Step 004870): Train loss 1.189, Val loss 40.921\nEp 1 (Step 004875): Train loss 1.229, Val loss 40.535\nEp 1 (Step 004880): Train loss 1.062, Val loss 40.862\nEp 1 (Step 004885): Train loss 1.113, Val loss 40.989\nEp 1 (Step 004890): Train loss 1.100, Val loss 41.008\nEp 1 (Step 004895): Train loss 1.132, Val loss 41.232\nEp 1 (Step 004900): Train loss 1.132, Val loss 41.354\nEp 1 (Step 004905): Train loss 1.063, Val loss 41.767\nEp 1 (Step 004910): Train loss 1.129, Val loss 41.210\nEp 1 (Step 004915): Train loss 1.137, Val loss 41.596\nEp 1 (Step 004920): Train loss 1.081, Val loss 41.362\nEp 1 (Step 004925): Train loss 1.061, Val loss 40.599\nEp 1 (Step 004930): Train loss 1.178, Val loss 41.255\nEp 1 (Step 004935): Train loss 1.072, Val loss 41.208\nEp 1 (Step 004940): Train loss 1.049, Val loss 41.830\nEp 1 (Step 004945): Train loss 1.022, Val loss 40.940\nEp 1 (Step 004950): Train loss 1.053, Val loss 41.383\nEp 1 (Step 004955): Train loss 1.194, Val loss 41.665\nEp 1 (Step 004960): Train loss 1.152, Val loss 41.300\nEp 1 (Step 004965): Train loss 1.104, Val loss 41.163\nEp 1 (Step 004970): Train loss 1.133, Val loss 41.060\nEp 1 (Step 004975): Train loss 1.000, Val loss 41.766\nEp 1 (Step 004980): Train loss 1.031, Val loss 41.062\nEp 1 (Step 004985): Train loss 1.127, Val loss 41.090\nEp 1 (Step 004990): Train loss 1.145, Val loss 41.104\nEp 1 (Step 004995): Train loss 1.041, Val loss 41.111\nEp 1 (Step 005000): Train loss 1.033, Val loss 41.206\nEp 1 (Step 005005): Train loss 1.153, Val loss 41.061\nEp 1 (Step 005010): Train loss 1.054, Val loss 41.785\nEp 1 (Step 005015): Train loss 1.179, Val loss 41.253\nEp 1 (Step 005020): Train loss 1.057, Val loss 41.514\nEp 1 (Step 005025): Train loss 1.021, Val loss 41.886\nEp 1 (Step 005030): Train loss 1.010, Val loss 41.663\nEp 1 (Step 005035): Train loss 0.991, Val loss 41.166\nEp 1 (Step 005040): Train loss 1.088, Val loss 41.777\nEp 1 (Step 005045): Train loss 1.029, Val loss 41.136\nEp 1 (Step 005050): Train loss 1.033, Val loss 41.893\nEp 1 (Step 005055): Train loss 1.028, Val loss 41.572\nEp 1 (Step 005060): Train loss 1.106, Val loss 41.187\nEp 1 (Step 005065): Train loss 1.116, Val loss 41.161\nEp 1 (Step 005070): Train loss 0.996, Val loss 41.557\nEp 1 (Step 005075): Train loss 0.975, Val loss 41.392\nEp 1 (Step 005080): Train loss 1.017, Val loss 42.303\nEp 1 (Step 005085): Train loss 1.080, Val loss 41.180\nEp 1 (Step 005090): Train loss 1.049, Val loss 41.933\nEp 1 (Step 005095): Train loss 1.170, Val loss 40.941\nEp 1 (Step 005100): Train loss 0.997, Val loss 41.693\nEp 1 (Step 005105): Train loss 0.916, Val loss 41.780\nEp 1 (Step 005110): Train loss 1.016, Val loss 41.229\nEp 1 (Step 005115): Train loss 1.110, Val loss 41.011\nEp 1 (Step 005120): Train loss 1.094, Val loss 40.761\nEp 1 (Step 005125): Train loss 0.967, Val loss 41.328\nEp 1 (Step 005130): Train loss 1.123, Val loss 41.377\nEp 1 (Step 005135): Train loss 1.029, Val loss 41.566\nEp 1 (Step 005140): Train loss 1.024, Val loss 41.214\nEp 1 (Step 005145): Train loss 1.088, Val loss 41.108\nEp 1 (Step 005150): Train loss 1.046, Val loss 42.165\nEp 1 (Step 005155): Train loss 1.018, Val loss 41.945\nEp 1 (Step 005160): Train loss 1.025, Val loss 41.554\nEp 1 (Step 005165): Train loss 1.034, Val loss 41.083\nEp 1 (Step 005170): Train loss 1.063, Val loss 41.023\nEp 1 (Step 005175): Train loss 1.141, Val loss 41.029\nEp 1 (Step 005180): Train loss 0.967, Val loss 41.199\nEp 1 (Step 005185): Train loss 1.129, Val loss 41.957\nEp 1 (Step 005190): Train loss 1.046, Val loss 41.531\nEp 1 (Step 005195): Train loss 1.090, Val loss 41.616\nEp 1 (Step 005200): Train loss 1.047, Val loss 42.267\nEp 1 (Step 005205): Train loss 1.058, Val loss 41.402\nEp 1 (Step 005210): Train loss 1.122, Val loss 41.524\nEp 1 (Step 005215): Train loss 1.022, Val loss 41.783\nEp 1 (Step 005220): Train loss 0.994, Val loss 41.790\nEp 1 (Step 005225): Train loss 0.987, Val loss 41.748\nEp 1 (Step 005230): Train loss 1.048, Val loss 41.544\nEp 1 (Step 005235): Train loss 1.020, Val loss 41.175\nEp 1 (Step 005240): Train loss 1.000, Val loss 42.138\nEp 1 (Step 005245): Train loss 1.023, Val loss 41.448\nEp 1 (Step 005250): Train loss 0.996, Val loss 41.805\nEp 1 (Step 005255): Train loss 1.057, Val loss 41.600\nEp 1 (Step 005260): Train loss 0.961, Val loss 42.011\nEp 1 (Step 005265): Train loss 0.929, Val loss 41.647\nEp 1 (Step 005270): Train loss 1.032, Val loss 41.646\nEp 1 (Step 005275): Train loss 1.014, Val loss 41.616\nEp 1 (Step 005280): Train loss 1.036, Val loss 41.463\nEp 1 (Step 005285): Train loss 1.011, Val loss 41.144\nEp 1 (Step 005290): Train loss 0.997, Val loss 41.823\nEp 1 (Step 005295): Train loss 1.088, Val loss 42.233\nEp 1 (Step 005300): Train loss 0.972, Val loss 42.517\nEp 1 (Step 005305): Train loss 1.025, Val loss 41.976\nEp 1 (Step 005310): Train loss 1.049, Val loss 42.032\nEp 1 (Step 005315): Train loss 0.932, Val loss 42.357\nEp 1 (Step 005320): Train loss 1.015, Val loss 41.644\nEp 1 (Step 005325): Train loss 0.973, Val loss 41.695\nEp 1 (Step 005330): Train loss 0.928, Val loss 41.505\nEp 1 (Step 005335): Train loss 1.030, Val loss 41.648\nEp 1 (Step 005340): Train loss 0.970, Val loss 41.718\nEp 1 (Step 005345): Train loss 0.951, Val loss 41.181\nEp 1 (Step 005350): Train loss 1.035, Val loss 42.099\nEp 1 (Step 005355): Train loss 0.959, Val loss 41.359\nEp 1 (Step 005360): Train loss 0.931, Val loss 41.945\nEp 1 (Step 005365): Train loss 1.022, Val loss 41.961\nEp 1 (Step 005370): Train loss 1.058, Val loss 41.609\nEp 1 (Step 005375): Train loss 0.932, Val loss 41.150\nEp 1 (Step 005380): Train loss 0.982, Val loss 41.648\nEp 1 (Step 005385): Train loss 0.956, Val loss 42.090\nEp 1 (Step 005390): Train loss 1.055, Val loss 41.802\nEp 1 (Step 005395): Train loss 0.976, Val loss 41.686\nEp 1 (Step 005400): Train loss 1.028, Val loss 42.409\nEp 1 (Step 005405): Train loss 1.033, Val loss 42.017\nEp 1 (Step 005410): Train loss 1.013, Val loss 42.170\nEp 1 (Step 005415): Train loss 1.018, Val loss 41.723\nEp 1 (Step 005420): Train loss 1.027, Val loss 42.461\nEp 1 (Step 005425): Train loss 0.959, Val loss 41.870\nEp 1 (Step 005430): Train loss 0.976, Val loss 42.357\nEp 1 (Step 005435): Train loss 1.081, Val loss 42.272\nEp 1 (Step 005440): Train loss 1.032, Val loss 42.358\nEp 1 (Step 005445): Train loss 0.883, Val loss 41.616\nEp 1 (Step 005450): Train loss 0.860, Val loss 41.766\nEp 1 (Step 005455): Train loss 0.990, Val loss 42.093\nEp 1 (Step 005460): Train loss 1.058, Val loss 41.619\nEp 1 (Step 005465): Train loss 1.003, Val loss 41.657\nEp 1 (Step 005470): Train loss 1.015, Val loss 41.698\nEp 1 (Step 005475): Train loss 1.008, Val loss 41.549\nEp 1 (Step 005480): Train loss 0.954, Val loss 41.177\nEp 1 (Step 005485): Train loss 1.015, Val loss 41.619\nEp 1 (Step 005490): Train loss 1.063, Val loss 42.047\nEp 1 (Step 005495): Train loss 1.009, Val loss 42.668\nEp 1 (Step 005500): Train loss 0.971, Val loss 42.164\nEp 1 (Step 005505): Train loss 0.988, Val loss 41.775\nEp 1 (Step 005510): Train loss 0.903, Val loss 42.002\nEp 1 (Step 005515): Train loss 1.077, Val loss 41.381\nEp 1 (Step 005520): Train loss 1.008, Val loss 41.912\nEp 1 (Step 005525): Train loss 0.984, Val loss 41.611\nEp 1 (Step 005530): Train loss 0.928, Val loss 41.295\nEp 1 (Step 005535): Train loss 0.975, Val loss 42.088\nEp 1 (Step 005540): Train loss 1.056, Val loss 42.722\nEp 1 (Step 005545): Train loss 0.945, Val loss 41.901\nEp 1 (Step 005550): Train loss 1.017, Val loss 42.137\nEp 1 (Step 005555): Train loss 1.018, Val loss 42.232\nEp 1 (Step 005560): Train loss 1.095, Val loss 42.114\nEp 1 (Step 005565): Train loss 0.986, Val loss 41.848\nEp 1 (Step 005570): Train loss 0.941, Val loss 41.959\nEp 1 (Step 005575): Train loss 0.971, Val loss 42.310\nEp 1 (Step 005580): Train loss 1.123, Val loss 42.113\nEp 1 (Step 005585): Train loss 0.978, Val loss 41.574\nEp 1 (Step 005590): Train loss 0.995, Val loss 42.111\nEp 1 (Step 005595): Train loss 0.994, Val loss 42.295\nEp 1 (Step 005600): Train loss 0.950, Val loss 42.307\nEp 1 (Step 005605): Train loss 1.042, Val loss 41.676\nEp 1 (Step 005610): Train loss 0.936, Val loss 42.021\nEp 1 (Step 005615): Train loss 0.972, Val loss 41.835\nEp 1 (Step 005620): Train loss 1.001, Val loss 42.059\nEp 1 (Step 005625): Train loss 1.042, Val loss 42.315\nEp 1 (Step 005630): Train loss 0.962, Val loss 42.414\nEp 1 (Step 005635): Train loss 0.978, Val loss 42.101\nEp 1 (Step 005640): Train loss 1.022, Val loss 42.073\nEp 1 (Step 005645): Train loss 0.983, Val loss 42.066\nEp 1 (Step 005650): Train loss 0.893, Val loss 42.447\nEp 1 (Step 005655): Train loss 0.919, Val loss 41.698\nEp 1 (Step 005660): Train loss 1.039, Val loss 41.396\nEp 1 (Step 005665): Train loss 0.979, Val loss 41.965\nEp 1 (Step 005670): Train loss 0.985, Val loss 42.374\nEp 1 (Step 005675): Train loss 1.008, Val loss 42.532\nEp 1 (Step 005680): Train loss 1.005, Val loss 42.251\nEp 1 (Step 005685): Train loss 0.996, Val loss 42.433\nEp 1 (Step 005690): Train loss 0.978, Val loss 41.877\nEp 1 (Step 005695): Train loss 0.998, Val loss 42.190\nEp 1 (Step 005700): Train loss 0.921, Val loss 42.132\nEp 1 (Step 005705): Train loss 0.950, Val loss 42.746\nEp 1 (Step 005710): Train loss 0.923, Val loss 42.131\nEp 1 (Step 005715): Train loss 0.988, Val loss 42.427\nEp 1 (Step 005720): Train loss 0.836, Val loss 42.392\nEp 1 (Step 005725): Train loss 1.011, Val loss 42.617\nEp 1 (Step 005730): Train loss 0.967, Val loss 42.807\nEp 1 (Step 005735): Train loss 0.904, Val loss 43.087\nEp 1 (Step 005740): Train loss 0.981, Val loss 42.344\nEp 1 (Step 005745): Train loss 0.940, Val loss 42.055\nEp 1 (Step 005750): Train loss 0.865, Val loss 42.486\nEp 1 (Step 005755): Train loss 1.106, Val loss 42.100\nEp 1 (Step 005760): Train loss 0.952, Val loss 42.085\nEp 1 (Step 005765): Train loss 1.029, Val loss 42.460\nEp 1 (Step 005770): Train loss 1.015, Val loss 42.556\nEp 1 (Step 005775): Train loss 0.912, Val loss 42.610\nEp 1 (Step 005780): Train loss 0.987, Val loss 42.438\nEp 1 (Step 005785): Train loss 0.960, Val loss 41.971\nEp 1 (Step 005790): Train loss 1.047, Val loss 41.740\nEp 1 (Step 005795): Train loss 0.912, Val loss 42.026\nEp 1 (Step 005800): Train loss 0.906, Val loss 42.398\nEp 1 (Step 005805): Train loss 0.931, Val loss 42.201\nEp 1 (Step 005810): Train loss 0.977, Val loss 42.262\nEp 1 (Step 005815): Train loss 0.996, Val loss 42.272\nEp 1 (Step 005820): Train loss 0.880, Val loss 42.025\nEp 1 (Step 005825): Train loss 0.949, Val loss 42.995\nEp 1 (Step 005830): Train loss 0.993, Val loss 41.994\nEp 1 (Step 005835): Train loss 0.978, Val loss 41.924\nEp 1 (Step 005840): Train loss 1.018, Val loss 41.885\nEp 1 (Step 005845): Train loss 0.915, Val loss 42.428\nEp 1 (Step 005850): Train loss 0.954, Val loss 42.473\nEp 1 (Step 005855): Train loss 0.955, Val loss 42.119\nEp 1 (Step 005860): Train loss 0.997, Val loss 41.820\nEp 1 (Step 005865): Train loss 0.959, Val loss 42.369\nEp 1 (Step 005870): Train loss 0.956, Val loss 42.887\nEp 1 (Step 005875): Train loss 1.081, Val loss 42.666\nEp 1 (Step 005880): Train loss 0.949, Val loss 42.644\nEp 1 (Step 005885): Train loss 0.940, Val loss 42.330\nEp 1 (Step 005890): Train loss 0.924, Val loss 42.543\nEp 1 (Step 005895): Train loss 0.983, Val loss 42.890\nEp 1 (Step 005900): Train loss 0.911, Val loss 42.688\nEp 1 (Step 005905): Train loss 0.856, Val loss 42.091\nEp 1 (Step 005910): Train loss 0.843, Val loss 42.636\nEp 1 (Step 005915): Train loss 1.005, Val loss 42.548\nEp 1 (Step 005920): Train loss 0.874, Val loss 42.892\nEp 1 (Step 005925): Train loss 0.958, Val loss 42.709\nEp 1 (Step 005930): Train loss 0.940, Val loss 43.219\nEp 1 (Step 005935): Train loss 0.865, Val loss 42.862\nEp 1 (Step 005940): Train loss 0.990, Val loss 42.411\nEp 1 (Step 005945): Train loss 0.902, Val loss 42.582\nEp 1 (Step 005950): Train loss 0.861, Val loss 42.899\nEp 1 (Step 005955): Train loss 0.954, Val loss 43.071\nEp 1 (Step 005960): Train loss 0.892, Val loss 42.884\nEp 1 (Step 005965): Train loss 0.990, Val loss 43.029\nEp 1 (Step 005970): Train loss 0.923, Val loss 42.680\nEp 1 (Step 005975): Train loss 0.920, Val loss 42.208\nEp 1 (Step 005980): Train loss 0.959, Val loss 42.300\nEp 1 (Step 005985): Train loss 0.912, Val loss 42.564\nEp 1 (Step 005990): Train loss 0.941, Val loss 42.938\nEp 1 (Step 005995): Train loss 0.864, Val loss 42.746\nEp 1 (Step 006000): Train loss 0.984, Val loss 42.676\nEp 1 (Step 006005): Train loss 0.994, Val loss 42.202\nEp 1 (Step 006010): Train loss 0.847, Val loss 41.911\nEp 1 (Step 006015): Train loss 0.857, Val loss 42.340\nEp 1 (Step 006020): Train loss 0.916, Val loss 42.427\nEp 1 (Step 006025): Train loss 1.020, Val loss 43.006\nEp 1 (Step 006030): Train loss 0.921, Val loss 42.499\nEp 1 (Step 006035): Train loss 0.918, Val loss 42.952\nEp 1 (Step 006040): Train loss 0.952, Val loss 42.546\nEp 1 (Step 006045): Train loss 0.848, Val loss 42.863\nEp 1 (Step 006050): Train loss 0.942, Val loss 41.930\nEp 1 (Step 006055): Train loss 0.904, Val loss 42.471\nEp 1 (Step 006060): Train loss 0.926, Val loss 42.263\nEp 1 (Step 006065): Train loss 0.800, Val loss 42.724\nEp 1 (Step 006070): Train loss 0.903, Val loss 42.510\nEp 1 (Step 006075): Train loss 0.927, Val loss 42.700\nEp 1 (Step 006080): Train loss 0.932, Val loss 42.867\nEp 1 (Step 006085): Train loss 0.857, Val loss 42.735\nEp 1 (Step 006090): Train loss 0.894, Val loss 42.172\nEp 1 (Step 006095): Train loss 0.933, Val loss 43.312\nEp 1 (Step 006100): Train loss 0.980, Val loss 42.996\nEp 1 (Step 006105): Train loss 0.917, Val loss 42.708\nEp 1 (Step 006110): Train loss 0.913, Val loss 43.087\nEp 1 (Step 006115): Train loss 0.832, Val loss 42.494\nEp 1 (Step 006120): Train loss 0.890, Val loss 42.607\nEp 1 (Step 006125): Train loss 0.943, Val loss 42.365\nEp 1 (Step 006130): Train loss 0.966, Val loss 42.316\nEp 1 (Step 006135): Train loss 0.877, Val loss 42.678\nEp 1 (Step 006140): Train loss 0.919, Val loss 43.165\nEp 1 (Step 006145): Train loss 0.861, Val loss 42.591\nEp 1 (Step 006150): Train loss 0.940, Val loss 42.970\nEp 1 (Step 006155): Train loss 0.859, Val loss 42.120\nEp 1 (Step 006160): Train loss 0.930, Val loss 42.802\nEp 1 (Step 006165): Train loss 0.928, Val loss 43.141\nEp 1 (Step 006170): Train loss 0.864, Val loss 43.281\nEp 1 (Step 006175): Train loss 0.975, Val loss 42.949\nEp 1 (Step 006180): Train loss 0.887, Val loss 42.593\nEp 1 (Step 006185): Train loss 0.877, Val loss 42.461\nEp 1 (Step 006190): Train loss 0.948, Val loss 42.693\nEp 1 (Step 006195): Train loss 0.971, Val loss 42.506\nEp 1 (Step 006200): Train loss 0.902, Val loss 43.229\nEp 1 (Step 006205): Train loss 0.904, Val loss 43.263\nEp 1 (Step 006210): Train loss 0.909, Val loss 42.715\nEp 1 (Step 006215): Train loss 0.962, Val loss 42.989\nEp 1 (Step 006220): Train loss 0.954, Val loss 42.731\nEp 1 (Step 006225): Train loss 0.987, Val loss 42.454\nEp 1 (Step 006230): Train loss 0.893, Val loss 42.507\nEp 1 (Step 006235): Train loss 0.893, Val loss 42.683\nEp 1 (Step 006240): Train loss 1.062, Val loss 43.011\nEp 1 (Step 006245): Train loss 0.841, Val loss 42.143\nEp 1 (Step 006250): Train loss 0.942, Val loss 42.959\nEp 1 (Step 006255): Train loss 1.027, Val loss 42.981\nEp 1 (Step 006260): Train loss 0.877, Val loss 42.765\nEp 1 (Step 006265): Train loss 0.972, Val loss 42.717\nEp 1 (Step 006270): Train loss 1.002, Val loss 42.751\nEp 1 (Step 006275): Train loss 0.908, Val loss 42.936\nEp 1 (Step 006280): Train loss 0.836, Val loss 42.763\nEp 1 (Step 006285): Train loss 0.885, Val loss 42.644\nEp 1 (Step 006290): Train loss 0.860, Val loss 42.497\nEp 1 (Step 006295): Train loss 0.965, Val loss 43.175\nEp 1 (Step 006300): Train loss 0.866, Val loss 42.859\nEp 1 (Step 006305): Train loss 0.838, Val loss 42.679\nEp 1 (Step 006310): Train loss 0.972, Val loss 43.349\nEp 1 (Step 006315): Train loss 0.902, Val loss 42.563\nEp 1 (Step 006320): Train loss 0.940, Val loss 43.439\nEp 1 (Step 006325): Train loss 0.940, Val loss 43.708\nEp 1 (Step 006330): Train loss 1.008, Val loss 42.277\nEp 1 (Step 006335): Train loss 0.926, Val loss 42.667\nEp 1 (Step 006340): Train loss 0.841, Val loss 42.733\nEp 1 (Step 006345): Train loss 0.848, Val loss 42.465\nEp 1 (Step 006350): Train loss 0.831, Val loss 42.972\nEp 1 (Step 006355): Train loss 0.877, Val loss 42.828\nEp 1 (Step 006360): Train loss 0.882, Val loss 42.705\nEp 1 (Step 006365): Train loss 0.846, Val loss 43.445\nEp 1 (Step 006370): Train loss 0.866, Val loss 43.000\nEp 1 (Step 006375): Train loss 0.961, Val loss 42.844\nEp 1 (Step 006380): Train loss 0.886, Val loss 43.614\nEp 1 (Step 006385): Train loss 0.870, Val loss 43.552\nEp 1 (Step 006390): Train loss 0.870, Val loss 42.896\nEp 1 (Step 006395): Train loss 0.918, Val loss 43.168\nEp 1 (Step 006400): Train loss 0.852, Val loss 42.871\nEp 1 (Step 006405): Train loss 0.819, Val loss 43.136\nEp 1 (Step 006410): Train loss 0.849, Val loss 43.218\nEp 1 (Step 006415): Train loss 0.895, Val loss 43.161\nEp 1 (Step 006420): Train loss 0.887, Val loss 42.683\nEp 1 (Step 006425): Train loss 0.857, Val loss 42.821\nEp 1 (Step 006430): Train loss 0.871, Val loss 42.638\nEp 1 (Step 006435): Train loss 0.927, Val loss 43.107\nEp 1 (Step 006440): Train loss 0.835, Val loss 42.614\nEp 1 (Step 006445): Train loss 0.912, Val loss 42.897\nEp 1 (Step 006450): Train loss 0.925, Val loss 43.483\nEp 1 (Step 006455): Train loss 0.892, Val loss 43.157\nEp 1 (Step 006460): Train loss 0.876, Val loss 43.379\nEp 1 (Step 006465): Train loss 0.910, Val loss 43.078\nEp 1 (Step 006470): Train loss 0.890, Val loss 42.987\nEp 1 (Step 006475): Train loss 0.866, Val loss 43.257\nEp 1 (Step 006480): Train loss 0.829, Val loss 43.201\nEp 1 (Step 006485): Train loss 0.958, Val loss 42.318\nEp 1 (Step 006490): Train loss 1.016, Val loss 43.575\nEp 1 (Step 006495): Train loss 0.894, Val loss 43.289\nEp 1 (Step 006500): Train loss 0.975, Val loss 42.810\nEp 1 (Step 006505): Train loss 0.866, Val loss 44.105\nEp 1 (Step 006510): Train loss 0.849, Val loss 43.194\nEp 1 (Step 006515): Train loss 0.917, Val loss 42.916\nEp 1 (Step 006520): Train loss 0.911, Val loss 43.414\nEp 1 (Step 006525): Train loss 0.923, Val loss 42.626\nEp 1 (Step 006530): Train loss 0.899, Val loss 43.589\nEp 1 (Step 006535): Train loss 0.914, Val loss 43.273\nEp 1 (Step 006540): Train loss 0.851, Val loss 42.740\nEp 1 (Step 006545): Train loss 0.865, Val loss 42.637\nEp 1 (Step 006550): Train loss 0.885, Val loss 42.591\nEp 1 (Step 006555): Train loss 0.876, Val loss 43.956\nEp 1 (Step 006560): Train loss 0.837, Val loss 43.065\nEp 1 (Step 006565): Train loss 0.860, Val loss 42.705\nEp 1 (Step 006570): Train loss 0.894, Val loss 43.635\nEp 1 (Step 006575): Train loss 0.946, Val loss 42.927\nEp 1 (Step 006580): Train loss 0.863, Val loss 42.520\nEp 1 (Step 006585): Train loss 0.828, Val loss 43.062\nEp 1 (Step 006590): Train loss 0.866, Val loss 42.835\nEp 1 (Step 006595): Train loss 0.853, Val loss 43.500\nEp 1 (Step 006600): Train loss 0.961, Val loss 43.310\nEp 1 (Step 006605): Train loss 0.789, Val loss 43.154\nEp 1 (Step 006610): Train loss 0.875, Val loss 43.217\nEp 1 (Step 006615): Train loss 0.837, Val loss 42.511\nEp 1 (Step 006620): Train loss 0.995, Val loss 43.386\nEp 1 (Step 006625): Train loss 0.936, Val loss 43.072\nEp 1 (Step 006630): Train loss 0.842, Val loss 43.693\nEp 1 (Step 006635): Train loss 0.942, Val loss 43.184\nEp 1 (Step 006640): Train loss 0.877, Val loss 43.676\nEp 1 (Step 006645): Train loss 0.859, Val loss 43.081\nEp 1 (Step 006650): Train loss 0.881, Val loss 43.086\nEp 1 (Step 006655): Train loss 0.795, Val loss 43.005\nEp 1 (Step 006660): Train loss 0.870, Val loss 43.042\nEp 1 (Step 006665): Train loss 0.828, Val loss 42.588\nEp 1 (Step 006670): Train loss 0.899, Val loss 43.500\nEp 1 (Step 006675): Train loss 0.864, Val loss 43.527\nEp 1 (Step 006680): Train loss 0.895, Val loss 43.100\nEp 1 (Step 006685): Train loss 0.816, Val loss 43.407\nEp 1 (Step 006690): Train loss 0.822, Val loss 43.354\nEp 1 (Step 006695): Train loss 0.876, Val loss 43.327\nEp 1 (Step 006700): Train loss 0.818, Val loss 43.048\nEp 1 (Step 006705): Train loss 0.939, Val loss 43.942\nEp 1 (Step 006710): Train loss 0.935, Val loss 43.023\nEp 1 (Step 006715): Train loss 0.827, Val loss 43.086\nEp 1 (Step 006720): Train loss 0.873, Val loss 43.682\nEp 1 (Step 006725): Train loss 0.950, Val loss 43.625\nEp 1 (Step 006730): Train loss 0.839, Val loss 43.752\nEp 1 (Step 006735): Train loss 0.853, Val loss 42.758\nEp 1 (Step 006740): Train loss 0.892, Val loss 43.956\nEp 1 (Step 006745): Train loss 0.821, Val loss 43.714\nEp 1 (Step 006750): Train loss 0.903, Val loss 43.219\nEp 1 (Step 006755): Train loss 0.967, Val loss 42.585\nEp 1 (Step 006760): Train loss 0.895, Val loss 42.954\nEp 1 (Step 006765): Train loss 0.809, Val loss 43.849\nEp 1 (Step 006770): Train loss 0.877, Val loss 43.350\nEp 1 (Step 006775): Train loss 0.926, Val loss 43.045\nEp 1 (Step 006780): Train loss 0.886, Val loss 43.207\nEp 1 (Step 006785): Train loss 0.806, Val loss 43.129\nEp 1 (Step 006790): Train loss 0.796, Val loss 43.035\nEp 1 (Step 006795): Train loss 0.926, Val loss 43.595\nEp 1 (Step 006800): Train loss 0.918, Val loss 43.450\nEp 1 (Step 006805): Train loss 0.980, Val loss 43.572\nEp 1 (Step 006810): Train loss 0.923, Val loss 43.288\nEp 1 (Step 006815): Train loss 0.865, Val loss 43.898\nEp 1 (Step 006820): Train loss 0.911, Val loss 43.923\nEp 1 (Step 006825): Train loss 0.822, Val loss 43.763\nEp 1 (Step 006830): Train loss 0.848, Val loss 43.422\nEp 1 (Step 006835): Train loss 0.888, Val loss 43.364\nEp 1 (Step 006840): Train loss 0.841, Val loss 43.496\nEp 1 (Step 006845): Train loss 0.902, Val loss 43.148\nEp 1 (Step 006850): Train loss 0.943, Val loss 43.272\nEp 1 (Step 006855): Train loss 0.849, Val loss 44.033\nEp 1 (Step 006860): Train loss 0.895, Val loss 43.676\nEp 1 (Step 006865): Train loss 0.870, Val loss 43.358\nEp 1 (Step 006870): Train loss 0.941, Val loss 43.431\nEp 1 (Step 006875): Train loss 0.828, Val loss 43.070\nEp 1 (Step 006880): Train loss 0.907, Val loss 43.016\nEp 1 (Step 006885): Train loss 0.815, Val loss 43.398\nEp 1 (Step 006890): Train loss 0.937, Val loss 42.825\nEp 1 (Step 006895): Train loss 0.887, Val loss 43.967\nEp 1 (Step 006900): Train loss 0.831, Val loss 43.680\nEp 1 (Step 006905): Train loss 0.936, Val loss 42.830\nEp 1 (Step 006910): Train loss 0.800, Val loss 43.229\nEp 1 (Step 006915): Train loss 0.841, Val loss 43.452\nEp 1 (Step 006920): Train loss 0.741, Val loss 43.095\nEp 1 (Step 006925): Train loss 0.842, Val loss 43.585\nEp 1 (Step 006930): Train loss 0.791, Val loss 43.799\nEp 1 (Step 006935): Train loss 0.866, Val loss 43.417\nEp 1 (Step 006940): Train loss 0.831, Val loss 43.679\nEp 1 (Step 006945): Train loss 0.837, Val loss 43.992\nEp 1 (Step 006950): Train loss 0.798, Val loss 44.176\nEp 1 (Step 006955): Train loss 0.841, Val loss 43.990\nEp 1 (Step 006960): Train loss 0.911, Val loss 43.874\nEp 1 (Step 006965): Train loss 0.837, Val loss 43.766\nEp 1 (Step 006970): Train loss 0.806, Val loss 43.394\nEp 1 (Step 006975): Train loss 0.837, Val loss 43.242\nEp 1 (Step 006980): Train loss 0.893, Val loss 43.648\nEp 1 (Step 006985): Train loss 0.781, Val loss 43.368\nEp 1 (Step 006990): Train loss 0.815, Val loss 43.518\nEp 1 (Step 006995): Train loss 0.793, Val loss 43.102\nEp 1 (Step 007000): Train loss 0.820, Val loss 43.439\nEp 1 (Step 007005): Train loss 0.855, Val loss 43.433\nEp 1 (Step 007010): Train loss 0.870, Val loss 44.048\nEp 1 (Step 007015): Train loss 0.820, Val loss 43.921\nEp 1 (Step 007020): Train loss 0.873, Val loss 44.011\nEp 1 (Step 007025): Train loss 0.928, Val loss 43.474\nEp 1 (Step 007030): Train loss 0.832, Val loss 44.136\nEp 1 (Step 007035): Train loss 0.805, Val loss 43.269\nEp 1 (Step 007040): Train loss 0.883, Val loss 43.856\nEp 1 (Step 007045): Train loss 0.852, Val loss 43.716\nEp 1 (Step 007050): Train loss 0.850, Val loss 43.777\nEp 1 (Step 007055): Train loss 0.871, Val loss 44.251\nEp 1 (Step 007060): Train loss 0.855, Val loss 43.140\nEp 1 (Step 007065): Train loss 0.898, Val loss 44.415\nEp 1 (Step 007070): Train loss 0.815, Val loss 43.785\nEp 1 (Step 007075): Train loss 0.884, Val loss 43.779\nEp 1 (Step 007080): Train loss 0.821, Val loss 43.600\nEp 1 (Step 007085): Train loss 0.844, Val loss 44.182\nEp 1 (Step 007090): Train loss 0.891, Val loss 43.697\nEp 1 (Step 007095): Train loss 0.917, Val loss 43.696\nEp 1 (Step 007100): Train loss 0.813, Val loss 43.971\nEp 1 (Step 007105): Train loss 0.850, Val loss 43.236\nEp 1 (Step 007110): Train loss 0.863, Val loss 43.855\nEp 1 (Step 007115): Train loss 0.862, Val loss 43.801\nEp 1 (Step 007120): Train loss 0.832, Val loss 43.533\nEp 1 (Step 007125): Train loss 0.899, Val loss 43.806\nEp 1 (Step 007130): Train loss 0.891, Val loss 43.255\nEp 1 (Step 007135): Train loss 0.881, Val loss 43.176\nEp 1 (Step 007140): Train loss 0.848, Val loss 43.788\nEp 1 (Step 007145): Train loss 0.947, Val loss 43.497\nEp 1 (Step 007150): Train loss 0.821, Val loss 43.930\nEp 1 (Step 007155): Train loss 0.853, Val loss 43.661\nEp 1 (Step 007160): Train loss 0.783, Val loss 43.930\nEp 1 (Step 007165): Train loss 0.804, Val loss 44.194\nEp 1 (Step 007170): Train loss 0.841, Val loss 43.522\nEp 1 (Step 007175): Train loss 0.814, Val loss 43.978\nEp 1 (Step 007180): Train loss 0.849, Val loss 44.150\nEp 1 (Step 007185): Train loss 0.716, Val loss 43.741\nEp 1 (Step 007190): Train loss 0.806, Val loss 44.119\nEp 1 (Step 007195): Train loss 0.844, Val loss 43.686\nEp 1 (Step 007200): Train loss 0.777, Val loss 43.733\nEp 1 (Step 007205): Train loss 0.913, Val loss 43.888\nEp 1 (Step 007210): Train loss 0.805, Val loss 43.927\nEp 1 (Step 007215): Train loss 0.821, Val loss 42.915\nEp 1 (Step 007220): Train loss 0.804, Val loss 44.351\nEp 1 (Step 007225): Train loss 0.826, Val loss 43.869\nEp 1 (Step 007230): Train loss 0.810, Val loss 43.408\nEp 1 (Step 007235): Train loss 0.766, Val loss 43.781\nEp 1 (Step 007240): Train loss 0.858, Val loss 43.658\nEp 1 (Step 007245): Train loss 0.757, Val loss 44.470\nEp 1 (Step 007250): Train loss 0.859, Val loss 44.109\nEp 1 (Step 007255): Train loss 0.804, Val loss 43.581\nEp 1 (Step 007260): Train loss 0.851, Val loss 43.454\nEp 1 (Step 007265): Train loss 0.839, Val loss 43.852\nEp 1 (Step 007270): Train loss 0.798, Val loss 43.139\nEp 1 (Step 007275): Train loss 0.826, Val loss 43.638\nEp 1 (Step 007280): Train loss 0.775, Val loss 43.950\nEp 1 (Step 007285): Train loss 0.797, Val loss 43.574\nEp 1 (Step 007290): Train loss 0.836, Val loss 43.899\nEp 1 (Step 007295): Train loss 0.828, Val loss 43.541\nEp 1 (Step 007300): Train loss 0.769, Val loss 43.632\nEp 1 (Step 007305): Train loss 0.753, Val loss 44.125\nEp 1 (Step 007310): Train loss 0.809, Val loss 43.876\nEp 1 (Step 007315): Train loss 0.880, Val loss 43.620\nEp 1 (Step 007320): Train loss 0.883, Val loss 43.444\nEp 1 (Step 007325): Train loss 0.898, Val loss 44.157\nEp 1 (Step 007330): Train loss 0.888, Val loss 43.527\nEp 1 (Step 007335): Train loss 0.755, Val loss 43.793\nEp 1 (Step 007340): Train loss 0.789, Val loss 44.400\nEp 1 (Step 007345): Train loss 0.869, Val loss 44.474\nEp 1 (Step 007350): Train loss 0.790, Val loss 43.786\nEp 1 (Step 007355): Train loss 0.825, Val loss 43.826\nEp 1 (Step 007360): Train loss 0.910, Val loss 43.652\nEp 1 (Step 007365): Train loss 0.804, Val loss 43.972\nEp 1 (Step 007370): Train loss 0.810, Val loss 43.806\nEp 1 (Step 007375): Train loss 0.850, Val loss 43.117\nEp 1 (Step 007380): Train loss 0.765, Val loss 44.148\nEp 1 (Step 007385): Train loss 0.821, Val loss 43.933\nEp 1 (Step 007390): Train loss 0.828, Val loss 44.094\nEp 1 (Step 007395): Train loss 0.817, Val loss 44.911\nEp 1 (Step 007400): Train loss 0.919, Val loss 43.969\nEp 1 (Step 007405): Train loss 0.853, Val loss 43.871\nEp 1 (Step 007410): Train loss 0.843, Val loss 43.645\nEp 1 (Step 007415): Train loss 0.810, Val loss 44.267\nEp 1 (Step 007420): Train loss 0.881, Val loss 43.694\nEp 1 (Step 007425): Train loss 0.823, Val loss 43.785\nEp 1 (Step 007430): Train loss 0.827, Val loss 44.016\nEp 1 (Step 007435): Train loss 0.878, Val loss 43.738\nEp 1 (Step 007440): Train loss 0.872, Val loss 44.196\nEp 1 (Step 007445): Train loss 0.785, Val loss 43.463\nEp 1 (Step 007450): Train loss 0.761, Val loss 43.835\nEp 1 (Step 007455): Train loss 0.831, Val loss 44.872\nEp 1 (Step 007460): Train loss 0.875, Val loss 44.016\nEp 1 (Step 007465): Train loss 0.770, Val loss 44.000\nEp 1 (Step 007470): Train loss 0.837, Val loss 44.095\nEp 1 (Step 007475): Train loss 0.752, Val loss 44.297\nEp 1 (Step 007480): Train loss 0.812, Val loss 44.200\nEp 1 (Step 007485): Train loss 0.784, Val loss 43.659\nEp 1 (Step 007490): Train loss 0.782, Val loss 43.720\nEp 1 (Step 007495): Train loss 0.781, Val loss 43.567\nEp 1 (Step 007500): Train loss 0.849, Val loss 43.948\nEp 1 (Step 007505): Train loss 0.826, Val loss 44.086\nEp 1 (Step 007510): Train loss 0.798, Val loss 44.110\nEp 1 (Step 007515): Train loss 0.814, Val loss 44.173\nEp 1 (Step 007520): Train loss 0.885, Val loss 43.371\nEp 1 (Step 007525): Train loss 0.736, Val loss 44.123\nEp 1 (Step 007530): Train loss 0.836, Val loss 44.239\nEp 1 (Step 007535): Train loss 0.766, Val loss 43.876\nEp 1 (Step 007540): Train loss 0.953, Val loss 44.094\nEp 1 (Step 007545): Train loss 0.782, Val loss 44.061\nEp 1 (Step 007550): Train loss 0.797, Val loss 43.693\nEp 1 (Step 007555): Train loss 0.788, Val loss 44.371\nEp 1 (Step 007560): Train loss 0.731, Val loss 43.528\nEp 1 (Step 007565): Train loss 0.794, Val loss 43.874\nEp 1 (Step 007570): Train loss 0.909, Val loss 44.158\nEp 1 (Step 007575): Train loss 0.815, Val loss 44.019\nEp 1 (Step 007580): Train loss 0.825, Val loss 43.594\nEp 1 (Step 007585): Train loss 0.802, Val loss 43.879\nEp 1 (Step 007590): Train loss 0.899, Val loss 43.746\nEp 1 (Step 007595): Train loss 0.902, Val loss 45.053\nEp 1 (Step 007600): Train loss 0.781, Val loss 43.934\nEp 1 (Step 007605): Train loss 0.705, Val loss 44.272\nEp 1 (Step 007610): Train loss 0.748, Val loss 44.958\nEp 1 (Step 007615): Train loss 0.809, Val loss 43.985\nEp 1 (Step 007620): Train loss 0.858, Val loss 43.863\nEp 1 (Step 007625): Train loss 0.764, Val loss 44.328\nEp 1 (Step 007630): Train loss 0.830, Val loss 44.760\nEp 1 (Step 007635): Train loss 0.803, Val loss 44.544\nEp 1 (Step 007640): Train loss 0.749, Val loss 44.259\nEp 1 (Step 007645): Train loss 0.734, Val loss 43.489\nEp 1 (Step 007650): Train loss 0.828, Val loss 44.906\nEp 1 (Step 007655): Train loss 0.729, Val loss 44.204\nEp 1 (Step 007660): Train loss 0.796, Val loss 44.645\nEp 1 (Step 007665): Train loss 0.813, Val loss 44.076\nEp 1 (Step 007670): Train loss 0.801, Val loss 44.491\nEp 1 (Step 007675): Train loss 0.836, Val loss 44.186\nEp 1 (Step 007680): Train loss 0.847, Val loss 43.805\nEp 1 (Step 007685): Train loss 0.743, Val loss 43.969\nEp 1 (Step 007690): Train loss 0.794, Val loss 43.932\nEp 1 (Step 007695): Train loss 0.849, Val loss 44.071\nEp 1 (Step 007700): Train loss 0.819, Val loss 43.988\nEp 1 (Step 007705): Train loss 0.752, Val loss 43.844\nEp 1 (Step 007710): Train loss 0.826, Val loss 44.234\nEp 1 (Step 007715): Train loss 0.869, Val loss 44.437\nEp 1 (Step 007720): Train loss 0.779, Val loss 44.199\nEp 1 (Step 007725): Train loss 0.845, Val loss 44.266\nEp 1 (Step 007730): Train loss 0.794, Val loss 44.363\nEp 1 (Step 007735): Train loss 0.824, Val loss 44.216\nEp 1 (Step 007740): Train loss 0.836, Val loss 44.860\nEp 1 (Step 007745): Train loss 0.802, Val loss 44.399\nEp 1 (Step 007750): Train loss 0.826, Val loss 44.509\nEp 1 (Step 007755): Train loss 0.768, Val loss 44.509\nEp 1 (Step 007760): Train loss 0.796, Val loss 43.514\nEp 1 (Step 007765): Train loss 0.761, Val loss 44.444\nEp 1 (Step 007770): Train loss 0.732, Val loss 44.510\nEp 1 (Step 007775): Train loss 0.812, Val loss 44.434\nEp 1 (Step 007780): Train loss 0.817, Val loss 43.834\nEp 1 (Step 007785): Train loss 0.790, Val loss 44.554\nEp 1 (Step 007790): Train loss 0.844, Val loss 44.168\nEp 1 (Step 007795): Train loss 0.770, Val loss 44.675\nEp 1 (Step 007800): Train loss 0.789, Val loss 43.751\nEp 1 (Step 007805): Train loss 0.859, Val loss 43.694\nEp 1 (Step 007810): Train loss 0.799, Val loss 44.391\nEp 1 (Step 007815): Train loss 0.850, Val loss 43.581\nEp 1 (Step 007820): Train loss 0.719, Val loss 43.788\nEp 1 (Step 007825): Train loss 0.929, Val loss 44.230\nEp 1 (Step 007830): Train loss 0.752, Val loss 43.602\nEp 1 (Step 007835): Train loss 0.751, Val loss 44.342\nEp 1 (Step 007840): Train loss 0.755, Val loss 44.050\nEp 1 (Step 007845): Train loss 0.748, Val loss 44.019\nEp 1 (Step 007850): Train loss 0.811, Val loss 43.796\nEp 1 (Step 007855): Train loss 0.761, Val loss 44.189\nEp 1 (Step 007860): Train loss 0.811, Val loss 44.561\nEp 1 (Step 007865): Train loss 0.788, Val loss 44.312\nEp 1 (Step 007870): Train loss 0.834, Val loss 44.017\nEp 1 (Step 007875): Train loss 0.870, Val loss 44.383\nEp 1 (Step 007880): Train loss 0.875, Val loss 44.713\nEp 1 (Step 007885): Train loss 0.761, Val loss 44.176\nEp 1 (Step 007890): Train loss 0.790, Val loss 44.331\nEp 1 (Step 007895): Train loss 0.820, Val loss 43.916\nEp 1 (Step 007900): Train loss 0.796, Val loss 44.490\nEp 1 (Step 007905): Train loss 0.835, Val loss 44.316\nEp 1 (Step 007910): Train loss 0.805, Val loss 44.707\nEp 1 (Step 007915): Train loss 0.837, Val loss 44.598\nEp 1 (Step 007920): Train loss 0.771, Val loss 44.105\nEp 1 (Step 007925): Train loss 0.732, Val loss 44.074\nEp 1 (Step 007930): Train loss 0.802, Val loss 43.847\nEp 1 (Step 007935): Train loss 0.784, Val loss 44.176\nEp 1 (Step 007940): Train loss 0.806, Val loss 45.198\nEp 1 (Step 007945): Train loss 0.768, Val loss 44.112\nEp 1 (Step 007950): Train loss 0.780, Val loss 44.578\nEp 1 (Step 007955): Train loss 0.722, Val loss 44.535\nEp 1 (Step 007960): Train loss 0.873, Val loss 44.123\nEp 1 (Step 007965): Train loss 0.796, Val loss 44.161\nEp 1 (Step 007970): Train loss 0.760, Val loss 44.280\nEp 1 (Step 007975): Train loss 0.816, Val loss 44.442\nEp 1 (Step 007980): Train loss 0.781, Val loss 43.649\nEp 1 (Step 007985): Train loss 0.724, Val loss 44.531\nEp 1 (Step 007990): Train loss 0.770, Val loss 43.356\nEp 1 (Step 007995): Train loss 0.775, Val loss 43.966\nEp 1 (Step 008000): Train loss 0.732, Val loss 44.936\nEp 1 (Step 008005): Train loss 0.729, Val loss 44.200\nEp 1 (Step 008010): Train loss 0.814, Val loss 43.954\nEp 1 (Step 008015): Train loss 0.708, Val loss 44.355\nEp 1 (Step 008020): Train loss 0.766, Val loss 44.605\nEp 1 (Step 008025): Train loss 0.837, Val loss 44.196\nEp 1 (Step 008030): Train loss 0.733, Val loss 43.944\nEp 1 (Step 008035): Train loss 0.760, Val loss 44.113\nEp 1 (Step 008040): Train loss 0.843, Val loss 44.552\nEp 1 (Step 008045): Train loss 0.823, Val loss 44.855\nEp 1 (Step 008050): Train loss 0.759, Val loss 43.929\nEp 1 (Step 008055): Train loss 0.711, Val loss 44.563\nEp 1 (Step 008060): Train loss 0.705, Val loss 44.335\nEp 1 (Step 008065): Train loss 0.786, Val loss 45.508\nEp 1 (Step 008070): Train loss 0.807, Val loss 45.041\nEp 1 (Step 008075): Train loss 0.843, Val loss 45.320\nEp 1 (Step 008080): Train loss 0.804, Val loss 44.718\nEp 1 (Step 008085): Train loss 0.749, Val loss 45.097\nEp 1 (Step 008090): Train loss 0.753, Val loss 45.259\nEp 1 (Step 008095): Train loss 0.738, Val loss 43.493\nEp 1 (Step 008100): Train loss 0.711, Val loss 44.616\nEp 1 (Step 008105): Train loss 0.779, Val loss 43.949\nEp 1 (Step 008110): Train loss 0.771, Val loss 44.400\nEp 1 (Step 008115): Train loss 0.704, Val loss 44.290\nEp 1 (Step 008120): Train loss 0.796, Val loss 44.150\nEp 1 (Step 008125): Train loss 0.699, Val loss 44.750\nEp 1 (Step 008130): Train loss 0.802, Val loss 44.004\nEp 1 (Step 008135): Train loss 0.703, Val loss 44.611\nEp 1 (Step 008140): Train loss 0.790, Val loss 44.380\nEp 1 (Step 008145): Train loss 0.783, Val loss 43.945\nEp 1 (Step 008150): Train loss 0.828, Val loss 44.732\nEp 1 (Step 008155): Train loss 0.822, Val loss 44.886\nEp 1 (Step 008160): Train loss 0.796, Val loss 44.561\nEp 1 (Step 008165): Train loss 0.764, Val loss 44.779\nEp 1 (Step 008170): Train loss 0.808, Val loss 43.873\nEp 1 (Step 008175): Train loss 0.770, Val loss 44.490\nEp 1 (Step 008180): Train loss 0.795, Val loss 44.570\nEp 1 (Step 008185): Train loss 0.748, Val loss 44.649\nEp 1 (Step 008190): Train loss 0.753, Val loss 44.089\nEp 1 (Step 008195): Train loss 0.750, Val loss 44.510\nEp 1 (Step 008200): Train loss 0.749, Val loss 44.699\nEp 1 (Step 008205): Train loss 0.675, Val loss 45.051\nEp 1 (Step 008210): Train loss 0.759, Val loss 44.643\nEp 1 (Step 008215): Train loss 0.796, Val loss 43.905\nEp 1 (Step 008220): Train loss 0.715, Val loss 44.611\nEp 1 (Step 008225): Train loss 0.830, Val loss 44.862\nEp 1 (Step 008230): Train loss 0.797, Val loss 44.845\nEp 1 (Step 008235): Train loss 0.689, Val loss 44.692\nEp 1 (Step 008240): Train loss 0.843, Val loss 44.532\nEp 1 (Step 008245): Train loss 0.743, Val loss 44.299\nEp 1 (Step 008250): Train loss 0.750, Val loss 44.245\nEp 1 (Step 008255): Train loss 0.789, Val loss 43.771\nEp 1 (Step 008260): Train loss 0.761, Val loss 44.027\nEp 1 (Step 008265): Train loss 0.777, Val loss 44.692\nEp 1 (Step 008270): Train loss 0.770, Val loss 45.073\nEp 1 (Step 008275): Train loss 0.835, Val loss 44.268\nEp 1 (Step 008280): Train loss 0.798, Val loss 44.319\nEp 1 (Step 008285): Train loss 0.710, Val loss 44.801\nEp 1 (Step 008290): Train loss 0.771, Val loss 44.655\nEp 1 (Step 008295): Train loss 0.730, Val loss 44.461\nEp 1 (Step 008300): Train loss 0.733, Val loss 45.214\nEp 1 (Step 008305): Train loss 0.778, Val loss 44.218\nEp 1 (Step 008310): Train loss 0.733, Val loss 44.237\nEp 1 (Step 008315): Train loss 0.729, Val loss 45.005\nEp 1 (Step 008320): Train loss 0.740, Val loss 44.604\nEp 1 (Step 008325): Train loss 0.720, Val loss 44.736\nEp 1 (Step 008330): Train loss 0.720, Val loss 44.929\nEp 1 (Step 008335): Train loss 0.770, Val loss 44.579\nEp 1 (Step 008340): Train loss 0.689, Val loss 44.842\nEp 1 (Step 008345): Train loss 0.733, Val loss 44.048\nEp 1 (Step 008350): Train loss 0.708, Val loss 44.415\nEp 1 (Step 008355): Train loss 0.721, Val loss 44.696\nEp 1 (Step 008360): Train loss 0.774, Val loss 44.094\nEp 1 (Step 008365): Train loss 0.831, Val loss 44.031\nEp 1 (Step 008370): Train loss 0.725, Val loss 43.739\nEp 1 (Step 008375): Train loss 0.802, Val loss 44.054\nEp 1 (Step 008380): Train loss 0.757, Val loss 44.713\nEp 1 (Step 008385): Train loss 0.720, Val loss 45.072\nEp 1 (Step 008390): Train loss 0.753, Val loss 43.787\nEp 1 (Step 008395): Train loss 0.783, Val loss 43.922\nEp 1 (Step 008400): Train loss 0.716, Val loss 44.351\nEp 1 (Step 008405): Train loss 0.748, Val loss 44.017\nEp 1 (Step 008410): Train loss 0.785, Val loss 44.028\nEp 1 (Step 008415): Train loss 0.717, Val loss 44.676\nEp 1 (Step 008420): Train loss 0.779, Val loss 44.674\nEp 1 (Step 008425): Train loss 0.805, Val loss 45.055\nEp 1 (Step 008430): Train loss 0.773, Val loss 44.800\nEp 1 (Step 008435): Train loss 0.778, Val loss 44.695\nEp 1 (Step 008440): Train loss 0.720, Val loss 44.885\nEp 1 (Step 008445): Train loss 0.737, Val loss 44.811\nEp 1 (Step 008450): Train loss 0.728, Val loss 43.945\nEp 1 (Step 008455): Train loss 0.724, Val loss 44.733\nEp 1 (Step 008460): Train loss 0.723, Val loss 44.710\nEp 1 (Step 008465): Train loss 0.732, Val loss 44.751\nEp 1 (Step 008470): Train loss 0.753, Val loss 44.955\nEp 1 (Step 008475): Train loss 0.763, Val loss 44.369\nEp 1 (Step 008480): Train loss 0.769, Val loss 44.734\nEp 1 (Step 008485): Train loss 0.808, Val loss 44.354\nEp 1 (Step 008490): Train loss 0.745, Val loss 44.433\nEp 1 (Step 008495): Train loss 0.825, Val loss 43.828\nEp 1 (Step 008500): Train loss 0.682, Val loss 44.585\nEp 1 (Step 008505): Train loss 0.678, Val loss 44.319\nEp 1 (Step 008510): Train loss 0.789, Val loss 44.496\nEp 1 (Step 008515): Train loss 0.752, Val loss 44.894\nEp 1 (Step 008520): Train loss 0.720, Val loss 44.217\nEp 1 (Step 008525): Train loss 0.727, Val loss 45.060\nEp 1 (Step 008530): Train loss 0.714, Val loss 44.715\nEp 1 (Step 008535): Train loss 0.856, Val loss 44.810\nEp 1 (Step 008540): Train loss 0.785, Val loss 44.754\nEp 1 (Step 008545): Train loss 0.794, Val loss 44.682\nEp 1 (Step 008550): Train loss 0.750, Val loss 44.415\nEp 1 (Step 008555): Train loss 0.723, Val loss 44.234\nEp 1 (Step 008560): Train loss 0.647, Val loss 44.300\nEp 1 (Step 008565): Train loss 0.835, Val loss 44.920\nEp 1 (Step 008570): Train loss 0.660, Val loss 44.923\nEp 1 (Step 008575): Train loss 0.730, Val loss 45.248\nEp 1 (Step 008580): Train loss 0.791, Val loss 45.361\nEp 1 (Step 008585): Train loss 0.677, Val loss 43.841\nEp 1 (Step 008590): Train loss 0.798, Val loss 44.263\nEp 1 (Step 008595): Train loss 0.712, Val loss 44.186\nEp 1 (Step 008600): Train loss 0.749, Val loss 44.559\nEp 1 (Step 008605): Train loss 0.722, Val loss 44.591\nEp 1 (Step 008610): Train loss 0.755, Val loss 44.694\nEp 1 (Step 008615): Train loss 0.699, Val loss 45.062\nEp 1 (Step 008620): Train loss 0.805, Val loss 44.534\nEp 1 (Step 008625): Train loss 0.717, Val loss 44.724\nEp 1 (Step 008630): Train loss 0.790, Val loss 44.705\nEp 1 (Step 008635): Train loss 0.720, Val loss 44.741\nEp 1 (Step 008640): Train loss 0.805, Val loss 44.211\nEp 1 (Step 008645): Train loss 0.754, Val loss 45.120\nEp 1 (Step 008650): Train loss 0.720, Val loss 44.560\nEp 1 (Step 008655): Train loss 0.735, Val loss 44.747\nEp 1 (Step 008660): Train loss 0.758, Val loss 45.658\nEp 1 (Step 008665): Train loss 0.786, Val loss 45.012\nEp 1 (Step 008670): Train loss 0.730, Val loss 44.688\nEp 1 (Step 008675): Train loss 0.659, Val loss 44.491\nEp 1 (Step 008680): Train loss 0.787, Val loss 45.088\nEp 1 (Step 008685): Train loss 0.761, Val loss 44.727\nEp 1 (Step 008690): Train loss 0.758, Val loss 44.097\nEp 1 (Step 008695): Train loss 0.682, Val loss 44.779\nEp 1 (Step 008700): Train loss 0.731, Val loss 44.845\nEp 1 (Step 008705): Train loss 0.668, Val loss 45.009\nEp 1 (Step 008710): Train loss 0.745, Val loss 44.616\nEp 1 (Step 008715): Train loss 0.753, Val loss 44.431\nEp 1 (Step 008720): Train loss 0.707, Val loss 44.276\nEp 1 (Step 008725): Train loss 0.683, Val loss 44.947\nEp 1 (Step 008730): Train loss 0.725, Val loss 45.289\nEp 1 (Step 008735): Train loss 0.752, Val loss 44.261\nEp 1 (Step 008740): Train loss 0.747, Val loss 44.438\nEp 1 (Step 008745): Train loss 0.770, Val loss 44.261\nEp 1 (Step 008750): Train loss 0.764, Val loss 44.720\nEp 1 (Step 008755): Train loss 0.831, Val loss 44.839\nEp 1 (Step 008760): Train loss 0.747, Val loss 45.120\nEp 1 (Step 008765): Train loss 0.799, Val loss 45.177\nEp 1 (Step 008770): Train loss 0.781, Val loss 44.934\nEp 1 (Step 008775): Train loss 0.719, Val loss 44.363\nEp 1 (Step 008780): Train loss 0.752, Val loss 44.611\nEp 1 (Step 008785): Train loss 0.735, Val loss 45.188\nEp 1 (Step 008790): Train loss 0.694, Val loss 44.627\nEp 1 (Step 008795): Train loss 0.671, Val loss 44.702\nEp 1 (Step 008800): Train loss 0.800, Val loss 44.830\nEp 1 (Step 008805): Train loss 0.751, Val loss 44.251\nEp 1 (Step 008810): Train loss 0.742, Val loss 45.007\nEp 1 (Step 008815): Train loss 0.719, Val loss 44.726\nEp 1 (Step 008820): Train loss 0.717, Val loss 44.599\nEp 1 (Step 008825): Train loss 0.721, Val loss 44.696\nEp 1 (Step 008830): Train loss 0.800, Val loss 44.941\nEp 1 (Step 008835): Train loss 0.731, Val loss 44.510\nEp 1 (Step 008840): Train loss 0.698, Val loss 44.557\nEp 1 (Step 008845): Train loss 0.723, Val loss 44.555\nEp 1 (Step 008850): Train loss 0.706, Val loss 44.390\nEp 1 (Step 008855): Train loss 0.742, Val loss 44.698\nEp 1 (Step 008860): Train loss 0.723, Val loss 44.512\nEp 1 (Step 008865): Train loss 0.780, Val loss 44.247\nEp 1 (Step 008870): Train loss 0.796, Val loss 44.864\nEp 1 (Step 008875): Train loss 0.670, Val loss 45.117\nEp 1 (Step 008880): Train loss 0.782, Val loss 44.943\nEp 1 (Step 008885): Train loss 0.789, Val loss 44.601\nEp 1 (Step 008890): Train loss 0.725, Val loss 45.047\nEp 1 (Step 008895): Train loss 0.644, Val loss 45.762\nEp 1 (Step 008900): Train loss 0.713, Val loss 45.533\nEp 1 (Step 008905): Train loss 0.761, Val loss 44.461\nEp 1 (Step 008910): Train loss 0.746, Val loss 45.221\nEp 1 (Step 008915): Train loss 0.762, Val loss 44.488\nEp 1 (Step 008920): Train loss 0.742, Val loss 45.382\nEp 1 (Step 008925): Train loss 0.706, Val loss 45.129\nEp 1 (Step 008930): Train loss 0.769, Val loss 45.069\nEp 1 (Step 008935): Train loss 0.711, Val loss 44.694\nEp 1 (Step 008940): Train loss 0.755, Val loss 44.706\nEp 1 (Step 008945): Train loss 0.778, Val loss 45.070\nEp 1 (Step 008950): Train loss 0.777, Val loss 45.436\nEp 1 (Step 008955): Train loss 0.688, Val loss 44.891\nEp 1 (Step 008960): Train loss 0.729, Val loss 44.596\nEp 1 (Step 008965): Train loss 0.687, Val loss 44.980\nEp 1 (Step 008970): Train loss 0.778, Val loss 44.794\nEp 1 (Step 008975): Train loss 0.748, Val loss 44.373\nEp 1 (Step 008980): Train loss 0.730, Val loss 44.886\nEp 1 (Step 008985): Train loss 0.765, Val loss 45.793\nEp 1 (Step 008990): Train loss 0.709, Val loss 45.327\nEp 1 (Step 008995): Train loss 0.767, Val loss 44.697\nEp 1 (Step 009000): Train loss 0.709, Val loss 45.097\nEp 1 (Step 009005): Train loss 0.785, Val loss 45.611\nEp 1 (Step 009010): Train loss 0.658, Val loss 45.385\nEp 1 (Step 009015): Train loss 0.675, Val loss 45.046\nEp 1 (Step 009020): Train loss 0.756, Val loss 44.536\nEp 1 (Step 009025): Train loss 0.705, Val loss 45.205\nEp 1 (Step 009030): Train loss 0.711, Val loss 45.276\nEp 1 (Step 009035): Train loss 0.766, Val loss 44.808\nEp 1 (Step 009040): Train loss 0.765, Val loss 45.357\nEp 1 (Step 009045): Train loss 0.748, Val loss 45.030\nEp 1 (Step 009050): Train loss 0.721, Val loss 44.857\nEp 1 (Step 009055): Train loss 0.669, Val loss 44.396\nEp 1 (Step 009060): Train loss 0.740, Val loss 44.508\nEp 1 (Step 009065): Train loss 0.710, Val loss 44.566\nEp 1 (Step 009070): Train loss 0.749, Val loss 45.255\nEp 1 (Step 009075): Train loss 0.643, Val loss 44.803\nEp 1 (Step 009080): Train loss 0.709, Val loss 45.265\nEp 1 (Step 009085): Train loss 0.722, Val loss 45.565\nEp 1 (Step 009090): Train loss 0.650, Val loss 45.120\nEp 1 (Step 009095): Train loss 0.769, Val loss 45.399\nEp 1 (Step 009100): Train loss 0.723, Val loss 45.122\nEp 1 (Step 009105): Train loss 0.808, Val loss 44.878\nEp 1 (Step 009110): Train loss 0.716, Val loss 44.911\nEp 1 (Step 009115): Train loss 0.718, Val loss 44.986\nEp 1 (Step 009120): Train loss 0.751, Val loss 45.254\nEp 1 (Step 009125): Train loss 0.710, Val loss 44.860\nEp 1 (Step 009130): Train loss 0.725, Val loss 45.223\nEp 1 (Step 009135): Train loss 0.720, Val loss 44.997\nEp 1 (Step 009140): Train loss 0.784, Val loss 44.849\nEp 1 (Step 009145): Train loss 0.749, Val loss 44.330\nEp 1 (Step 009150): Train loss 0.775, Val loss 45.660\nEp 1 (Step 009155): Train loss 0.688, Val loss 45.259\nEp 1 (Step 009160): Train loss 0.821, Val loss 45.133\nEp 1 (Step 009165): Train loss 0.750, Val loss 44.832\nEp 1 (Step 009170): Train loss 0.702, Val loss 45.647\nEp 1 (Step 009175): Train loss 0.709, Val loss 45.139\nEp 1 (Step 009180): Train loss 0.789, Val loss 44.858\nEp 1 (Step 009185): Train loss 0.752, Val loss 45.262\nEp 1 (Step 009190): Train loss 0.726, Val loss 45.058\nEp 1 (Step 009195): Train loss 0.744, Val loss 45.247\nEp 1 (Step 009200): Train loss 0.696, Val loss 45.481\nEp 1 (Step 009205): Train loss 0.682, Val loss 45.343\nEp 1 (Step 009210): Train loss 0.642, Val loss 44.466\nEp 1 (Step 009215): Train loss 0.716, Val loss 44.350\nEp 1 (Step 009220): Train loss 0.711, Val loss 44.994\nEp 1 (Step 009225): Train loss 0.695, Val loss 44.761\nEp 1 (Step 009230): Train loss 0.731, Val loss 45.075\nEp 1 (Step 009235): Train loss 0.688, Val loss 45.711\nEp 1 (Step 009240): Train loss 0.688, Val loss 44.756\nEp 1 (Step 009245): Train loss 0.703, Val loss 45.178\nEp 1 (Step 009250): Train loss 0.720, Val loss 45.513\nEp 1 (Step 009255): Train loss 0.715, Val loss 44.911\nEp 1 (Step 009260): Train loss 0.788, Val loss 44.684\nEp 1 (Step 009265): Train loss 0.666, Val loss 46.408\nEp 1 (Step 009270): Train loss 0.740, Val loss 44.884\nEp 1 (Step 009275): Train loss 0.749, Val loss 45.344\nEp 1 (Step 009280): Train loss 0.724, Val loss 45.551\nEp 1 (Step 009285): Train loss 0.787, Val loss 45.819\nEp 1 (Step 009290): Train loss 0.802, Val loss 44.819\nEp 1 (Step 009295): Train loss 0.736, Val loss 45.949\nEp 1 (Step 009300): Train loss 0.753, Val loss 44.807\nEp 1 (Step 009305): Train loss 0.743, Val loss 45.088\nEp 1 (Step 009310): Train loss 0.751, Val loss 45.302\nEp 1 (Step 009315): Train loss 0.718, Val loss 44.824\nEp 1 (Step 009320): Train loss 0.796, Val loss 45.238\nEp 1 (Step 009325): Train loss 0.740, Val loss 44.907\nEp 1 (Step 009330): Train loss 0.696, Val loss 45.115\nEp 1 (Step 009335): Train loss 0.658, Val loss 45.474\nEp 1 (Step 009340): Train loss 0.709, Val loss 44.814\nEp 1 (Step 009345): Train loss 0.673, Val loss 44.589\nEp 1 (Step 009350): Train loss 0.725, Val loss 45.312\nEp 1 (Step 009355): Train loss 0.776, Val loss 44.747\nEp 1 (Step 009360): Train loss 0.721, Val loss 44.694\nEp 1 (Step 009365): Train loss 0.659, Val loss 44.453\nEp 1 (Step 009370): Train loss 0.721, Val loss 44.607\nEp 1 (Step 009375): Train loss 0.687, Val loss 45.123\nEp 1 (Step 009380): Train loss 0.672, Val loss 45.339\nEp 1 (Step 009385): Train loss 0.678, Val loss 45.188\nEp 1 (Step 009390): Train loss 0.722, Val loss 45.124\nEp 1 (Step 009395): Train loss 0.762, Val loss 44.635\nEp 1 (Step 009400): Train loss 0.749, Val loss 45.804\nEp 1 (Step 009405): Train loss 0.719, Val loss 45.098\nEp 1 (Step 009410): Train loss 0.671, Val loss 45.561\nEp 1 (Step 009415): Train loss 0.718, Val loss 44.420\nEp 1 (Step 009420): Train loss 0.665, Val loss 45.253\nEp 1 (Step 009425): Train loss 0.699, Val loss 44.861\nEp 1 (Step 009430): Train loss 0.652, Val loss 45.141\nEp 1 (Step 009435): Train loss 0.724, Val loss 44.933\nEp 1 (Step 009440): Train loss 0.653, Val loss 45.103\nEp 1 (Step 009445): Train loss 0.688, Val loss 44.842\nEp 1 (Step 009450): Train loss 0.701, Val loss 45.127\nEp 1 (Step 009455): Train loss 0.709, Val loss 45.377\nEp 1 (Step 009460): Train loss 0.675, Val loss 45.467\nEp 1 (Step 009465): Train loss 0.674, Val loss 44.614\nEp 1 (Step 009470): Train loss 0.641, Val loss 45.183\nEp 1 (Step 009475): Train loss 0.741, Val loss 44.988\nEp 1 (Step 009480): Train loss 0.670, Val loss 45.107\nEp 1 (Step 009485): Train loss 0.711, Val loss 44.717\nEp 1 (Step 009490): Train loss 0.746, Val loss 45.409\nEp 1 (Step 009495): Train loss 0.733, Val loss 45.986\nEp 1 (Step 009500): Train loss 0.751, Val loss 44.401\nEp 1 (Step 009505): Train loss 0.739, Val loss 44.788\nEp 1 (Step 009510): Train loss 0.693, Val loss 45.176\nEp 1 (Step 009515): Train loss 0.761, Val loss 44.692\nEp 1 (Step 009520): Train loss 0.734, Val loss 44.795\nEp 1 (Step 009525): Train loss 0.784, Val loss 45.210\nEp 1 (Step 009530): Train loss 0.653, Val loss 45.157\nEp 1 (Step 009535): Train loss 0.666, Val loss 45.632\nEp 1 (Step 009540): Train loss 0.721, Val loss 44.974\nEp 1 (Step 009545): Train loss 0.673, Val loss 44.253\nEp 1 (Step 009550): Train loss 0.666, Val loss 45.596\nEp 1 (Step 009555): Train loss 0.738, Val loss 45.021\nEp 1 (Step 009560): Train loss 0.682, Val loss 45.253\nEp 1 (Step 009565): Train loss 0.700, Val loss 45.337\nEp 1 (Step 009570): Train loss 0.704, Val loss 44.902\nEp 1 (Step 009575): Train loss 0.694, Val loss 46.078\nEp 1 (Step 009580): Train loss 0.698, Val loss 45.618\nEp 1 (Step 009585): Train loss 0.716, Val loss 45.875\nEp 1 (Step 009590): Train loss 0.687, Val loss 44.911\nEp 1 (Step 009595): Train loss 0.755, Val loss 45.257\nEp 1 (Step 009600): Train loss 0.634, Val loss 44.906\nEp 1 (Step 009605): Train loss 0.666, Val loss 44.954\nEp 1 (Step 009610): Train loss 0.705, Val loss 44.993\nEp 1 (Step 009615): Train loss 0.763, Val loss 44.565\nEp 1 (Step 009620): Train loss 0.734, Val loss 45.320\nEp 1 (Step 009625): Train loss 0.683, Val loss 44.797\nEp 1 (Step 009630): Train loss 0.653, Val loss 45.551\nEp 1 (Step 009635): Train loss 0.649, Val loss 46.112\nEp 1 (Step 009640): Train loss 0.691, Val loss 45.103\nEp 1 (Step 009645): Train loss 0.683, Val loss 45.208\nEp 1 (Step 009650): Train loss 0.686, Val loss 45.060\nEp 1 (Step 009655): Train loss 0.681, Val loss 45.432\nEp 1 (Step 009660): Train loss 0.664, Val loss 44.946\nEp 1 (Step 009665): Train loss 0.698, Val loss 45.315\nEp 1 (Step 009670): Train loss 0.664, Val loss 44.949\nEp 1 (Step 009675): Train loss 0.693, Val loss 45.433\nEp 1 (Step 009680): Train loss 0.699, Val loss 44.362\nEp 1 (Step 009685): Train loss 0.707, Val loss 44.949\nEp 1 (Step 009690): Train loss 0.719, Val loss 45.381\nEp 1 (Step 009695): Train loss 0.657, Val loss 45.193\nEp 1 (Step 009700): Train loss 0.750, Val loss 44.886\nEp 1 (Step 009705): Train loss 0.668, Val loss 45.589\nEp 1 (Step 009710): Train loss 0.712, Val loss 45.492\nEp 1 (Step 009715): Train loss 0.703, Val loss 45.414\nEp 1 (Step 009720): Train loss 0.687, Val loss 45.616\nEp 1 (Step 009725): Train loss 0.670, Val loss 45.354\nEp 1 (Step 009730): Train loss 0.680, Val loss 45.934\nEp 1 (Step 009735): Train loss 0.667, Val loss 45.658\nEp 1 (Step 009740): Train loss 0.703, Val loss 45.616\nEp 1 (Step 009745): Train loss 0.742, Val loss 45.123\nEp 1 (Step 009750): Train loss 0.705, Val loss 45.240\nEp 1 (Step 009755): Train loss 0.729, Val loss 45.308\nEp 1 (Step 009760): Train loss 0.715, Val loss 44.805\nEp 1 (Step 009765): Train loss 0.710, Val loss 45.079\nEp 1 (Step 009770): Train loss 0.656, Val loss 45.220\nEp 1 (Step 009775): Train loss 0.664, Val loss 44.846\nEp 1 (Step 009780): Train loss 0.670, Val loss 44.839\nEp 1 (Step 009785): Train loss 0.687, Val loss 45.230\nEp 1 (Step 009790): Train loss 0.656, Val loss 46.215\nEp 1 (Step 009795): Train loss 0.685, Val loss 45.086\nEp 1 (Step 009800): Train loss 0.682, Val loss 44.812\nEp 1 (Step 009805): Train loss 0.726, Val loss 45.449\nEp 1 (Step 009810): Train loss 0.736, Val loss 45.016\nEp 1 (Step 009815): Train loss 0.665, Val loss 44.909\nEp 1 (Step 009820): Train loss 0.748, Val loss 45.255\nEp 1 (Step 009825): Train loss 0.656, Val loss 44.713\nEp 1 (Step 009830): Train loss 0.669, Val loss 45.550\nEp 1 (Step 009835): Train loss 0.726, Val loss 45.528\nEp 1 (Step 009840): Train loss 0.734, Val loss 45.789\nEp 1 (Step 009845): Train loss 0.693, Val loss 45.531\nEp 1 (Step 009850): Train loss 0.728, Val loss 45.319\nEp 1 (Step 009855): Train loss 0.687, Val loss 45.068\nEp 1 (Step 009860): Train loss 0.813, Val loss 45.009\nEp 1 (Step 009865): Train loss 0.657, Val loss 45.118\nEp 1 (Step 009870): Train loss 0.682, Val loss 45.488\nEp 1 (Step 009875): Train loss 0.704, Val loss 45.144\nEp 1 (Step 009880): Train loss 0.642, Val loss 44.664\nEp 1 (Step 009885): Train loss 0.680, Val loss 45.537\nEp 1 (Step 009890): Train loss 0.736, Val loss 45.310\nEp 1 (Step 009895): Train loss 0.649, Val loss 45.981\nEp 1 (Step 009900): Train loss 0.708, Val loss 45.621\nEp 1 (Step 009905): Train loss 0.710, Val loss 45.040\nEp 1 (Step 009910): Train loss 0.673, Val loss 45.316\nEp 1 (Step 009915): Train loss 0.710, Val loss 45.625\nEp 1 (Step 009920): Train loss 0.705, Val loss 44.923\nEp 1 (Step 009925): Train loss 0.662, Val loss 44.737\nEp 1 (Step 009930): Train loss 0.688, Val loss 45.142\nEp 1 (Step 009935): Train loss 0.668, Val loss 45.033\nEp 1 (Step 009940): Train loss 0.701, Val loss 45.527\nEp 1 (Step 009945): Train loss 0.674, Val loss 44.977\nEp 1 (Step 009950): Train loss 0.696, Val loss 45.256\nEp 1 (Step 009955): Train loss 0.703, Val loss 45.043\nEp 1 (Step 009960): Train loss 0.614, Val loss 44.515\nEp 1 (Step 009965): Train loss 0.672, Val loss 44.751\nEp 1 (Step 009970): Train loss 0.705, Val loss 45.180\nEp 1 (Step 009975): Train loss 0.669, Val loss 45.529\nEp 1 (Step 009980): Train loss 0.722, Val loss 46.028\nEp 1 (Step 009985): Train loss 0.694, Val loss 44.843\nEp 1 (Step 009990): Train loss 0.671, Val loss 45.323\nEp 1 (Step 009995): Train loss 0.722, Val loss 44.667\nEp 1 (Step 010000): Train loss 0.750, Val loss 46.193\nEp 1 (Step 010005): Train loss 0.700, Val loss 45.919\nEp 1 (Step 010010): Train loss 0.685, Val loss 44.965\nEp 1 (Step 010015): Train loss 0.659, Val loss 45.396\nEp 1 (Step 010020): Train loss 0.711, Val loss 45.950\nEp 1 (Step 010025): Train loss 0.647, Val loss 45.036\nEp 1 (Step 010030): Train loss 0.665, Val loss 45.606\nEp 1 (Step 010035): Train loss 0.652, Val loss 44.781\nEp 1 (Step 010040): Train loss 0.702, Val loss 45.143\nEp 1 (Step 010045): Train loss 0.685, Val loss 45.983\nEp 1 (Step 010050): Train loss 0.636, Val loss 45.442\nEp 1 (Step 010055): Train loss 0.750, Val loss 45.480\nEp 1 (Step 010060): Train loss 0.707, Val loss 44.836\nEp 1 (Step 010065): Train loss 0.653, Val loss 45.278\nEp 1 (Step 010070): Train loss 0.644, Val loss 45.030\nEp 1 (Step 010075): Train loss 0.685, Val loss 44.967\nEp 1 (Step 010080): Train loss 0.642, Val loss 45.585\nEp 1 (Step 010085): Train loss 0.660, Val loss 45.198\nEp 1 (Step 010090): Train loss 0.606, Val loss 44.348\nEp 1 (Step 010095): Train loss 0.702, Val loss 45.576\nEp 1 (Step 010100): Train loss 0.631, Val loss 45.869\nEp 1 (Step 010105): Train loss 0.681, Val loss 45.657\nEp 1 (Step 010110): Train loss 0.643, Val loss 44.854\nEp 1 (Step 010115): Train loss 0.681, Val loss 45.708\nEp 1 (Step 010120): Train loss 0.665, Val loss 45.512\nEp 1 (Step 010125): Train loss 0.651, Val loss 45.122\nEp 1 (Step 010130): Train loss 0.648, Val loss 45.398\nEp 1 (Step 010135): Train loss 0.684, Val loss 45.511\nEp 1 (Step 010140): Train loss 0.673, Val loss 45.901\nEp 1 (Step 010145): Train loss 0.665, Val loss 45.528\nEp 1 (Step 010150): Train loss 0.683, Val loss 45.974\nEp 1 (Step 010155): Train loss 0.712, Val loss 45.670\nEp 1 (Step 010160): Train loss 0.640, Val loss 45.430\nEp 1 (Step 010165): Train loss 0.635, Val loss 46.305\nEp 1 (Step 010170): Train loss 0.731, Val loss 45.972\nEp 1 (Step 010175): Train loss 0.694, Val loss 45.715\nEp 1 (Step 010180): Train loss 0.704, Val loss 45.796\nEp 1 (Step 010185): Train loss 0.688, Val loss 44.765\nEp 1 (Step 010190): Train loss 0.669, Val loss 45.929\nEp 1 (Step 010195): Train loss 0.674, Val loss 45.476\nEp 1 (Step 010200): Train loss 0.620, Val loss 45.052\nEp 1 (Step 010205): Train loss 0.675, Val loss 45.606\nEp 1 (Step 010210): Train loss 0.651, Val loss 45.831\nEp 1 (Step 010215): Train loss 0.695, Val loss 45.492\nEp 1 (Step 010220): Train loss 0.622, Val loss 45.377\nEp 1 (Step 010225): Train loss 0.677, Val loss 45.157\nEp 1 (Step 010230): Train loss 0.685, Val loss 44.970\nEp 1 (Step 010235): Train loss 0.654, Val loss 45.104\nEp 1 (Step 010240): Train loss 0.714, Val loss 45.665\nEp 1 (Step 010245): Train loss 0.715, Val loss 45.772\nEp 1 (Step 010250): Train loss 0.635, Val loss 45.113\nEp 1 (Step 010255): Train loss 0.673, Val loss 45.718\nEp 1 (Step 010260): Train loss 0.600, Val loss 45.116\nEp 1 (Step 010265): Train loss 0.705, Val loss 45.291\nEp 1 (Step 010270): Train loss 0.666, Val loss 45.399\nEp 1 (Step 010275): Train loss 0.620, Val loss 45.076\nEp 1 (Step 010280): Train loss 0.657, Val loss 45.495\nEp 1 (Step 010285): Train loss 0.685, Val loss 45.558\nEp 1 (Step 010290): Train loss 0.718, Val loss 45.706\nEp 1 (Step 010295): Train loss 0.682, Val loss 45.653\nEp 1 (Step 010300): Train loss 0.664, Val loss 45.233\nEp 1 (Step 010305): Train loss 0.657, Val loss 45.752\nEp 1 (Step 010310): Train loss 0.724, Val loss 45.349\nEp 1 (Step 010315): Train loss 0.693, Val loss 45.332\nEp 1 (Step 010320): Train loss 0.722, Val loss 45.460\nEp 1 (Step 010325): Train loss 0.628, Val loss 45.378\nEp 1 (Step 010330): Train loss 0.658, Val loss 45.292\nEp 1 (Step 010335): Train loss 0.695, Val loss 46.087\nEp 1 (Step 010340): Train loss 0.607, Val loss 45.445\nEp 1 (Step 010345): Train loss 0.677, Val loss 45.266\nEp 1 (Step 010350): Train loss 0.721, Val loss 44.915\nEp 1 (Step 010355): Train loss 0.641, Val loss 45.882\nEp 1 (Step 010360): Train loss 0.684, Val loss 45.511\nEp 1 (Step 010365): Train loss 0.634, Val loss 45.417\nEp 1 (Step 010370): Train loss 0.618, Val loss 45.367\nEp 1 (Step 010375): Train loss 0.687, Val loss 45.897\nEp 1 (Step 010380): Train loss 0.647, Val loss 45.794\nEp 1 (Step 010385): Train loss 0.638, Val loss 45.945\nEp 1 (Step 010390): Train loss 0.710, Val loss 45.578\nEp 1 (Step 010395): Train loss 0.601, Val loss 45.583\nEp 1 (Step 010400): Train loss 0.670, Val loss 45.765\nEp 1 (Step 010405): Train loss 0.611, Val loss 44.794\nEp 1 (Step 010410): Train loss 0.666, Val loss 45.497\nEp 1 (Step 010415): Train loss 0.668, Val loss 45.721\nEp 1 (Step 010420): Train loss 0.763, Val loss 45.732\nEp 1 (Step 010425): Train loss 0.661, Val loss 45.062\nEp 1 (Step 010430): Train loss 0.656, Val loss 45.367\nEp 1 (Step 010435): Train loss 0.676, Val loss 45.835\nEp 1 (Step 010440): Train loss 0.629, Val loss 45.545\nEp 1 (Step 010445): Train loss 0.693, Val loss 45.403\nEp 1 (Step 010450): Train loss 0.716, Val loss 46.030\nEp 1 (Step 010455): Train loss 0.733, Val loss 45.819\nEp 1 (Step 010460): Train loss 0.658, Val loss 45.732\nEp 1 (Step 010465): Train loss 0.643, Val loss 45.465\nEp 1 (Step 010470): Train loss 0.672, Val loss 46.348\nEp 1 (Step 010475): Train loss 0.721, Val loss 45.419\nEp 1 (Step 010480): Train loss 0.721, Val loss 45.511\nEp 1 (Step 010485): Train loss 0.765, Val loss 45.412\nEp 1 (Step 010490): Train loss 0.646, Val loss 45.548\nEp 1 (Step 010495): Train loss 0.600, Val loss 44.808\nEp 1 (Step 010500): Train loss 0.652, Val loss 45.525\nEp 1 (Step 010505): Train loss 0.659, Val loss 46.324\nEp 1 (Step 010510): Train loss 0.698, Val loss 45.693\nEp 1 (Step 010515): Train loss 0.573, Val loss 45.397\nEp 1 (Step 010520): Train loss 0.627, Val loss 45.574\nEp 1 (Step 010525): Train loss 0.660, Val loss 46.256\nEp 1 (Step 010530): Train loss 0.646, Val loss 45.709\nEp 1 (Step 010535): Train loss 0.721, Val loss 45.436\nEp 1 (Step 010540): Train loss 0.694, Val loss 45.992\nEp 1 (Step 010545): Train loss 0.739, Val loss 44.708\nEp 1 (Step 010550): Train loss 0.675, Val loss 45.385\nEp 1 (Step 010555): Train loss 0.592, Val loss 45.619\nEp 1 (Step 010560): Train loss 0.634, Val loss 45.571\nEp 1 (Step 010565): Train loss 0.603, Val loss 46.142\nEp 1 (Step 010570): Train loss 0.665, Val loss 45.151\nEp 1 (Step 010575): Train loss 0.614, Val loss 44.866\nEp 1 (Step 010580): Train loss 0.668, Val loss 45.242\nEp 1 (Step 010585): Train loss 0.654, Val loss 46.253\nEp 1 (Step 010590): Train loss 0.702, Val loss 45.513\nEp 1 (Step 010595): Train loss 0.688, Val loss 45.168\nEp 1 (Step 010600): Train loss 0.656, Val loss 44.877\nEp 1 (Step 010605): Train loss 0.662, Val loss 45.809\nEp 1 (Step 010610): Train loss 0.641, Val loss 45.392\nEp 1 (Step 010615): Train loss 0.719, Val loss 45.733\nEp 1 (Step 010620): Train loss 0.670, Val loss 45.925\nEp 1 (Step 010625): Train loss 0.641, Val loss 45.646\nEp 1 (Step 010630): Train loss 0.665, Val loss 45.001\nEp 1 (Step 010635): Train loss 0.682, Val loss 45.676\nEp 1 (Step 010640): Train loss 0.639, Val loss 45.155\nEp 1 (Step 010645): Train loss 0.663, Val loss 45.407\nEp 1 (Step 010650): Train loss 0.659, Val loss 45.758\nEp 1 (Step 010655): Train loss 0.627, Val loss 45.251\nEp 1 (Step 010660): Train loss 0.664, Val loss 45.210\nEp 1 (Step 010665): Train loss 0.702, Val loss 45.567\nEp 1 (Step 010670): Train loss 0.616, Val loss 46.071\nEp 1 (Step 010675): Train loss 0.675, Val loss 44.866\nEp 1 (Step 010680): Train loss 0.667, Val loss 45.532\nEp 1 (Step 010685): Train loss 0.670, Val loss 45.136\nEp 1 (Step 010690): Train loss 0.653, Val loss 45.021\nEp 1 (Step 010695): Train loss 0.642, Val loss 45.998\nEp 1 (Step 010700): Train loss 0.666, Val loss 45.083\nEp 1 (Step 010705): Train loss 0.647, Val loss 44.694\nEp 1 (Step 010710): Train loss 0.666, Val loss 45.602\nEp 1 (Step 010715): Train loss 0.770, Val loss 45.344\nEp 1 (Step 010720): Train loss 0.655, Val loss 45.116\nEp 1 (Step 010725): Train loss 0.649, Val loss 44.792\nEp 1 (Step 010730): Train loss 0.755, Val loss 45.394\nEp 1 (Step 010735): Train loss 0.703, Val loss 45.527\nEp 1 (Step 010740): Train loss 0.591, Val loss 45.808\nEp 1 (Step 010745): Train loss 0.669, Val loss 45.216\nEp 1 (Step 010750): Train loss 0.717, Val loss 45.011\nEp 1 (Step 010755): Train loss 0.655, Val loss 45.458\nEp 1 (Step 010760): Train loss 0.698, Val loss 44.683\nEp 1 (Step 010765): Train loss 0.663, Val loss 45.350\nEp 1 (Step 010770): Train loss 0.687, Val loss 45.372\nEp 1 (Step 010775): Train loss 0.698, Val loss 45.047\nEp 1 (Step 010780): Train loss 0.649, Val loss 45.202\nEp 1 (Step 010785): Train loss 0.702, Val loss 45.525\nEp 1 (Step 010790): Train loss 0.680, Val loss 45.082\nEp 1 (Step 010795): Train loss 0.634, Val loss 45.239\nEp 1 (Step 010800): Train loss 0.673, Val loss 44.951\nEp 1 (Step 010805): Train loss 0.646, Val loss 45.658\nEp 1 (Step 010810): Train loss 0.652, Val loss 45.127\nEp 1 (Step 010815): Train loss 0.796, Val loss 45.655\nEp 1 (Step 010820): Train loss 0.721, Val loss 46.132\nEp 1 (Step 010825): Train loss 0.692, Val loss 45.327\nEp 1 (Step 010830): Train loss 0.663, Val loss 44.970\nEp 1 (Step 010835): Train loss 0.772, Val loss 45.182\nEp 1 (Step 010840): Train loss 0.653, Val loss 45.634\nEp 1 (Step 010845): Train loss 0.604, Val loss 44.915\nEp 1 (Step 010850): Train loss 0.690, Val loss 45.900\nEp 1 (Step 010855): Train loss 0.688, Val loss 45.887\nEp 1 (Step 010860): Train loss 0.635, Val loss 45.979\nEp 1 (Step 010865): Train loss 0.706, Val loss 45.792\nEp 1 (Step 010870): Train loss 0.619, Val loss 45.403\nEp 1 (Step 010875): Train loss 0.658, Val loss 45.949\nEp 1 (Step 010880): Train loss 0.690, Val loss 46.105\nEp 1 (Step 010885): Train loss 0.693, Val loss 45.141\nEp 1 (Step 010890): Train loss 0.746, Val loss 45.347\nEp 1 (Step 010895): Train loss 0.666, Val loss 46.056\nEp 1 (Step 010900): Train loss 0.643, Val loss 45.582\nEp 1 (Step 010905): Train loss 0.698, Val loss 45.740\nEp 1 (Step 010910): Train loss 0.672, Val loss 45.552\nEp 1 (Step 010915): Train loss 0.671, Val loss 44.947\nEp 1 (Step 010920): Train loss 0.666, Val loss 45.211\nEp 1 (Step 010925): Train loss 0.635, Val loss 45.488\nEp 1 (Step 010930): Train loss 0.660, Val loss 45.402\nEp 1 (Step 010935): Train loss 0.707, Val loss 45.534\nEp 1 (Step 010940): Train loss 0.701, Val loss 45.118\nEp 1 (Step 010945): Train loss 0.625, Val loss 46.038\nEp 1 (Step 010950): Train loss 0.715, Val loss 45.350\nEp 1 (Step 010955): Train loss 0.625, Val loss 45.900\nEp 1 (Step 010960): Train loss 0.690, Val loss 45.784\nEp 1 (Step 010965): Train loss 0.620, Val loss 45.117\nEp 1 (Step 010970): Train loss 0.664, Val loss 46.825\nEp 1 (Step 010975): Train loss 0.661, Val loss 45.172\nEp 1 (Step 010980): Train loss 0.619, Val loss 45.454\nEp 1 (Step 010985): Train loss 0.681, Val loss 45.650\nEp 1 (Step 010990): Train loss 0.694, Val loss 45.483\nEp 1 (Step 010995): Train loss 0.693, Val loss 45.906\nEp 1 (Step 011000): Train loss 0.650, Val loss 46.097\nEp 1 (Step 011005): Train loss 0.598, Val loss 45.468\nEp 1 (Step 011010): Train loss 0.621, Val loss 45.774\nEp 1 (Step 011015): Train loss 0.675, Val loss 45.962\nEp 1 (Step 011020): Train loss 0.698, Val loss 45.553\nEp 1 (Step 011025): Train loss 0.665, Val loss 46.039\nEp 1 (Step 011030): Train loss 0.654, Val loss 45.777\nEp 1 (Step 011035): Train loss 0.719, Val loss 45.394\nEp 1 (Step 011040): Train loss 0.647, Val loss 44.859\nEp 1 (Step 011045): Train loss 0.675, Val loss 45.555\nEp 1 (Step 011050): Train loss 0.736, Val loss 44.853\nEp 1 (Step 011055): Train loss 0.592, Val loss 45.760\nEp 1 (Step 011060): Train loss 0.629, Val loss 45.420\nEp 1 (Step 011065): Train loss 0.629, Val loss 46.085\nEp 1 (Step 011070): Train loss 0.701, Val loss 46.041\nEp 1 (Step 011075): Train loss 0.648, Val loss 45.869\nEp 1 (Step 011080): Train loss 0.680, Val loss 45.571\nEp 1 (Step 011085): Train loss 0.617, Val loss 45.541\nEp 1 (Step 011090): Train loss 0.631, Val loss 46.380\nEp 1 (Step 011095): Train loss 0.659, Val loss 45.268\nEp 1 (Step 011100): Train loss 0.722, Val loss 44.602\nEp 1 (Step 011105): Train loss 0.603, Val loss 46.262\nEp 1 (Step 011110): Train loss 0.756, Val loss 45.167\nEp 1 (Step 011115): Train loss 0.678, Val loss 45.578\nEp 1 (Step 011120): Train loss 0.645, Val loss 45.171\nEp 1 (Step 011125): Train loss 0.639, Val loss 45.638\nEp 1 (Step 011130): Train loss 0.650, Val loss 45.696\nEp 1 (Step 011135): Train loss 0.666, Val loss 45.515\nEp 1 (Step 011140): Train loss 0.682, Val loss 45.529\nEp 1 (Step 011145): Train loss 0.649, Val loss 45.651\nEp 1 (Step 011150): Train loss 0.572, Val loss 46.133\nEp 1 (Step 011155): Train loss 0.610, Val loss 46.135\nEp 1 (Step 011160): Train loss 0.687, Val loss 46.633\nEp 1 (Step 011165): Train loss 0.643, Val loss 45.407\nEp 1 (Step 011170): Train loss 0.625, Val loss 45.743\nEp 1 (Step 011175): Train loss 0.645, Val loss 45.684\nEp 1 (Step 011180): Train loss 0.636, Val loss 45.302\nEp 1 (Step 011185): Train loss 0.632, Val loss 45.426\nEp 1 (Step 011190): Train loss 0.623, Val loss 45.775\nEp 1 (Step 011195): Train loss 0.642, Val loss 45.557\nEp 1 (Step 011200): Train loss 0.613, Val loss 45.322\nEp 1 (Step 011205): Train loss 0.609, Val loss 46.038\nEp 1 (Step 011210): Train loss 0.650, Val loss 44.719\nEp 1 (Step 011215): Train loss 0.698, Val loss 46.197\nEp 1 (Step 011220): Train loss 0.648, Val loss 46.272\nEp 1 (Step 011225): Train loss 0.678, Val loss 45.660\nEp 1 (Step 011230): Train loss 0.651, Val loss 45.850\nEp 1 (Step 011235): Train loss 0.662, Val loss 45.385\nEp 1 (Step 011240): Train loss 0.607, Val loss 46.010\nEp 1 (Step 011245): Train loss 0.597, Val loss 46.097\nEp 1 (Step 011250): Train loss 0.658, Val loss 45.392\nEp 1 (Step 011255): Train loss 0.635, Val loss 45.144\nEp 1 (Step 011260): Train loss 0.639, Val loss 45.770\nEp 1 (Step 011265): Train loss 0.606, Val loss 45.857\nEp 1 (Step 011270): Train loss 0.676, Val loss 44.817\nEp 1 (Step 011275): Train loss 0.654, Val loss 45.675\nEp 1 (Step 011280): Train loss 0.645, Val loss 45.787\nEp 1 (Step 011285): Train loss 0.669, Val loss 45.043\nEp 1 (Step 011290): Train loss 0.696, Val loss 46.195\nEp 1 (Step 011295): Train loss 0.648, Val loss 45.872\nEp 1 (Step 011300): Train loss 0.638, Val loss 45.639\nEp 1 (Step 011305): Train loss 0.713, Val loss 45.600\nEp 1 (Step 011310): Train loss 0.627, Val loss 45.933\nEp 1 (Step 011315): Train loss 0.626, Val loss 45.470\nEp 1 (Step 011320): Train loss 0.608, Val loss 46.361\nEp 1 (Step 011325): Train loss 0.716, Val loss 46.128\nEp 1 (Step 011330): Train loss 0.634, Val loss 45.884\nEp 1 (Step 011335): Train loss 0.661, Val loss 45.746\nEp 1 (Step 011340): Train loss 0.648, Val loss 45.592\nEp 1 (Step 011345): Train loss 0.608, Val loss 45.989\nEp 1 (Step 011350): Train loss 0.683, Val loss 45.618\nEp 1 (Step 011355): Train loss 0.615, Val loss 45.990\nEp 1 (Step 011360): Train loss 0.586, Val loss 46.006\nEp 1 (Step 011365): Train loss 0.659, Val loss 45.877\nEp 1 (Step 011370): Train loss 0.594, Val loss 46.303\nEp 1 (Step 011375): Train loss 0.652, Val loss 45.948\nEp 1 (Step 011380): Train loss 0.642, Val loss 45.374\nEp 1 (Step 011385): Train loss 0.715, Val loss 44.901\nEp 1 (Step 011390): Train loss 0.653, Val loss 45.121\nEp 1 (Step 011395): Train loss 0.607, Val loss 45.667\nEp 1 (Step 011400): Train loss 0.613, Val loss 45.560\nEp 1 (Step 011405): Train loss 0.604, Val loss 44.927\nEp 1 (Step 011410): Train loss 0.608, Val loss 46.405\nEp 1 (Step 011415): Train loss 0.616, Val loss 46.280\nEp 1 (Step 011420): Train loss 0.606, Val loss 45.515\nEp 1 (Step 011425): Train loss 0.670, Val loss 45.907\nEp 1 (Step 011430): Train loss 0.596, Val loss 45.492\nEp 1 (Step 011435): Train loss 0.637, Val loss 45.271\nEp 1 (Step 011440): Train loss 0.617, Val loss 45.643\nEp 1 (Step 011445): Train loss 0.620, Val loss 46.220\nEp 1 (Step 011450): Train loss 0.645, Val loss 46.084\nEp 1 (Step 011455): Train loss 0.700, Val loss 44.986\nEp 1 (Step 011460): Train loss 0.657, Val loss 45.744\nEp 1 (Step 011465): Train loss 0.651, Val loss 45.530\nEp 1 (Step 011470): Train loss 0.571, Val loss 46.058\nEp 1 (Step 011475): Train loss 0.605, Val loss 46.885\nEp 1 (Step 011480): Train loss 0.560, Val loss 45.998\nEp 1 (Step 011485): Train loss 0.606, Val loss 46.036\nEp 1 (Step 011490): Train loss 0.706, Val loss 45.508\nEp 1 (Step 011495): Train loss 0.604, Val loss 45.583\nEp 1 (Step 011500): Train loss 0.665, Val loss 45.447\nEp 1 (Step 011505): Train loss 0.639, Val loss 46.257\nEp 1 (Step 011510): Train loss 0.635, Val loss 46.064\nEp 1 (Step 011515): Train loss 0.667, Val loss 45.925\nEp 1 (Step 011520): Train loss 0.616, Val loss 45.682\nEp 1 (Step 011525): Train loss 0.631, Val loss 45.188\nEp 1 (Step 011530): Train loss 0.629, Val loss 46.186\nEp 1 (Step 011535): Train loss 0.588, Val loss 45.464\nEp 1 (Step 011540): Train loss 0.645, Val loss 45.932\nEp 1 (Step 011545): Train loss 0.662, Val loss 45.146\nEp 1 (Step 011550): Train loss 0.611, Val loss 45.741\nEp 1 (Step 011555): Train loss 0.619, Val loss 46.138\nEp 1 (Step 011560): Train loss 0.649, Val loss 44.681\nEp 1 (Step 011565): Train loss 0.625, Val loss 45.310\nEp 1 (Step 011570): Train loss 0.689, Val loss 45.203\nEp 1 (Step 011575): Train loss 0.674, Val loss 45.188\nEp 1 (Step 011580): Train loss 0.638, Val loss 45.885\nEp 1 (Step 011585): Train loss 0.646, Val loss 46.200\nEp 1 (Step 011590): Train loss 0.633, Val loss 46.115\nEp 1 (Step 011595): Train loss 0.679, Val loss 45.374\nEp 1 (Step 011600): Train loss 0.648, Val loss 46.553\nEp 1 (Step 011605): Train loss 0.636, Val loss 45.454\nEp 1 (Step 011610): Train loss 0.660, Val loss 45.744\nEp 1 (Step 011615): Train loss 0.673, Val loss 45.819\nEp 1 (Step 011620): Train loss 0.633, Val loss 45.799\nEp 1 (Step 011625): Train loss 0.710, Val loss 45.786\nEp 1 (Step 011630): Train loss 0.664, Val loss 45.164\nEp 1 (Step 011635): Train loss 0.655, Val loss 46.257\nEp 1 (Step 011640): Train loss 0.639, Val loss 46.145\nEp 1 (Step 011645): Train loss 0.661, Val loss 46.015\nEp 1 (Step 011650): Train loss 0.646, Val loss 45.936\nEp 1 (Step 011655): Train loss 0.607, Val loss 46.233\nEp 1 (Step 011660): Train loss 0.677, Val loss 45.320\nEp 1 (Step 011665): Train loss 0.630, Val loss 45.830\nEp 1 (Step 011670): Train loss 0.592, Val loss 45.914\nEp 1 (Step 011675): Train loss 0.608, Val loss 45.444\nEp 1 (Step 011680): Train loss 0.598, Val loss 45.462\nEp 1 (Step 011685): Train loss 0.594, Val loss 46.086\nEp 1 (Step 011690): Train loss 0.640, Val loss 45.485\nEp 1 (Step 011695): Train loss 0.614, Val loss 46.147\nEp 1 (Step 011700): Train loss 0.649, Val loss 45.355\nEp 1 (Step 011705): Train loss 0.572, Val loss 45.952\nEp 1 (Step 011710): Train loss 0.638, Val loss 46.095\nEp 1 (Step 011715): Train loss 0.652, Val loss 44.982\nEp 1 (Step 011720): Train loss 0.708, Val loss 45.810\nEp 1 (Step 011725): Train loss 0.660, Val loss 45.597\nEp 1 (Step 011730): Train loss 0.628, Val loss 45.520\nEp 1 (Step 011735): Train loss 0.589, Val loss 44.741\nEp 1 (Step 011740): Train loss 0.608, Val loss 45.709\nEp 1 (Step 011745): Train loss 0.598, Val loss 45.301\nEp 1 (Step 011750): Train loss 0.631, Val loss 46.018\nEp 1 (Step 011755): Train loss 0.612, Val loss 45.900\nEp 1 (Step 011760): Train loss 0.620, Val loss 46.224\nEp 1 (Step 011765): Train loss 0.586, Val loss 46.124\nEp 1 (Step 011770): Train loss 0.630, Val loss 46.111\nEp 1 (Step 011775): Train loss 0.632, Val loss 45.925\nEp 1 (Step 011780): Train loss 0.591, Val loss 46.016\nEp 1 (Step 011785): Train loss 0.602, Val loss 45.375\nEp 1 (Step 011790): Train loss 0.598, Val loss 45.571\nEp 1 (Step 011795): Train loss 0.649, Val loss 45.933\nEp 1 (Step 011800): Train loss 0.640, Val loss 45.462\nEp 1 (Step 011805): Train loss 0.600, Val loss 45.752\nEp 1 (Step 011810): Train loss 0.670, Val loss 46.922\nEp 1 (Step 011815): Train loss 0.640, Val loss 44.579\nEp 1 (Step 011820): Train loss 0.668, Val loss 45.545\nEp 1 (Step 011825): Train loss 0.606, Val loss 45.071\nEp 1 (Step 011830): Train loss 0.599, Val loss 45.718\nEp 1 (Step 011835): Train loss 0.667, Val loss 46.033\nEp 1 (Step 011840): Train loss 0.605, Val loss 46.287\nEp 1 (Step 011845): Train loss 0.639, Val loss 46.732\nEp 1 (Step 011850): Train loss 0.622, Val loss 45.466\nEp 1 (Step 011855): Train loss 0.703, Val loss 45.520\nEp 1 (Step 011860): Train loss 0.651, Val loss 46.056\nEp 1 (Step 011865): Train loss 0.621, Val loss 45.708\nEp 1 (Step 011870): Train loss 0.559, Val loss 46.412\nEp 1 (Step 011875): Train loss 0.611, Val loss 46.221\nEp 1 (Step 011880): Train loss 0.584, Val loss 45.652\nEp 1 (Step 011885): Train loss 0.635, Val loss 45.950\nEp 1 (Step 011890): Train loss 0.553, Val loss 45.857\nEp 1 (Step 011895): Train loss 0.598, Val loss 45.981\nEp 1 (Step 011900): Train loss 0.670, Val loss 46.559\nEp 1 (Step 011905): Train loss 0.598, Val loss 45.452\nEp 1 (Step 011910): Train loss 0.652, Val loss 46.333\nEp 1 (Step 011915): Train loss 0.682, Val loss 46.372\nEp 1 (Step 011920): Train loss 0.574, Val loss 46.601\nEp 1 (Step 011925): Train loss 0.651, Val loss 45.965\nEp 1 (Step 011930): Train loss 0.642, Val loss 46.526\nEp 1 (Step 011935): Train loss 0.631, Val loss 46.643\nEp 1 (Step 011940): Train loss 0.625, Val loss 45.726\nEp 1 (Step 011945): Train loss 0.652, Val loss 46.402\nEp 1 (Step 011950): Train loss 0.625, Val loss 46.105\nEp 1 (Step 011955): Train loss 0.695, Val loss 46.490\nEp 1 (Step 011960): Train loss 0.633, Val loss 45.785\nEp 1 (Step 011965): Train loss 0.655, Val loss 45.481\nEp 1 (Step 011970): Train loss 0.583, Val loss 45.493\nEp 1 (Step 011975): Train loss 0.579, Val loss 46.243\nEp 1 (Step 011980): Train loss 0.606, Val loss 45.817\nEp 1 (Step 011985): Train loss 0.615, Val loss 45.355\nEp 1 (Step 011990): Train loss 0.649, Val loss 45.736\nEp 1 (Step 011995): Train loss 0.679, Val loss 45.899\nEp 1 (Step 012000): Train loss 0.720, Val loss 46.446\nEp 1 (Step 012005): Train loss 0.615, Val loss 45.850\nEp 1 (Step 012010): Train loss 0.627, Val loss 45.784\nEp 1 (Step 012015): Train loss 0.594, Val loss 46.238\nEp 1 (Step 012020): Train loss 0.632, Val loss 46.007\nEp 1 (Step 012025): Train loss 0.688, Val loss 46.133\nEp 1 (Step 012030): Train loss 0.664, Val loss 46.605\nEp 1 (Step 012035): Train loss 0.575, Val loss 45.456\nEp 1 (Step 012040): Train loss 0.602, Val loss 44.717\nEp 1 (Step 012045): Train loss 0.605, Val loss 45.780\nEp 1 (Step 012050): Train loss 0.678, Val loss 45.748\nEp 1 (Step 012055): Train loss 0.646, Val loss 45.989\nEp 1 (Step 012060): Train loss 0.632, Val loss 46.784\nEp 1 (Step 012065): Train loss 0.559, Val loss 46.241\nEp 1 (Step 012070): Train loss 0.618, Val loss 45.477\nEp 1 (Step 012075): Train loss 0.651, Val loss 45.798\nEp 1 (Step 012080): Train loss 0.573, Val loss 46.631\nEp 1 (Step 012085): Train loss 0.581, Val loss 45.401\nEp 1 (Step 012090): Train loss 0.651, Val loss 45.421\nEp 1 (Step 012095): Train loss 0.615, Val loss 45.970\nEp 1 (Step 012100): Train loss 0.635, Val loss 45.535\nEp 1 (Step 012105): Train loss 0.636, Val loss 45.866\nEp 1 (Step 012110): Train loss 0.638, Val loss 46.486\nEp 1 (Step 012115): Train loss 0.572, Val loss 45.375\nEp 1 (Step 012120): Train loss 0.551, Val loss 45.671\nEp 1 (Step 012125): Train loss 0.654, Val loss 45.025\nEp 1 (Step 012130): Train loss 0.644, Val loss 46.296\nEp 1 (Step 012135): Train loss 0.655, Val loss 46.034\nEp 1 (Step 012140): Train loss 0.621, Val loss 45.698\nEp 1 (Step 012145): Train loss 0.590, Val loss 45.626\nEp 1 (Step 012150): Train loss 0.687, Val loss 46.010\nEp 1 (Step 012155): Train loss 0.578, Val loss 46.338\nEp 1 (Step 012160): Train loss 0.605, Val loss 45.519\nEp 1 (Step 012165): Train loss 0.605, Val loss 45.897\nEp 1 (Step 012170): Train loss 0.675, Val loss 46.398\nEp 1 (Step 012175): Train loss 0.679, Val loss 45.803\nEp 1 (Step 012180): Train loss 0.602, Val loss 45.871\nEp 1 (Step 012185): Train loss 0.610, Val loss 45.745\nEp 1 (Step 012190): Train loss 0.533, Val loss 46.228\nEp 1 (Step 012195): Train loss 0.573, Val loss 46.419\nEp 1 (Step 012200): Train loss 0.616, Val loss 46.666\nEp 1 (Step 012205): Train loss 0.625, Val loss 46.284\nEp 1 (Step 012210): Train loss 0.657, Val loss 46.062\nEp 1 (Step 012215): Train loss 0.635, Val loss 46.097\nEp 1 (Step 012220): Train loss 0.596, Val loss 45.701\nEp 1 (Step 012225): Train loss 0.639, Val loss 45.432\nEp 1 (Step 012230): Train loss 0.633, Val loss 45.408\nEp 1 (Step 012235): Train loss 0.648, Val loss 45.375\nEp 1 (Step 012240): Train loss 0.628, Val loss 45.203\nEp 1 (Step 012245): Train loss 0.512, Val loss 46.193\nEp 1 (Step 012250): Train loss 0.585, Val loss 46.483\nEp 1 (Step 012255): Train loss 0.628, Val loss 45.939\nEp 1 (Step 012260): Train loss 0.585, Val loss 46.400\nEp 1 (Step 012265): Train loss 0.556, Val loss 45.950\nEp 1 (Step 012270): Train loss 0.622, Val loss 46.391\nEp 1 (Step 012275): Train loss 0.662, Val loss 45.673\nEp 1 (Step 012280): Train loss 0.647, Val loss 46.074\nEp 1 (Step 012285): Train loss 0.623, Val loss 46.352\nEp 1 (Step 012290): Train loss 0.620, Val loss 45.794\nEp 1 (Step 012295): Train loss 0.609, Val loss 45.712\nEp 1 (Step 012300): Train loss 0.587, Val loss 46.125\nEp 1 (Step 012305): Train loss 0.644, Val loss 46.589\nEp 1 (Step 012310): Train loss 0.619, Val loss 45.951\nEp 1 (Step 012315): Train loss 0.593, Val loss 46.564\nEp 1 (Step 012320): Train loss 0.636, Val loss 45.608\nEp 1 (Step 012325): Train loss 0.584, Val loss 45.749\nEp 1 (Step 012330): Train loss 0.586, Val loss 45.952\nEp 1 (Step 012335): Train loss 0.611, Val loss 45.435\nEp 1 (Step 012340): Train loss 0.624, Val loss 46.242\nEp 1 (Step 012345): Train loss 0.579, Val loss 46.372\nEp 1 (Step 012350): Train loss 0.557, Val loss 45.547\nEp 1 (Step 012355): Train loss 0.614, Val loss 45.789\nEp 1 (Step 012360): Train loss 0.636, Val loss 46.934\nEp 1 (Step 012365): Train loss 0.648, Val loss 46.358\nEp 1 (Step 012370): Train loss 0.612, Val loss 45.334\nEp 1 (Step 012375): Train loss 0.633, Val loss 46.077\nEp 1 (Step 012380): Train loss 0.544, Val loss 45.493\nEp 1 (Step 012385): Train loss 0.618, Val loss 45.320\nEp 1 (Step 012390): Train loss 0.614, Val loss 45.770\nEp 1 (Step 012395): Train loss 0.668, Val loss 45.789\nEp 1 (Step 012400): Train loss 0.646, Val loss 46.475\nEp 1 (Step 012405): Train loss 0.601, Val loss 46.396\nEp 1 (Step 012410): Train loss 0.661, Val loss 46.252\nEp 1 (Step 012415): Train loss 0.604, Val loss 46.618\nEp 1 (Step 012420): Train loss 0.569, Val loss 46.366\nEp 1 (Step 012425): Train loss 0.621, Val loss 45.461\nEp 1 (Step 012430): Train loss 0.638, Val loss 46.719\nEp 1 (Step 012435): Train loss 0.578, Val loss 45.604\nEp 1 (Step 012440): Train loss 0.550, Val loss 45.547\nEp 1 (Step 012445): Train loss 0.557, Val loss 45.695\nEp 1 (Step 012450): Train loss 0.622, Val loss 46.051\nEp 1 (Step 012455): Train loss 0.586, Val loss 46.337\nEp 1 (Step 012460): Train loss 0.566, Val loss 45.219\nEp 1 (Step 012465): Train loss 0.584, Val loss 46.217\nEp 1 (Step 012470): Train loss 0.565, Val loss 46.266\nEp 1 (Step 012475): Train loss 0.587, Val loss 46.201\nEp 1 (Step 012480): Train loss 0.579, Val loss 45.958\nEp 1 (Step 012485): Train loss 0.657, Val loss 45.906\nEp 1 (Step 012490): Train loss 0.635, Val loss 45.383\nEp 1 (Step 012495): Train loss 0.576, Val loss 46.194\nEp 1 (Step 012500): Train loss 0.639, Val loss 46.109\nEp 1 (Step 012505): Train loss 0.622, Val loss 45.467\nEp 1 (Step 012510): Train loss 0.599, Val loss 46.147\nEp 1 (Step 012515): Train loss 0.581, Val loss 45.423\nEp 1 (Step 012520): Train loss 0.621, Val loss 45.920\nEp 1 (Step 012525): Train loss 0.602, Val loss 46.408\nEp 1 (Step 012530): Train loss 0.546, Val loss 46.294\nEp 1 (Step 012535): Train loss 0.667, Val loss 46.043\nEp 1 (Step 012540): Train loss 0.592, Val loss 45.911\nEp 1 (Step 012545): Train loss 0.664, Val loss 46.401\nEp 1 (Step 012550): Train loss 0.668, Val loss 46.414\nEp 1 (Step 012555): Train loss 0.569, Val loss 45.539\nEp 1 (Step 012560): Train loss 0.632, Val loss 46.213\nEp 1 (Step 012565): Train loss 0.589, Val loss 45.845\nEp 1 (Step 012570): Train loss 0.597, Val loss 46.260\nEp 1 (Step 012575): Train loss 0.647, Val loss 45.942\nEp 1 (Step 012580): Train loss 0.628, Val loss 46.275\nEp 1 (Step 012585): Train loss 0.586, Val loss 45.839\nEp 1 (Step 012590): Train loss 0.570, Val loss 46.387\nEp 1 (Step 012595): Train loss 0.561, Val loss 46.336\nEp 1 (Step 012600): Train loss 0.580, Val loss 46.261\nEp 1 (Step 012605): Train loss 0.613, Val loss 46.383\nEp 1 (Step 012610): Train loss 0.575, Val loss 46.454\nEp 1 (Step 012615): Train loss 0.630, Val loss 45.700\nEp 1 (Step 012620): Train loss 0.616, Val loss 46.257\nEp 1 (Step 012625): Train loss 0.594, Val loss 46.244\nEp 1 (Step 012630): Train loss 0.561, Val loss 46.413\nEp 1 (Step 012635): Train loss 0.627, Val loss 47.331\nEp 1 (Step 012640): Train loss 0.618, Val loss 45.984\nEp 1 (Step 012645): Train loss 0.583, Val loss 46.130\nEp 1 (Step 012650): Train loss 0.607, Val loss 46.165\nEp 1 (Step 012655): Train loss 0.617, Val loss 45.700\nEp 1 (Step 012660): Train loss 0.565, Val loss 46.032\nEp 1 (Step 012665): Train loss 0.588, Val loss 45.862\nEp 1 (Step 012670): Train loss 0.608, Val loss 45.522\nEp 1 (Step 012675): Train loss 0.594, Val loss 46.367\nEp 1 (Step 012680): Train loss 0.612, Val loss 45.445\nEp 1 (Step 012685): Train loss 0.601, Val loss 46.225\nEp 1 (Step 012690): Train loss 0.613, Val loss 45.785\nEp 1 (Step 012695): Train loss 0.568, Val loss 46.822\nEp 1 (Step 012700): Train loss 0.643, Val loss 45.599\nEp 1 (Step 012705): Train loss 0.571, Val loss 45.634\nEp 1 (Step 012710): Train loss 0.641, Val loss 46.040\nEp 1 (Step 012715): Train loss 0.543, Val loss 45.510\nEp 1 (Step 012720): Train loss 0.581, Val loss 45.895\nEp 1 (Step 012725): Train loss 0.655, Val loss 45.836\nEp 1 (Step 012730): Train loss 0.586, Val loss 45.867\nEp 1 (Step 012735): Train loss 0.636, Val loss 46.526\nEp 1 (Step 012740): Train loss 0.636, Val loss 46.480\nEp 1 (Step 012745): Train loss 0.553, Val loss 45.879\nEp 1 (Step 012750): Train loss 0.584, Val loss 45.544\nEp 1 (Step 012755): Train loss 0.648, Val loss 45.905\nEp 1 (Step 012760): Train loss 0.593, Val loss 46.026\nEp 1 (Step 012765): Train loss 0.591, Val loss 46.410\nEp 1 (Step 012770): Train loss 0.601, Val loss 46.077\nEp 1 (Step 012775): Train loss 0.545, Val loss 45.901\nEp 1 (Step 012780): Train loss 0.604, Val loss 45.920\nEp 1 (Step 012785): Train loss 0.627, Val loss 46.732\nEp 1 (Step 012790): Train loss 0.646, Val loss 46.222\nEp 1 (Step 012795): Train loss 0.652, Val loss 46.358\nEp 1 (Step 012800): Train loss 0.620, Val loss 46.585\nEp 1 (Step 012805): Train loss 0.542, Val loss 46.332\nEp 1 (Step 012810): Train loss 0.634, Val loss 45.401\nEp 1 (Step 012815): Train loss 0.621, Val loss 45.949\nEp 1 (Step 012820): Train loss 0.556, Val loss 45.578\nEp 1 (Step 012825): Train loss 0.638, Val loss 46.754\nEp 1 (Step 012830): Train loss 0.564, Val loss 46.170\nEp 1 (Step 012835): Train loss 0.622, Val loss 45.753\nEp 1 (Step 012840): Train loss 0.609, Val loss 46.128\nEp 1 (Step 012845): Train loss 0.531, Val loss 46.199\nEp 1 (Step 012850): Train loss 0.587, Val loss 46.566\nEp 1 (Step 012855): Train loss 0.651, Val loss 45.570\nEp 1 (Step 012860): Train loss 0.597, Val loss 45.677\nEp 1 (Step 012865): Train loss 0.572, Val loss 45.949\nEp 1 (Step 012870): Train loss 0.611, Val loss 46.357\nEp 1 (Step 012875): Train loss 0.635, Val loss 45.789\nEp 1 (Step 012880): Train loss 0.660, Val loss 46.150\nEp 1 (Step 012885): Train loss 0.586, Val loss 45.984\nEp 1 (Step 012890): Train loss 0.573, Val loss 46.402\nEp 1 (Step 012895): Train loss 0.609, Val loss 45.740\nEp 1 (Step 012900): Train loss 0.548, Val loss 45.673\nEp 1 (Step 012905): Train loss 0.572, Val loss 46.232\nEp 1 (Step 012910): Train loss 0.586, Val loss 46.576\nEp 1 (Step 012915): Train loss 0.591, Val loss 45.628\nEp 1 (Step 012920): Train loss 0.565, Val loss 46.305\nEp 1 (Step 012925): Train loss 0.616, Val loss 46.626\nEp 1 (Step 012930): Train loss 0.578, Val loss 46.204\nEp 1 (Step 012935): Train loss 0.699, Val loss 46.287\nEp 1 (Step 012940): Train loss 0.581, Val loss 45.551\nEp 1 (Step 012945): Train loss 0.654, Val loss 46.053\nEp 1 (Step 012950): Train loss 0.652, Val loss 46.159\nEp 1 (Step 012955): Train loss 0.642, Val loss 45.947\nEp 1 (Step 012960): Train loss 0.575, Val loss 45.267\nEp 1 (Step 012965): Train loss 0.597, Val loss 46.457\nEp 1 (Step 012970): Train loss 0.591, Val loss 46.964\nEp 1 (Step 012975): Train loss 0.593, Val loss 46.423\nEp 1 (Step 012980): Train loss 0.602, Val loss 46.551\nEp 1 (Step 012985): Train loss 0.623, Val loss 45.630\nEp 1 (Step 012990): Train loss 0.532, Val loss 46.379\nEp 1 (Step 012995): Train loss 0.556, Val loss 45.661\nEp 1 (Step 013000): Train loss 0.609, Val loss 46.138\nEp 1 (Step 013005): Train loss 0.562, Val loss 45.887\nEp 1 (Step 013010): Train loss 0.600, Val loss 45.639\nEp 1 (Step 013015): Train loss 0.597, Val loss 46.468\nEp 1 (Step 013020): Train loss 0.565, Val loss 46.861\nEp 1 (Step 013025): Train loss 0.580, Val loss 46.596\nEp 1 (Step 013030): Train loss 0.633, Val loss 46.272\nEp 1 (Step 013035): Train loss 0.533, Val loss 45.454\nEp 1 (Step 013040): Train loss 0.587, Val loss 45.561\nEp 1 (Step 013045): Train loss 0.553, Val loss 46.026\nEp 1 (Step 013050): Train loss 0.597, Val loss 45.672\nEp 1 (Step 013055): Train loss 0.620, Val loss 46.044\nEp 1 (Step 013060): Train loss 0.594, Val loss 45.412\nEp 1 (Step 013065): Train loss 0.638, Val loss 46.191\nEp 1 (Step 013070): Train loss 0.565, Val loss 46.470\nEp 1 (Step 013075): Train loss 0.587, Val loss 45.640\nEp 1 (Step 013080): Train loss 0.592, Val loss 46.343\nEp 1 (Step 013085): Train loss 0.622, Val loss 45.843\nEp 1 (Step 013090): Train loss 0.569, Val loss 46.296\nEp 1 (Step 013095): Train loss 0.556, Val loss 46.371\nEp 1 (Step 013100): Train loss 0.610, Val loss 46.104\nEp 1 (Step 013105): Train loss 0.661, Val loss 46.101\nEp 1 (Step 013110): Train loss 0.623, Val loss 45.905\nEp 1 (Step 013115): Train loss 0.571, Val loss 46.230\nEp 1 (Step 013120): Train loss 0.604, Val loss 45.809\nEp 1 (Step 013125): Train loss 0.626, Val loss 46.531\nEp 1 (Step 013130): Train loss 0.628, Val loss 46.433\nEp 1 (Step 013135): Train loss 0.545, Val loss 46.134\nEp 1 (Step 013140): Train loss 0.582, Val loss 46.062\nEp 1 (Step 013145): Train loss 0.549, Val loss 46.424\nEp 1 (Step 013150): Train loss 0.557, Val loss 46.686\nEp 1 (Step 013155): Train loss 0.615, Val loss 45.823\nEp 1 (Step 013160): Train loss 0.629, Val loss 46.154\nEp 1 (Step 013165): Train loss 0.618, Val loss 45.695\nEp 1 (Step 013170): Train loss 0.642, Val loss 45.688\nEp 1 (Step 013175): Train loss 0.605, Val loss 45.850\nEp 1 (Step 013180): Train loss 0.613, Val loss 46.114\nEp 1 (Step 013185): Train loss 0.706, Val loss 46.162\nEp 1 (Step 013190): Train loss 0.584, Val loss 46.126\nEp 1 (Step 013195): Train loss 0.599, Val loss 46.033\nEp 1 (Step 013200): Train loss 0.545, Val loss 45.813\nEp 1 (Step 013205): Train loss 0.601, Val loss 45.913\nEp 1 (Step 013210): Train loss 0.617, Val loss 45.502\nEp 1 (Step 013215): Train loss 0.560, Val loss 45.944\nEp 1 (Step 013220): Train loss 0.610, Val loss 46.443\nEp 1 (Step 013225): Train loss 0.614, Val loss 46.074\nEp 1 (Step 013230): Train loss 0.612, Val loss 46.162\nEp 1 (Step 013235): Train loss 0.566, Val loss 45.953\nEp 1 (Step 013240): Train loss 0.645, Val loss 46.580\nEp 1 (Step 013245): Train loss 0.579, Val loss 45.960\nEp 1 (Step 013250): Train loss 0.566, Val loss 45.291\nEp 1 (Step 013255): Train loss 0.630, Val loss 45.797\nEp 1 (Step 013260): Train loss 0.618, Val loss 46.730\nEp 1 (Step 013265): Train loss 0.592, Val loss 46.192\nEp 1 (Step 013270): Train loss 0.581, Val loss 46.835\nEp 1 (Step 013275): Train loss 0.576, Val loss 46.038\nEp 1 (Step 013280): Train loss 0.601, Val loss 45.637\nEp 1 (Step 013285): Train loss 0.522, Val loss 46.114\nEp 1 (Step 013290): Train loss 0.602, Val loss 46.185\nEp 1 (Step 013295): Train loss 0.591, Val loss 45.892\nEp 1 (Step 013300): Train loss 0.590, Val loss 46.458\nEp 1 (Step 013305): Train loss 0.624, Val loss 46.284\nEp 1 (Step 013310): Train loss 0.618, Val loss 46.278\nEp 1 (Step 013315): Train loss 0.517, Val loss 46.035\nEp 1 (Step 013320): Train loss 0.553, Val loss 45.790\nEp 1 (Step 013325): Train loss 0.575, Val loss 46.130\nEp 1 (Step 013330): Train loss 0.590, Val loss 46.515\nEp 1 (Step 013335): Train loss 0.567, Val loss 45.201\nEp 1 (Step 013340): Train loss 0.583, Val loss 46.303\nEp 1 (Step 013345): Train loss 0.578, Val loss 45.585\nEp 1 (Step 013350): Train loss 0.628, Val loss 46.464\nEp 1 (Step 013355): Train loss 0.575, Val loss 45.807\nEp 1 (Step 013360): Train loss 0.552, Val loss 46.312\nEp 1 (Step 013365): Train loss 0.641, Val loss 46.109\nEp 1 (Step 013370): Train loss 0.540, Val loss 46.830\nEp 1 (Step 013375): Train loss 0.590, Val loss 45.857\nEp 1 (Step 013380): Train loss 0.622, Val loss 46.511\nEp 1 (Step 013385): Train loss 0.572, Val loss 45.941\nEp 1 (Step 013390): Train loss 0.585, Val loss 45.929\nEp 1 (Step 013395): Train loss 0.578, Val loss 45.565\nEp 1 (Step 013400): Train loss 0.570, Val loss 46.203\nEp 1 (Step 013405): Train loss 0.569, Val loss 45.665\nEp 1 (Step 013410): Train loss 0.624, Val loss 46.190\nEp 1 (Step 013415): Train loss 0.519, Val loss 46.723\nEp 1 (Step 013420): Train loss 0.592, Val loss 46.048\nEp 1 (Step 013425): Train loss 0.588, Val loss 45.296\nEp 1 (Step 013430): Train loss 0.661, Val loss 46.288\nEp 1 (Step 013435): Train loss 0.554, Val loss 46.162\nEp 1 (Step 013440): Train loss 0.532, Val loss 46.232\nEp 1 (Step 013445): Train loss 0.586, Val loss 45.906\nEp 1 (Step 013450): Train loss 0.571, Val loss 45.831\nEp 1 (Step 013455): Train loss 0.569, Val loss 46.224\nEp 1 (Step 013460): Train loss 0.613, Val loss 46.513\nEp 1 (Step 013465): Train loss 0.603, Val loss 46.114\nEp 1 (Step 013470): Train loss 0.647, Val loss 46.417\nEp 1 (Step 013475): Train loss 0.595, Val loss 46.357\nEp 1 (Step 013480): Train loss 0.630, Val loss 46.461\nEp 1 (Step 013485): Train loss 0.555, Val loss 46.322\nEp 1 (Step 013490): Train loss 0.535, Val loss 45.846\nEp 1 (Step 013495): Train loss 0.565, Val loss 46.122\nEp 1 (Step 013500): Train loss 0.563, Val loss 46.813\nEp 1 (Step 013505): Train loss 0.553, Val loss 46.830\nEp 1 (Step 013510): Train loss 0.530, Val loss 45.852\nEp 1 (Step 013515): Train loss 0.574, Val loss 46.089\nEp 1 (Step 013520): Train loss 0.632, Val loss 45.908\nEp 1 (Step 013525): Train loss 0.591, Val loss 46.265\nEp 1 (Step 013530): Train loss 0.640, Val loss 46.309\nEp 1 (Step 013535): Train loss 0.556, Val loss 45.503\nEp 1 (Step 013540): Train loss 0.610, Val loss 46.378\nEp 1 (Step 013545): Train loss 0.564, Val loss 46.034\nEp 1 (Step 013550): Train loss 0.622, Val loss 45.785\nEp 1 (Step 013555): Train loss 0.502, Val loss 46.142\nEp 1 (Step 013560): Train loss 0.584, Val loss 46.443\nEp 1 (Step 013565): Train loss 0.587, Val loss 46.472\nEp 1 (Step 013570): Train loss 0.611, Val loss 45.279\nEp 1 (Step 013575): Train loss 0.586, Val loss 46.466\nEp 1 (Step 013580): Train loss 0.692, Val loss 46.040\nEp 1 (Step 013585): Train loss 0.592, Val loss 46.107\nEp 1 (Step 013590): Train loss 0.592, Val loss 45.832\nEp 1 (Step 013595): Train loss 0.568, Val loss 46.276\nEp 1 (Step 013600): Train loss 0.575, Val loss 45.749\nEp 1 (Step 013605): Train loss 0.623, Val loss 46.117\nEp 1 (Step 013610): Train loss 0.554, Val loss 46.686\nEp 1 (Step 013615): Train loss 0.588, Val loss 46.092\nEp 1 (Step 013620): Train loss 0.578, Val loss 45.520\nEp 1 (Step 013625): Train loss 0.577, Val loss 46.164\nEp 1 (Step 013630): Train loss 0.602, Val loss 46.450\nEp 1 (Step 013635): Train loss 0.570, Val loss 46.628\nEp 1 (Step 013640): Train loss 0.633, Val loss 45.900\nEp 1 (Step 013645): Train loss 0.580, Val loss 46.947\nEp 1 (Step 013650): Train loss 0.625, Val loss 45.703\nEp 1 (Step 013655): Train loss 0.621, Val loss 45.748\nEp 1 (Step 013660): Train loss 0.589, Val loss 45.832\nEp 1 (Step 013665): Train loss 0.606, Val loss 45.925\nEp 1 (Step 013670): Train loss 0.606, Val loss 45.899\nEp 1 (Step 013675): Train loss 0.549, Val loss 46.346\nEp 1 (Step 013680): Train loss 0.576, Val loss 45.374\nEp 1 (Step 013685): Train loss 0.567, Val loss 46.709\nEp 1 (Step 013690): Train loss 0.568, Val loss 45.375\nEp 1 (Step 013695): Train loss 0.541, Val loss 46.099\nEp 1 (Step 013700): Train loss 0.607, Val loss 46.456\nEp 1 (Step 013705): Train loss 0.585, Val loss 46.315\nEp 1 (Step 013710): Train loss 0.581, Val loss 45.951\nEp 1 (Step 013715): Train loss 0.576, Val loss 46.200\nEp 1 (Step 013720): Train loss 0.629, Val loss 46.285\nEp 1 (Step 013725): Train loss 0.585, Val loss 46.429\nEp 1 (Step 013730): Train loss 0.609, Val loss 46.577\nEp 1 (Step 013735): Train loss 0.591, Val loss 46.287\nEp 1 (Step 013740): Train loss 0.602, Val loss 46.613\nEp 1 (Step 013745): Train loss 0.596, Val loss 45.438\nEp 1 (Step 013750): Train loss 0.623, Val loss 46.558\nEp 1 (Step 013755): Train loss 0.583, Val loss 46.409\nEp 1 (Step 013760): Train loss 0.596, Val loss 47.177\nEp 1 (Step 013765): Train loss 0.593, Val loss 45.994\nEp 1 (Step 013770): Train loss 0.654, Val loss 46.372\nEp 1 (Step 013775): Train loss 0.585, Val loss 45.562\nEp 1 (Step 013780): Train loss 0.568, Val loss 45.414\nEp 1 (Step 013785): Train loss 0.637, Val loss 46.435\nEp 1 (Step 013790): Train loss 0.596, Val loss 45.518\nEp 1 (Step 013795): Train loss 0.575, Val loss 45.462\nEp 1 (Step 013800): Train loss 0.506, Val loss 46.719\nEp 1 (Step 013805): Train loss 0.590, Val loss 45.668\nEp 1 (Step 013810): Train loss 0.523, Val loss 45.506\nEp 1 (Step 013815): Train loss 0.552, Val loss 46.370\nEp 1 (Step 013820): Train loss 0.592, Val loss 45.719\nEp 1 (Step 013825): Train loss 0.551, Val loss 46.432\nEp 1 (Step 013830): Train loss 0.632, Val loss 46.410\nEp 1 (Step 013835): Train loss 0.573, Val loss 46.046\nEp 1 (Step 013840): Train loss 0.551, Val loss 45.582\nEp 1 (Step 013845): Train loss 0.527, Val loss 46.043\nEp 1 (Step 013850): Train loss 0.545, Val loss 46.194\nEp 1 (Step 013855): Train loss 0.546, Val loss 45.993\nEp 1 (Step 013860): Train loss 0.570, Val loss 45.561\nEp 1 (Step 013865): Train loss 0.570, Val loss 45.944\nEp 1 (Step 013870): Train loss 0.518, Val loss 46.447\nEp 1 (Step 013875): Train loss 0.578, Val loss 46.023\nEp 1 (Step 013880): Train loss 0.580, Val loss 45.983\nEp 1 (Step 013885): Train loss 0.600, Val loss 45.963\nEp 1 (Step 013890): Train loss 0.663, Val loss 45.866\nEp 1 (Step 013895): Train loss 0.578, Val loss 46.184\nEp 1 (Step 013900): Train loss 0.617, Val loss 46.669\nEp 1 (Step 013905): Train loss 0.543, Val loss 46.650\nEp 1 (Step 013910): Train loss 0.536, Val loss 46.084\nEp 1 (Step 013915): Train loss 0.538, Val loss 45.409\nEp 1 (Step 013920): Train loss 0.623, Val loss 46.202\nEp 1 (Step 013925): Train loss 0.552, Val loss 46.225\nEp 1 (Step 013930): Train loss 0.554, Val loss 45.868\nEp 1 (Step 013935): Train loss 0.590, Val loss 45.983\nEp 1 (Step 013940): Train loss 0.616, Val loss 45.816\nEp 1 (Step 013945): Train loss 0.615, Val loss 46.343\nEp 1 (Step 013950): Train loss 0.591, Val loss 45.347\nEp 1 (Step 013955): Train loss 0.581, Val loss 46.435\nEp 1 (Step 013960): Train loss 0.603, Val loss 45.801\nEp 1 (Step 013965): Train loss 0.542, Val loss 46.165\nEp 1 (Step 013970): Train loss 0.609, Val loss 45.752\nEp 1 (Step 013975): Train loss 0.622, Val loss 46.224\nEp 1 (Step 013980): Train loss 0.528, Val loss 45.766\nEp 1 (Step 013985): Train loss 0.608, Val loss 46.317\nEp 1 (Step 013990): Train loss 0.501, Val loss 46.132\nEp 1 (Step 013995): Train loss 0.532, Val loss 46.488\nEp 1 (Step 014000): Train loss 0.585, Val loss 45.912\nEp 1 (Step 014005): Train loss 0.535, Val loss 45.907\nEp 1 (Step 014010): Train loss 0.629, Val loss 45.668\nEp 1 (Step 014015): Train loss 0.611, Val loss 46.000\nEp 1 (Step 014020): Train loss 0.577, Val loss 45.664\nEp 1 (Step 014025): Train loss 0.532, Val loss 45.541\nEp 1 (Step 014030): Train loss 0.630, Val loss 46.431\nEp 1 (Step 014035): Train loss 0.579, Val loss 46.330\nEp 1 (Step 014040): Train loss 0.549, Val loss 45.984\nEp 1 (Step 014045): Train loss 0.577, Val loss 46.662\nEp 1 (Step 014050): Train loss 0.630, Val loss 46.187\nEp 1 (Step 014055): Train loss 0.586, Val loss 45.986\nEp 1 (Step 014060): Train loss 0.568, Val loss 46.250\nEp 1 (Step 014065): Train loss 0.588, Val loss 47.347\nEp 1 (Step 014070): Train loss 0.536, Val loss 46.079\nEp 1 (Step 014075): Train loss 0.599, Val loss 46.219\nEp 1 (Step 014080): Train loss 0.605, Val loss 46.146\nEp 1 (Step 014085): Train loss 0.590, Val loss 46.002\nEp 1 (Step 014090): Train loss 0.541, Val loss 45.807\nEp 1 (Step 014095): Train loss 0.555, Val loss 46.998\nEp 1 (Step 014100): Train loss 0.569, Val loss 46.414\nEp 1 (Step 014105): Train loss 0.558, Val loss 46.121\nEp 1 (Step 014110): Train loss 0.606, Val loss 46.463\nEp 1 (Step 014115): Train loss 0.546, Val loss 46.149\nEp 1 (Step 014120): Train loss 0.583, Val loss 46.522\nEp 1 (Step 014125): Train loss 0.548, Val loss 45.422\nEp 1 (Step 014130): Train loss 0.610, Val loss 45.684\nEp 1 (Step 014135): Train loss 0.536, Val loss 45.913\nEp 1 (Step 014140): Train loss 0.619, Val loss 46.023\nEp 1 (Step 014145): Train loss 0.587, Val loss 45.235\nEp 1 (Step 014150): Train loss 0.533, Val loss 46.018\nEp 1 (Step 014155): Train loss 0.526, Val loss 45.940\nEp 1 (Step 014160): Train loss 0.563, Val loss 46.336\nEp 1 (Step 014165): Train loss 0.563, Val loss 46.258\nEp 1 (Step 014170): Train loss 0.601, Val loss 45.001\nEp 1 (Step 014175): Train loss 0.612, Val loss 46.511\nEp 1 (Step 014180): Train loss 0.574, Val loss 45.402\nEp 1 (Step 014185): Train loss 0.600, Val loss 46.168\nEp 1 (Step 014190): Train loss 0.526, Val loss 46.236\nEp 1 (Step 014195): Train loss 0.541, Val loss 46.449\nEp 1 (Step 014200): Train loss 0.591, Val loss 46.469\nEp 1 (Step 014205): Train loss 0.527, Val loss 46.030\nEp 1 (Step 014210): Train loss 0.567, Val loss 45.976\nEp 1 (Step 014215): Train loss 0.568, Val loss 45.729\nEp 1 (Step 014220): Train loss 0.596, Val loss 46.292\nEp 1 (Step 014225): Train loss 0.557, Val loss 46.800\nEp 1 (Step 014230): Train loss 0.574, Val loss 45.765\nEp 1 (Step 014235): Train loss 0.593, Val loss 45.974\nEp 1 (Step 014240): Train loss 0.524, Val loss 45.825\nEp 1 (Step 014245): Train loss 0.565, Val loss 45.574\nEp 1 (Step 014250): Train loss 0.541, Val loss 46.459\nEp 1 (Step 014255): Train loss 0.556, Val loss 46.570\nEp 1 (Step 014260): Train loss 0.551, Val loss 46.211\nEp 1 (Step 014265): Train loss 0.574, Val loss 45.606\nEp 1 (Step 014270): Train loss 0.544, Val loss 46.343\nEp 1 (Step 014275): Train loss 0.516, Val loss 46.371\nEp 1 (Step 014280): Train loss 0.574, Val loss 46.240\nEp 1 (Step 014285): Train loss 0.531, Val loss 46.540\nEp 1 (Step 014290): Train loss 0.557, Val loss 46.405\nEp 1 (Step 014295): Train loss 0.547, Val loss 46.360\nEp 1 (Step 014300): Train loss 0.575, Val loss 46.102\nEp 1 (Step 014305): Train loss 0.551, Val loss 46.447\nEp 1 (Step 014310): Train loss 0.606, Val loss 45.806\nEp 1 (Step 014315): Train loss 0.568, Val loss 46.593\nEp 1 (Step 014320): Train loss 0.535, Val loss 46.378\nEp 1 (Step 014325): Train loss 0.535, Val loss 46.103\nEp 1 (Step 014330): Train loss 0.553, Val loss 45.813\nEp 1 (Step 014335): Train loss 0.573, Val loss 45.550\nEp 1 (Step 014340): Train loss 0.634, Val loss 46.250\nEp 1 (Step 014345): Train loss 0.549, Val loss 46.176\nEp 1 (Step 014350): Train loss 0.585, Val loss 45.981\nEp 1 (Step 014355): Train loss 0.543, Val loss 45.315\nEp 1 (Step 014360): Train loss 0.542, Val loss 46.145\nEp 1 (Step 014365): Train loss 0.550, Val loss 47.019\nEp 1 (Step 014370): Train loss 0.606, Val loss 45.694\nEp 1 (Step 014375): Train loss 0.551, Val loss 45.422\nEp 1 (Step 014380): Train loss 0.567, Val loss 46.253\nEp 1 (Step 014385): Train loss 0.598, Val loss 45.981\nEp 1 (Step 014390): Train loss 0.517, Val loss 45.938\nEp 1 (Step 014395): Train loss 0.599, Val loss 46.530\nEp 1 (Step 014400): Train loss 0.537, Val loss 46.286\nEp 1 (Step 014405): Train loss 0.519, Val loss 46.435\nEp 1 (Step 014410): Train loss 0.548, Val loss 46.375\nEp 1 (Step 014415): Train loss 0.518, Val loss 45.531\nEp 1 (Step 014420): Train loss 0.619, Val loss 46.953\nEp 1 (Step 014425): Train loss 0.594, Val loss 46.367\nEp 1 (Step 014430): Train loss 0.537, Val loss 45.899\nEp 1 (Step 014435): Train loss 0.646, Val loss 46.650\nEp 1 (Step 014440): Train loss 0.587, Val loss 47.171\nEp 1 (Step 014445): Train loss 0.577, Val loss 46.676\nEp 1 (Step 014450): Train loss 0.636, Val loss 46.270\nEp 1 (Step 014455): Train loss 0.567, Val loss 46.544\nEp 1 (Step 014460): Train loss 0.511, Val loss 45.943\nEp 1 (Step 014465): Train loss 0.549, Val loss 46.169\nEp 1 (Step 014470): Train loss 0.537, Val loss 46.943\nEp 1 (Step 014475): Train loss 0.541, Val loss 45.993\nEp 1 (Step 014480): Train loss 0.581, Val loss 45.680\nEp 1 (Step 014485): Train loss 0.546, Val loss 46.397\nEp 1 (Step 014490): Train loss 0.606, Val loss 46.433\nEp 1 (Step 014495): Train loss 0.578, Val loss 46.512\nEp 1 (Step 014500): Train loss 0.660, Val loss 46.036\nEp 1 (Step 014505): Train loss 0.629, Val loss 46.633\nEp 1 (Step 014510): Train loss 0.568, Val loss 46.124\nEp 1 (Step 014515): Train loss 0.601, Val loss 45.397\nEp 1 (Step 014520): Train loss 0.561, Val loss 46.014\nEp 1 (Step 014525): Train loss 0.522, Val loss 46.462\nEp 1 (Step 014530): Train loss 0.532, Val loss 46.410\nEp 1 (Step 014535): Train loss 0.530, Val loss 46.727\nEp 1 (Step 014540): Train loss 0.544, Val loss 46.108\nEp 1 (Step 014545): Train loss 0.558, Val loss 46.235\nEp 1 (Step 014550): Train loss 0.544, Val loss 46.058\nEp 1 (Step 014555): Train loss 0.579, Val loss 45.138\nEp 1 (Step 014560): Train loss 0.564, Val loss 46.606\nEp 1 (Step 014565): Train loss 0.587, Val loss 45.928\nEp 1 (Step 014570): Train loss 0.584, Val loss 45.803\nEp 1 (Step 014575): Train loss 0.598, Val loss 45.689\nEp 1 (Step 014580): Train loss 0.567, Val loss 46.163\nEp 1 (Step 014585): Train loss 0.569, Val loss 45.478\nEp 1 (Step 014590): Train loss 0.593, Val loss 46.823\nEp 1 (Step 014595): Train loss 0.623, Val loss 45.437\nEp 1 (Step 014600): Train loss 0.582, Val loss 46.571\nEp 1 (Step 014605): Train loss 0.586, Val loss 46.409\nEp 1 (Step 014610): Train loss 0.570, Val loss 46.714\nEp 1 (Step 014615): Train loss 0.607, Val loss 46.354\nEp 1 (Step 014620): Train loss 0.590, Val loss 46.182\nEp 1 (Step 014625): Train loss 0.623, Val loss 46.834\nEp 1 (Step 014630): Train loss 0.522, Val loss 46.581\nEp 1 (Step 014635): Train loss 0.540, Val loss 47.350\nEp 1 (Step 014640): Train loss 0.540, Val loss 46.098\nEp 1 (Step 014645): Train loss 0.524, Val loss 46.248\nEp 1 (Step 014650): Train loss 0.583, Val loss 46.448\nEp 1 (Step 014655): Train loss 0.566, Val loss 45.928\nEp 1 (Step 014660): Train loss 0.559, Val loss 46.080\nEp 1 (Step 014665): Train loss 0.547, Val loss 46.297\nEp 1 (Step 014670): Train loss 0.623, Val loss 46.166\nEp 1 (Step 014675): Train loss 0.524, Val loss 47.236\nEp 1 (Step 014680): Train loss 0.620, Val loss 46.179\nEp 1 (Step 014685): Train loss 0.533, Val loss 45.764\nEp 1 (Step 014690): Train loss 0.581, Val loss 46.204\nEp 1 (Step 014695): Train loss 0.551, Val loss 46.166\nEp 1 (Step 014700): Train loss 0.563, Val loss 46.091\nEp 1 (Step 014705): Train loss 0.564, Val loss 47.031\nEp 1 (Step 014710): Train loss 0.570, Val loss 46.390\nEp 1 (Step 014715): Train loss 0.505, Val loss 45.897\nEp 1 (Step 014720): Train loss 0.565, Val loss 46.050\nEp 1 (Step 014725): Train loss 0.566, Val loss 45.742\nEp 1 (Step 014730): Train loss 0.525, Val loss 46.517\nEp 1 (Step 014735): Train loss 0.563, Val loss 46.565\nEp 1 (Step 014740): Train loss 0.587, Val loss 45.930\nEp 1 (Step 014745): Train loss 0.582, Val loss 45.813\nEp 1 (Step 014750): Train loss 0.549, Val loss 46.224\nEp 1 (Step 014755): Train loss 0.570, Val loss 46.561\nEp 1 (Step 014760): Train loss 0.590, Val loss 45.905\nEp 1 (Step 014765): Train loss 0.549, Val loss 46.322\nEp 1 (Step 014770): Train loss 0.553, Val loss 45.940\nEp 1 (Step 014775): Train loss 0.675, Val loss 45.827\nEp 1 (Step 014780): Train loss 0.577, Val loss 45.789\nEp 1 (Step 014785): Train loss 0.491, Val loss 46.539\nEp 1 (Step 014790): Train loss 0.568, Val loss 46.215\nEp 1 (Step 014795): Train loss 0.563, Val loss 46.274\nEp 1 (Step 014800): Train loss 0.504, Val loss 46.206\nEp 1 (Step 014805): Train loss 0.537, Val loss 46.183\nEp 1 (Step 014810): Train loss 0.627, Val loss 45.985\nEp 1 (Step 014815): Train loss 0.551, Val loss 46.834\nEp 1 (Step 014820): Train loss 0.591, Val loss 46.064\nEp 1 (Step 014825): Train loss 0.610, Val loss 46.347\nEp 1 (Step 014830): Train loss 0.537, Val loss 46.595\nEp 1 (Step 014835): Train loss 0.587, Val loss 45.966\nEp 1 (Step 014840): Train loss 0.543, Val loss 46.240\nEp 1 (Step 014845): Train loss 0.608, Val loss 46.584\nEp 1 (Step 014850): Train loss 0.518, Val loss 45.998\nEp 1 (Step 014855): Train loss 0.546, Val loss 46.466\nEp 1 (Step 014860): Train loss 0.550, Val loss 46.218\nEp 1 (Step 014865): Train loss 0.530, Val loss 46.556\nEp 1 (Step 014870): Train loss 0.568, Val loss 46.066\nEp 1 (Step 014875): Train loss 0.611, Val loss 46.631\nEp 1 (Step 014880): Train loss 0.503, Val loss 46.189\nEp 1 (Step 014885): Train loss 0.562, Val loss 46.831\nEp 1 (Step 014890): Train loss 0.539, Val loss 46.352\nEp 1 (Step 014895): Train loss 0.588, Val loss 46.840\nEp 1 (Step 014900): Train loss 0.559, Val loss 46.183\nEp 1 (Step 014905): Train loss 0.547, Val loss 45.801\nEp 1 (Step 014910): Train loss 0.577, Val loss 46.809\nEp 1 (Step 014915): Train loss 0.506, Val loss 45.921\nEp 1 (Step 014920): Train loss 0.548, Val loss 46.412\nEp 1 (Step 014925): Train loss 0.520, Val loss 45.967\nEp 1 (Step 014930): Train loss 0.567, Val loss 46.655\nEp 1 (Step 014935): Train loss 0.588, Val loss 46.099\nEp 1 (Step 014940): Train loss 0.569, Val loss 46.638\nEp 1 (Step 014945): Train loss 0.505, Val loss 46.466\nEp 1 (Step 014950): Train loss 0.642, Val loss 46.022\nEp 1 (Step 014955): Train loss 0.558, Val loss 46.155\nEp 1 (Step 014960): Train loss 0.521, Val loss 47.003\nEp 1 (Step 014965): Train loss 0.538, Val loss 46.909\nEp 1 (Step 014970): Train loss 0.546, Val loss 45.949\nEp 1 (Step 014975): Train loss 0.552, Val loss 46.062\nEp 1 (Step 014980): Train loss 0.556, Val loss 46.541\nEp 1 (Step 014985): Train loss 0.588, Val loss 46.058\nEp 1 (Step 014990): Train loss 0.566, Val loss 46.099\nEp 1 (Step 014995): Train loss 0.517, Val loss 45.872\nEp 1 (Step 015000): Train loss 0.582, Val loss 46.394\nEp 1 (Step 015005): Train loss 0.597, Val loss 45.891\nEp 1 (Step 015010): Train loss 0.535, Val loss 46.312\nEp 1 (Step 015015): Train loss 0.507, Val loss 45.688\nEp 1 (Step 015020): Train loss 0.537, Val loss 46.554\nEp 1 (Step 015025): Train loss 0.524, Val loss 45.588\nEp 1 (Step 015030): Train loss 0.545, Val loss 47.270\nEp 1 (Step 015035): Train loss 0.573, Val loss 46.076\nEp 1 (Step 015040): Train loss 0.596, Val loss 46.007\nEp 1 (Step 015045): Train loss 0.584, Val loss 46.191\nEp 1 (Step 015050): Train loss 0.524, Val loss 46.206\nEp 1 (Step 015055): Train loss 0.547, Val loss 46.735\nEp 1 (Step 015060): Train loss 0.520, Val loss 45.972\nEp 1 (Step 015065): Train loss 0.520, Val loss 46.166\nEp 1 (Step 015070): Train loss 0.569, Val loss 45.645\nEp 1 (Step 015075): Train loss 0.573, Val loss 46.155\nEp 1 (Step 015080): Train loss 0.490, Val loss 46.485\nEp 1 (Step 015085): Train loss 0.593, Val loss 47.325\nEp 1 (Step 015090): Train loss 0.560, Val loss 46.237\nEp 1 (Step 015095): Train loss 0.588, Val loss 45.863\nEp 1 (Step 015100): Train loss 0.614, Val loss 46.613\nEp 1 (Step 015105): Train loss 0.522, Val loss 46.020\nEp 1 (Step 015110): Train loss 0.536, Val loss 46.704\nEp 1 (Step 015115): Train loss 0.555, Val loss 45.721\nEp 1 (Step 015120): Train loss 0.582, Val loss 45.973\nEp 1 (Step 015125): Train loss 0.604, Val loss 46.880\nEp 1 (Step 015130): Train loss 0.497, Val loss 46.161\nEp 1 (Step 015135): Train loss 0.468, Val loss 46.196\nEp 1 (Step 015140): Train loss 0.538, Val loss 45.873\nEp 1 (Step 015145): Train loss 0.585, Val loss 46.487\nEp 1 (Step 015150): Train loss 0.509, Val loss 46.184\nEp 1 (Step 015155): Train loss 0.537, Val loss 45.480\nEp 1 (Step 015160): Train loss 0.547, Val loss 45.873\nEp 1 (Step 015165): Train loss 0.598, Val loss 45.704\nEp 1 (Step 015170): Train loss 0.528, Val loss 46.822\nEp 1 (Step 015175): Train loss 0.542, Val loss 46.079\nEp 1 (Step 015180): Train loss 0.532, Val loss 46.038\nEp 1 (Step 015185): Train loss 0.523, Val loss 46.791\nEp 1 (Step 015190): Train loss 0.551, Val loss 46.169\nEp 1 (Step 015195): Train loss 0.541, Val loss 46.719\nEp 1 (Step 015200): Train loss 0.591, Val loss 46.370\nEp 1 (Step 015205): Train loss 0.554, Val loss 46.735\nEp 1 (Step 015210): Train loss 0.580, Val loss 45.805\nEp 1 (Step 015215): Train loss 0.598, Val loss 46.165\nEp 1 (Step 015220): Train loss 0.586, Val loss 45.620\nEp 1 (Step 015225): Train loss 0.531, Val loss 45.694\nEp 1 (Step 015230): Train loss 0.516, Val loss 46.430\nEp 1 (Step 015235): Train loss 0.528, Val loss 46.507\nEp 1 (Step 015240): Train loss 0.497, Val loss 46.772\nEp 1 (Step 015245): Train loss 0.547, Val loss 47.145\nEp 1 (Step 015250): Train loss 0.586, Val loss 46.671\nEp 1 (Step 015255): Train loss 0.530, Val loss 46.019\nEp 1 (Step 015260): Train loss 0.500, Val loss 46.435\nEp 1 (Step 015265): Train loss 0.575, Val loss 45.266\nEp 1 (Step 015270): Train loss 0.521, Val loss 46.184\nEp 1 (Step 015275): Train loss 0.552, Val loss 46.006\nEp 1 (Step 015280): Train loss 0.549, Val loss 46.097\nEp 1 (Step 015285): Train loss 0.526, Val loss 47.267\nEp 1 (Step 015290): Train loss 0.515, Val loss 46.722\nEp 1 (Step 015295): Train loss 0.592, Val loss 46.751\nEp 1 (Step 015300): Train loss 0.529, Val loss 46.829\nEp 1 (Step 015305): Train loss 0.497, Val loss 45.743\nEp 1 (Step 015310): Train loss 0.519, Val loss 45.644\nEp 1 (Step 015315): Train loss 0.504, Val loss 46.700\nEp 1 (Step 015320): Train loss 0.596, Val loss 46.127\nEp 1 (Step 015325): Train loss 0.603, Val loss 46.157\nEp 1 (Step 015330): Train loss 0.547, Val loss 46.202\nEp 1 (Step 015335): Train loss 0.518, Val loss 47.094\nEp 1 (Step 015340): Train loss 0.548, Val loss 46.522\nEp 1 (Step 015345): Train loss 0.518, Val loss 45.838\nEp 1 (Step 015350): Train loss 0.614, Val loss 46.680\nEp 1 (Step 015355): Train loss 0.524, Val loss 46.777\nEp 1 (Step 015360): Train loss 0.609, Val loss 46.098\nEp 1 (Step 015365): Train loss 0.609, Val loss 46.829\nEp 1 (Step 015370): Train loss 0.569, Val loss 45.443\nEp 1 (Step 015375): Train loss 0.508, Val loss 46.422\nEp 1 (Step 015380): Train loss 0.494, Val loss 46.315\nEp 1 (Step 015385): Train loss 0.528, Val loss 46.471\nEp 1 (Step 015390): Train loss 0.564, Val loss 46.596\nEp 1 (Step 015395): Train loss 0.530, Val loss 45.481\nEp 1 (Step 015400): Train loss 0.528, Val loss 45.432\nEp 1 (Step 015405): Train loss 0.542, Val loss 45.748\nEp 1 (Step 015410): Train loss 0.563, Val loss 46.334\nEp 1 (Step 015415): Train loss 0.537, Val loss 45.926\nEp 1 (Step 015420): Train loss 0.552, Val loss 45.859\nEp 1 (Step 015425): Train loss 0.602, Val loss 46.525\nEp 1 (Step 015430): Train loss 0.556, Val loss 46.255\nEp 1 (Step 015435): Train loss 0.522, Val loss 46.767\nEp 1 (Step 015440): Train loss 0.528, Val loss 46.533\nEp 1 (Step 015445): Train loss 0.543, Val loss 45.929\nEp 1 (Step 015450): Train loss 0.532, Val loss 46.475\nEp 1 (Step 015455): Train loss 0.520, Val loss 46.813\nEp 1 (Step 015460): Train loss 0.518, Val loss 46.398\nEp 1 (Step 015465): Train loss 0.565, Val loss 45.949\nEp 1 (Step 015470): Train loss 0.577, Val loss 45.972\nEp 1 (Step 015475): Train loss 0.471, Val loss 46.366\nEp 1 (Step 015480): Train loss 0.596, Val loss 46.918\nEp 1 (Step 015485): Train loss 0.596, Val loss 45.696\nEp 1 (Step 015490): Train loss 0.538, Val loss 46.109\nEp 1 (Step 015495): Train loss 0.575, Val loss 45.944\nEp 1 (Step 015500): Train loss 0.525, Val loss 46.334\nEp 1 (Step 015505): Train loss 0.510, Val loss 46.415\nEp 1 (Step 015510): Train loss 0.528, Val loss 46.564\nEp 1 (Step 015515): Train loss 0.536, Val loss 46.209\nEp 1 (Step 015520): Train loss 0.559, Val loss 46.017\nEp 1 (Step 015525): Train loss 0.572, Val loss 46.678\nEp 1 (Step 015530): Train loss 0.535, Val loss 46.341\nEp 1 (Step 015535): Train loss 0.540, Val loss 46.469\nEp 1 (Step 015540): Train loss 0.576, Val loss 46.925\nEp 1 (Step 015545): Train loss 0.575, Val loss 46.179\nEp 1 (Step 015550): Train loss 0.509, Val loss 46.255\nEp 1 (Step 015555): Train loss 0.519, Val loss 46.664\nEp 1 (Step 015560): Train loss 0.556, Val loss 46.671\nEp 1 (Step 015565): Train loss 0.515, Val loss 47.019\nEp 1 (Step 015570): Train loss 0.514, Val loss 46.278\nEp 1 (Step 015575): Train loss 0.553, Val loss 46.134\nEp 1 (Step 015580): Train loss 0.541, Val loss 46.173\nEp 1 (Step 015585): Train loss 0.537, Val loss 46.117\nEp 1 (Step 015590): Train loss 0.556, Val loss 46.614\nEp 1 (Step 015595): Train loss 0.543, Val loss 46.878\nEp 1 (Step 015600): Train loss 0.520, Val loss 46.904\nEp 1 (Step 015605): Train loss 0.527, Val loss 45.832\nEp 1 (Step 015610): Train loss 0.563, Val loss 45.920\nEp 1 (Step 015615): Train loss 0.529, Val loss 47.129\nEp 1 (Step 015620): Train loss 0.544, Val loss 46.079\nEp 1 (Step 015625): Train loss 0.523, Val loss 46.036\nEp 1 (Step 015630): Train loss 0.538, Val loss 46.537\nEp 1 (Step 015635): Train loss 0.564, Val loss 46.169\nEp 1 (Step 015640): Train loss 0.545, Val loss 46.047\nEp 1 (Step 015645): Train loss 0.515, Val loss 45.656\nEp 1 (Step 015650): Train loss 0.526, Val loss 45.935\nEp 1 (Step 015655): Train loss 0.551, Val loss 46.544\nEp 1 (Step 015660): Train loss 0.558, Val loss 46.217\nEp 1 (Step 015665): Train loss 0.590, Val loss 45.956\nEp 1 (Step 015670): Train loss 0.504, Val loss 46.035\nEp 1 (Step 015675): Train loss 0.513, Val loss 46.812\nEp 1 (Step 015680): Train loss 0.559, Val loss 46.533\nEp 1 (Step 015685): Train loss 0.509, Val loss 46.342\nEp 1 (Step 015690): Train loss 0.444, Val loss 46.508\nEp 1 (Step 015695): Train loss 0.528, Val loss 46.470\nEp 1 (Step 015700): Train loss 0.506, Val loss 46.433\nEp 1 (Step 015705): Train loss 0.516, Val loss 46.406\nEp 1 (Step 015710): Train loss 0.545, Val loss 45.240\nEp 1 (Step 015715): Train loss 0.568, Val loss 45.926\nEp 1 (Step 015720): Train loss 0.591, Val loss 46.124\nEp 1 (Step 015725): Train loss 0.512, Val loss 46.780\nEp 1 (Step 015730): Train loss 0.535, Val loss 46.323\nEp 1 (Step 015735): Train loss 0.579, Val loss 46.165\nEp 1 (Step 015740): Train loss 0.518, Val loss 46.134\nEp 1 (Step 015745): Train loss 0.563, Val loss 45.759\nEp 1 (Step 015750): Train loss 0.608, Val loss 46.097\nEp 1 (Step 015755): Train loss 0.547, Val loss 46.772\nEp 1 (Step 015760): Train loss 0.531, Val loss 45.617\nEp 1 (Step 015765): Train loss 0.544, Val loss 46.610\nEp 1 (Step 015770): Train loss 0.521, Val loss 46.441\nEp 1 (Step 015775): Train loss 0.529, Val loss 46.135\nEp 1 (Step 015780): Train loss 0.450, Val loss 46.635\nEp 1 (Step 015785): Train loss 0.550, Val loss 46.184\nEp 1 (Step 015790): Train loss 0.520, Val loss 46.274\nEp 1 (Step 015795): Train loss 0.552, Val loss 46.576\nEp 1 (Step 015800): Train loss 0.509, Val loss 46.294\nEp 1 (Step 015805): Train loss 0.520, Val loss 45.518\nEp 1 (Step 015810): Train loss 0.574, Val loss 46.131\nEp 1 (Step 015815): Train loss 0.509, Val loss 46.357\nEp 1 (Step 015820): Train loss 0.519, Val loss 46.920\nEp 1 (Step 015825): Train loss 0.548, Val loss 46.636\nEp 1 (Step 015830): Train loss 0.511, Val loss 47.210\nEp 1 (Step 015835): Train loss 0.527, Val loss 46.427\nEp 1 (Step 015840): Train loss 0.501, Val loss 46.692\nEp 1 (Step 015845): Train loss 0.510, Val loss 46.938\nEp 1 (Step 015850): Train loss 0.498, Val loss 47.033\nEp 1 (Step 015855): Train loss 0.474, Val loss 47.143\nEp 1 (Step 015860): Train loss 0.506, Val loss 45.992\nEp 1 (Step 015865): Train loss 0.494, Val loss 46.403\nEp 1 (Step 015870): Train loss 0.542, Val loss 45.851\nEp 1 (Step 015875): Train loss 0.574, Val loss 46.452\nEp 1 (Step 015880): Train loss 0.529, Val loss 47.005\nEp 1 (Step 015885): Train loss 0.587, Val loss 46.632\nEp 1 (Step 015890): Train loss 0.597, Val loss 46.550\nEp 1 (Step 015895): Train loss 0.551, Val loss 46.658\nEp 1 (Step 015900): Train loss 0.523, Val loss 46.663\nEp 1 (Step 015905): Train loss 0.577, Val loss 46.554\nEp 1 (Step 015910): Train loss 0.547, Val loss 46.376\nEp 1 (Step 015915): Train loss 0.497, Val loss 47.426\nEp 1 (Step 015920): Train loss 0.562, Val loss 46.485\nEp 1 (Step 015925): Train loss 0.506, Val loss 46.414\nEp 1 (Step 015930): Train loss 0.551, Val loss 46.258\nEp 1 (Step 015935): Train loss 0.505, Val loss 45.966\nEp 1 (Step 015940): Train loss 0.573, Val loss 46.179\nEp 1 (Step 015945): Train loss 0.516, Val loss 46.786\nEp 1 (Step 015950): Train loss 0.545, Val loss 46.234\nEp 1 (Step 015955): Train loss 0.541, Val loss 46.979\nEp 1 (Step 015960): Train loss 0.550, Val loss 46.543\nEp 1 (Step 015965): Train loss 0.523, Val loss 46.217\nEp 1 (Step 015970): Train loss 0.501, Val loss 46.487\nEp 1 (Step 015975): Train loss 0.559, Val loss 46.005\nEp 1 (Step 015980): Train loss 0.564, Val loss 46.099\nEp 1 (Step 015985): Train loss 0.543, Val loss 45.908\nEp 1 (Step 015990): Train loss 0.552, Val loss 46.605\nEp 1 (Step 015995): Train loss 0.504, Val loss 46.387\nEp 1 (Step 016000): Train loss 0.553, Val loss 46.371\nEp 1 (Step 016005): Train loss 0.515, Val loss 46.129\nEp 1 (Step 016010): Train loss 0.536, Val loss 46.359\nEp 1 (Step 016015): Train loss 0.537, Val loss 46.131\nEp 1 (Step 016020): Train loss 0.532, Val loss 46.735\nEp 1 (Step 016025): Train loss 0.531, Val loss 46.966\nEp 1 (Step 016030): Train loss 0.529, Val loss 46.591\nEp 1 (Step 016035): Train loss 0.558, Val loss 45.736\nEp 1 (Step 016040): Train loss 0.482, Val loss 47.009\nEp 1 (Step 016045): Train loss 0.533, Val loss 46.888\nEp 1 (Step 016050): Train loss 0.524, Val loss 46.031\nEp 1 (Step 016055): Train loss 0.596, Val loss 46.120\nEp 1 (Step 016060): Train loss 0.584, Val loss 46.615\nEp 1 (Step 016065): Train loss 0.530, Val loss 46.273\nEp 1 (Step 016070): Train loss 0.578, Val loss 45.480\nEp 1 (Step 016075): Train loss 0.587, Val loss 46.903\nEp 1 (Step 016080): Train loss 0.539, Val loss 46.217\nEp 1 (Step 016085): Train loss 0.534, Val loss 46.075\nEp 1 (Step 016090): Train loss 0.560, Val loss 46.442\nEp 1 (Step 016095): Train loss 0.496, Val loss 45.881\nEp 1 (Step 016100): Train loss 0.578, Val loss 46.576\nEp 1 (Step 016105): Train loss 0.524, Val loss 47.122\nEp 1 (Step 016110): Train loss 0.520, Val loss 46.580\nEp 1 (Step 016115): Train loss 0.520, Val loss 47.027\nEp 1 (Step 016120): Train loss 0.512, Val loss 46.273\nEp 1 (Step 016125): Train loss 0.593, Val loss 46.038\nEp 1 (Step 016130): Train loss 0.509, Val loss 46.549\nEp 1 (Step 016135): Train loss 0.522, Val loss 46.417\nEp 1 (Step 016140): Train loss 0.525, Val loss 46.136\nEp 1 (Step 016145): Train loss 0.484, Val loss 46.602\nEp 1 (Step 016150): Train loss 0.542, Val loss 45.735\nEp 1 (Step 016155): Train loss 0.512, Val loss 46.343\nEp 1 (Step 016160): Train loss 0.493, Val loss 46.403\nEp 1 (Step 016165): Train loss 0.477, Val loss 46.565\nEp 1 (Step 016170): Train loss 0.538, Val loss 46.308\nEp 1 (Step 016175): Train loss 0.527, Val loss 46.259\nEp 1 (Step 016180): Train loss 0.509, Val loss 47.063\nEp 1 (Step 016185): Train loss 0.522, Val loss 46.046\nEp 1 (Step 016190): Train loss 0.594, Val loss 46.772\nEp 1 (Step 016195): Train loss 0.544, Val loss 47.183\nEp 1 (Step 016200): Train loss 0.537, Val loss 45.655\nEp 1 (Step 016205): Train loss 0.514, Val loss 46.267\nEp 1 (Step 016210): Train loss 0.540, Val loss 46.426\nEp 1 (Step 016215): Train loss 0.556, Val loss 46.540\nEp 1 (Step 016220): Train loss 0.545, Val loss 46.245\nEp 1 (Step 016225): Train loss 0.493, Val loss 46.611\nEp 1 (Step 016230): Train loss 0.592, Val loss 46.313\nEp 1 (Step 016235): Train loss 0.550, Val loss 46.063\nEp 1 (Step 016240): Train loss 0.541, Val loss 46.035\nEp 1 (Step 016245): Train loss 0.526, Val loss 46.649\nEp 1 (Step 016250): Train loss 0.539, Val loss 45.703\nEp 1 (Step 016255): Train loss 0.568, Val loss 46.448\nEp 1 (Step 016260): Train loss 0.580, Val loss 46.454\nEp 1 (Step 016265): Train loss 0.531, Val loss 46.014\nEp 1 (Step 016270): Train loss 0.555, Val loss 46.297\nEp 1 (Step 016275): Train loss 0.512, Val loss 46.175\nEp 1 (Step 016280): Train loss 0.487, Val loss 46.300\nEp 1 (Step 016285): Train loss 0.500, Val loss 46.809\nEp 1 (Step 016290): Train loss 0.526, Val loss 46.198\nEp 1 (Step 016295): Train loss 0.528, Val loss 47.391\nEp 1 (Step 016300): Train loss 0.560, Val loss 46.542\nEp 1 (Step 016305): Train loss 0.494, Val loss 46.621\nEp 1 (Step 016310): Train loss 0.499, Val loss 46.395\nEp 1 (Step 016315): Train loss 0.560, Val loss 46.564\nEp 1 (Step 016320): Train loss 0.544, Val loss 46.893\nEp 1 (Step 016325): Train loss 0.549, Val loss 46.640\nEp 1 (Step 016330): Train loss 0.529, Val loss 46.975\nEp 1 (Step 016335): Train loss 0.529, Val loss 46.439\nEp 1 (Step 016340): Train loss 0.541, Val loss 46.351\nEp 1 (Step 016345): Train loss 0.538, Val loss 46.739\nEp 1 (Step 016350): Train loss 0.564, Val loss 46.227\nEp 1 (Step 016355): Train loss 0.534, Val loss 46.248\nEp 1 (Step 016360): Train loss 0.531, Val loss 46.450\nEp 1 (Step 016365): Train loss 0.552, Val loss 47.161\nEp 1 (Step 016370): Train loss 0.562, Val loss 46.342\nEp 1 (Step 016375): Train loss 0.518, Val loss 46.291\nEp 1 (Step 016380): Train loss 0.552, Val loss 46.784\nEp 1 (Step 016385): Train loss 0.522, Val loss 46.837\nEp 1 (Step 016390): Train loss 0.541, Val loss 46.951\nEp 1 (Step 016395): Train loss 0.502, Val loss 46.864\nEp 1 (Step 016400): Train loss 0.542, Val loss 46.975\nEp 1 (Step 016405): Train loss 0.525, Val loss 45.934\nEp 1 (Step 016410): Train loss 0.511, Val loss 45.985\nEp 1 (Step 016415): Train loss 0.522, Val loss 46.567\nEp 1 (Step 016420): Train loss 0.506, Val loss 46.072\nEp 1 (Step 016425): Train loss 0.511, Val loss 46.213\nEp 1 (Step 016430): Train loss 0.530, Val loss 46.768\nEp 1 (Step 016435): Train loss 0.514, Val loss 47.277\nEp 1 (Step 016440): Train loss 0.524, Val loss 45.817\nEp 1 (Step 016445): Train loss 0.512, Val loss 46.710\nEp 1 (Step 016450): Train loss 0.550, Val loss 46.806\nEp 1 (Step 016455): Train loss 0.591, Val loss 47.153\nEp 1 (Step 016460): Train loss 0.463, Val loss 47.061\nEp 1 (Step 016465): Train loss 0.530, Val loss 46.027\nEp 1 (Step 016470): Train loss 0.527, Val loss 46.613\nEp 1 (Step 016475): Train loss 0.503, Val loss 46.385\nEp 1 (Step 016480): Train loss 0.465, Val loss 46.303\nEp 1 (Step 016485): Train loss 0.487, Val loss 47.208\nEp 1 (Step 016490): Train loss 0.509, Val loss 47.143\nEp 1 (Step 016495): Train loss 0.517, Val loss 46.807\nEp 1 (Step 016500): Train loss 0.509, Val loss 47.364\nEp 1 (Step 016505): Train loss 0.501, Val loss 46.595\nEp 1 (Step 016510): Train loss 0.535, Val loss 45.920\nEp 1 (Step 016515): Train loss 0.514, Val loss 46.757\nEp 1 (Step 016520): Train loss 0.566, Val loss 47.233\nEp 1 (Step 016525): Train loss 0.473, Val loss 46.152\nEp 1 (Step 016530): Train loss 0.584, Val loss 47.286\nEp 1 (Step 016535): Train loss 0.516, Val loss 46.064\nEp 1 (Step 016540): Train loss 0.498, Val loss 46.177\nEp 1 (Step 016545): Train loss 0.558, Val loss 46.795\nEp 1 (Step 016550): Train loss 0.486, Val loss 46.189\nEp 1 (Step 016555): Train loss 0.525, Val loss 46.953\nEp 1 (Step 016560): Train loss 0.477, Val loss 46.237\nEp 1 (Step 016565): Train loss 0.480, Val loss 46.024\nEp 1 (Step 016570): Train loss 0.574, Val loss 46.087\nEp 1 (Step 016575): Train loss 0.517, Val loss 46.621\nEp 1 (Step 016580): Train loss 0.526, Val loss 46.584\nEp 1 (Step 016585): Train loss 0.496, Val loss 46.456\nEp 1 (Step 016590): Train loss 0.555, Val loss 46.979\nEp 1 (Step 016595): Train loss 0.512, Val loss 45.470\nEp 1 (Step 016600): Train loss 0.511, Val loss 46.602\nEp 1 (Step 016605): Train loss 0.476, Val loss 47.171\nEp 1 (Step 016610): Train loss 0.501, Val loss 46.246\nEp 1 (Step 016615): Train loss 0.481, Val loss 47.802\nEp 1 (Step 016620): Train loss 0.545, Val loss 46.420\nEp 1 (Step 016625): Train loss 0.545, Val loss 46.828\nEp 1 (Step 016630): Train loss 0.545, Val loss 46.408\nEp 1 (Step 016635): Train loss 0.566, Val loss 45.961\nEp 1 (Step 016640): Train loss 0.456, Val loss 46.178\nEp 1 (Step 016645): Train loss 0.499, Val loss 46.815\nEp 1 (Step 016650): Train loss 0.512, Val loss 46.681\nEp 1 (Step 016655): Train loss 0.521, Val loss 46.784\nEp 1 (Step 016660): Train loss 0.520, Val loss 47.202\nEp 1 (Step 016665): Train loss 0.539, Val loss 46.280\nEp 1 (Step 016670): Train loss 0.536, Val loss 46.541\nEp 1 (Step 016675): Train loss 0.551, Val loss 46.843\nEp 1 (Step 016680): Train loss 0.494, Val loss 46.675\nEp 1 (Step 016685): Train loss 0.538, Val loss 46.527\nEp 1 (Step 016690): Train loss 0.465, Val loss 46.768\nEp 1 (Step 016695): Train loss 0.501, Val loss 46.832\nEp 1 (Step 016700): Train loss 0.490, Val loss 46.476\nEp 1 (Step 016705): Train loss 0.504, Val loss 46.845\nEp 1 (Step 016710): Train loss 0.459, Val loss 47.379\nEp 1 (Step 016715): Train loss 0.515, Val loss 47.479\nEp 1 (Step 016720): Train loss 0.485, Val loss 45.824\nEp 1 (Step 016725): Train loss 0.546, Val loss 46.499\nEp 1 (Step 016730): Train loss 0.509, Val loss 46.597\nEp 1 (Step 016735): Train loss 0.523, Val loss 45.920\nEp 1 (Step 016740): Train loss 0.507, Val loss 46.070\nEp 1 (Step 016745): Train loss 0.528, Val loss 46.060\nEp 1 (Step 016750): Train loss 0.558, Val loss 46.277\nEp 1 (Step 016755): Train loss 0.502, Val loss 46.113\nEp 1 (Step 016760): Train loss 0.522, Val loss 46.353\nEp 1 (Step 016765): Train loss 0.542, Val loss 45.658\nEp 1 (Step 016770): Train loss 0.550, Val loss 46.217\nEp 1 (Step 016775): Train loss 0.498, Val loss 46.524\nEp 1 (Step 016780): Train loss 0.503, Val loss 45.916\nEp 1 (Step 016785): Train loss 0.512, Val loss 46.853\nEp 1 (Step 016790): Train loss 0.505, Val loss 46.747\nEp 1 (Step 016795): Train loss 0.582, Val loss 46.597\nEp 1 (Step 016800): Train loss 0.632, Val loss 46.673\nEp 1 (Step 016805): Train loss 0.505, Val loss 46.332\nEp 1 (Step 016810): Train loss 0.585, Val loss 45.970\nEp 1 (Step 016815): Train loss 0.494, Val loss 46.689\nEp 1 (Step 016820): Train loss 0.482, Val loss 46.737\nEp 1 (Step 016825): Train loss 0.499, Val loss 46.557\nEp 1 (Step 016830): Train loss 0.553, Val loss 46.925\nEp 1 (Step 016835): Train loss 0.513, Val loss 46.983\nEp 1 (Step 016840): Train loss 0.489, Val loss 47.031\nEp 1 (Step 016845): Train loss 0.528, Val loss 46.668\nEp 1 (Step 016850): Train loss 0.462, Val loss 46.516\nEp 1 (Step 016855): Train loss 0.503, Val loss 46.473\nEp 1 (Step 016860): Train loss 0.520, Val loss 46.266\nEp 1 (Step 016865): Train loss 0.530, Val loss 46.304\nEp 1 (Step 016870): Train loss 0.527, Val loss 45.839\nEp 1 (Step 016875): Train loss 0.536, Val loss 46.328\nEp 1 (Step 016880): Train loss 0.539, Val loss 45.730\nEp 1 (Step 016885): Train loss 0.476, Val loss 45.758\nEp 1 (Step 016890): Train loss 0.498, Val loss 46.132\nEp 1 (Step 016895): Train loss 0.529, Val loss 46.705\nEp 1 (Step 016900): Train loss 0.515, Val loss 46.569\nEp 1 (Step 016905): Train loss 0.448, Val loss 45.851\nEp 1 (Step 016910): Train loss 0.529, Val loss 46.701\nEp 1 (Step 016915): Train loss 0.510, Val loss 46.286\nEp 1 (Step 016920): Train loss 0.506, Val loss 46.687\nEp 1 (Step 016925): Train loss 0.543, Val loss 47.268\nEp 1 (Step 016930): Train loss 0.520, Val loss 46.307\nEp 1 (Step 016935): Train loss 0.536, Val loss 46.161\nEp 1 (Step 016940): Train loss 0.512, Val loss 46.309\nEp 1 (Step 016945): Train loss 0.527, Val loss 45.921\nEp 1 (Step 016950): Train loss 0.559, Val loss 45.924\nEp 1 (Step 016955): Train loss 0.546, Val loss 46.595\nEp 1 (Step 016960): Train loss 0.505, Val loss 47.128\nEp 1 (Step 016965): Train loss 0.547, Val loss 46.309\nEp 1 (Step 016970): Train loss 0.545, Val loss 46.475\nEp 1 (Step 016975): Train loss 0.487, Val loss 45.888\nEp 1 (Step 016980): Train loss 0.488, Val loss 46.132\nEp 1 (Step 016985): Train loss 0.515, Val loss 46.484\nEp 1 (Step 016990): Train loss 0.515, Val loss 46.380\nEp 1 (Step 016995): Train loss 0.587, Val loss 45.583\nEp 1 (Step 017000): Train loss 0.522, Val loss 46.497\nEp 1 (Step 017005): Train loss 0.578, Val loss 45.748\nEp 1 (Step 017010): Train loss 0.515, Val loss 46.561\nEp 1 (Step 017015): Train loss 0.511, Val loss 46.946\nEp 1 (Step 017020): Train loss 0.492, Val loss 46.557\nEp 1 (Step 017025): Train loss 0.482, Val loss 46.370\nEp 1 (Step 017030): Train loss 0.503, Val loss 46.979\nEp 1 (Step 017035): Train loss 0.581, Val loss 46.287\nEp 1 (Step 017040): Train loss 0.498, Val loss 46.433\nEp 1 (Step 017045): Train loss 0.507, Val loss 46.247\nEp 1 (Step 017050): Train loss 0.556, Val loss 46.510\nEp 1 (Step 017055): Train loss 0.524, Val loss 46.326\nEp 1 (Step 017060): Train loss 0.557, Val loss 45.797\nEp 1 (Step 017065): Train loss 0.512, Val loss 46.326\nEp 1 (Step 017070): Train loss 0.486, Val loss 45.747\nEp 1 (Step 017075): Train loss 0.483, Val loss 46.085\nEp 1 (Step 017080): Train loss 0.518, Val loss 46.762\nEp 1 (Step 017085): Train loss 0.604, Val loss 46.174\nEp 1 (Step 017090): Train loss 0.510, Val loss 45.625\nEp 1 (Step 017095): Train loss 0.547, Val loss 46.674\nEp 1 (Step 017100): Train loss 0.465, Val loss 46.244\nEp 1 (Step 017105): Train loss 0.551, Val loss 46.358\nEp 1 (Step 017110): Train loss 0.504, Val loss 46.677\nEp 1 (Step 017115): Train loss 0.490, Val loss 46.277\nEp 1 (Step 017120): Train loss 0.517, Val loss 45.990\nEp 1 (Step 017125): Train loss 0.507, Val loss 46.224\nEp 1 (Step 017130): Train loss 0.554, Val loss 46.158\nEp 1 (Step 017135): Train loss 0.519, Val loss 46.531\nEp 1 (Step 017140): Train loss 0.512, Val loss 46.259\nEp 1 (Step 017145): Train loss 0.504, Val loss 45.542\nEp 1 (Step 017150): Train loss 0.499, Val loss 46.069\nEp 1 (Step 017155): Train loss 0.546, Val loss 46.956\nEp 1 (Step 017160): Train loss 0.541, Val loss 46.108\nEp 1 (Step 017165): Train loss 0.532, Val loss 46.653\nEp 1 (Step 017170): Train loss 0.552, Val loss 46.297\nEp 1 (Step 017175): Train loss 0.521, Val loss 46.609\nEp 1 (Step 017180): Train loss 0.532, Val loss 46.041\nEp 1 (Step 017185): Train loss 0.504, Val loss 46.303\nEp 1 (Step 017190): Train loss 0.501, Val loss 46.990\nEp 1 (Step 017195): Train loss 0.485, Val loss 46.551\nEp 1 (Step 017200): Train loss 0.504, Val loss 46.539\nEp 1 (Step 017205): Train loss 0.535, Val loss 46.740\nEp 1 (Step 017210): Train loss 0.514, Val loss 46.899\nEp 1 (Step 017215): Train loss 0.539, Val loss 46.696\nEp 1 (Step 017220): Train loss 0.523, Val loss 46.266\nEp 1 (Step 017225): Train loss 0.473, Val loss 47.063\nEp 1 (Step 017230): Train loss 0.464, Val loss 47.470\nEp 1 (Step 017235): Train loss 0.490, Val loss 46.597\nEp 1 (Step 017240): Train loss 0.499, Val loss 47.069\nEp 1 (Step 017245): Train loss 0.485, Val loss 46.137\nEp 1 (Step 017250): Train loss 0.507, Val loss 46.642\nEp 1 (Step 017255): Train loss 0.587, Val loss 46.931\nEp 1 (Step 017260): Train loss 0.514, Val loss 46.354\nEp 1 (Step 017265): Train loss 0.482, Val loss 46.303\nEp 1 (Step 017270): Train loss 0.556, Val loss 46.474\nEp 1 (Step 017275): Train loss 0.509, Val loss 46.184\nEp 1 (Step 017280): Train loss 0.507, Val loss 45.118\nEp 1 (Step 017285): Train loss 0.508, Val loss 46.613\nEp 1 (Step 017290): Train loss 0.530, Val loss 46.660\nEp 1 (Step 017295): Train loss 0.520, Val loss 46.445\nEp 1 (Step 017300): Train loss 0.494, Val loss 45.816\nEp 1 (Step 017305): Train loss 0.468, Val loss 46.479\nEp 1 (Step 017310): Train loss 0.508, Val loss 46.140\nEp 1 (Step 017315): Train loss 0.550, Val loss 46.057\nEp 1 (Step 017320): Train loss 0.505, Val loss 46.433\nEp 1 (Step 017325): Train loss 0.481, Val loss 45.895\nEp 1 (Step 017330): Train loss 0.535, Val loss 46.232\nEp 1 (Step 017335): Train loss 0.532, Val loss 46.417\nEp 1 (Step 017340): Train loss 0.547, Val loss 46.626\nEp 1 (Step 017345): Train loss 0.544, Val loss 46.563\nEp 1 (Step 017350): Train loss 0.515, Val loss 46.127\nEp 1 (Step 017355): Train loss 0.525, Val loss 46.984\nEp 1 (Step 017360): Train loss 0.520, Val loss 47.374\nEp 1 (Step 017365): Train loss 0.500, Val loss 46.252\nEp 1 (Step 017370): Train loss 0.548, Val loss 46.832\nEp 1 (Step 017375): Train loss 0.543, Val loss 46.259\nEp 1 (Step 017380): Train loss 0.552, Val loss 45.837\nEp 1 (Step 017385): Train loss 0.511, Val loss 46.776\nEp 1 (Step 017390): Train loss 0.531, Val loss 46.169\nEp 1 (Step 017395): Train loss 0.513, Val loss 46.199\nEp 1 (Step 017400): Train loss 0.520, Val loss 46.066\nEp 1 (Step 017405): Train loss 0.469, Val loss 46.103\nEp 1 (Step 017410): Train loss 0.513, Val loss 45.944\nEp 1 (Step 017415): Train loss 0.486, Val loss 46.499\nEp 1 (Step 017420): Train loss 0.504, Val loss 45.963\nEp 1 (Step 017425): Train loss 0.534, Val loss 46.258\nEp 1 (Step 017430): Train loss 0.524, Val loss 46.703\nEp 1 (Step 017435): Train loss 0.526, Val loss 46.520\nEp 1 (Step 017440): Train loss 0.526, Val loss 46.350\nEp 1 (Step 017445): Train loss 0.491, Val loss 46.580\nEp 1 (Step 017450): Train loss 0.490, Val loss 46.747\nEp 1 (Step 017455): Train loss 0.549, Val loss 46.547\nEp 1 (Step 017460): Train loss 0.479, Val loss 46.818\nEp 1 (Step 017465): Train loss 0.520, Val loss 46.003\nEp 1 (Step 017470): Train loss 0.577, Val loss 45.539\nEp 1 (Step 017475): Train loss 0.514, Val loss 45.496\nEp 1 (Step 017480): Train loss 0.502, Val loss 46.290\nEp 1 (Step 017485): Train loss 0.531, Val loss 45.774\nEp 1 (Step 017490): Train loss 0.518, Val loss 45.956\nEp 1 (Step 017495): Train loss 0.505, Val loss 46.615\nEp 1 (Step 017500): Train loss 0.510, Val loss 46.147\nEp 1 (Step 017505): Train loss 0.512, Val loss 46.942\nEp 1 (Step 017510): Train loss 0.495, Val loss 46.143\nEp 1 (Step 017515): Train loss 0.520, Val loss 46.506\nEp 1 (Step 017520): Train loss 0.523, Val loss 46.227\nEp 1 (Step 017525): Train loss 0.493, Val loss 46.411\nEp 1 (Step 017530): Train loss 0.505, Val loss 46.911\nEp 1 (Step 017535): Train loss 0.556, Val loss 46.091\nEp 1 (Step 017540): Train loss 0.544, Val loss 46.625\nEp 1 (Step 017545): Train loss 0.514, Val loss 45.893\nEp 1 (Step 017550): Train loss 0.491, Val loss 46.434\nEp 1 (Step 017555): Train loss 0.539, Val loss 46.656\nEp 1 (Step 017560): Train loss 0.460, Val loss 47.040\nEp 1 (Step 017565): Train loss 0.555, Val loss 46.635\nEp 1 (Step 017570): Train loss 0.458, Val loss 46.686\nEp 1 (Step 017575): Train loss 0.486, Val loss 46.345\nEp 1 (Step 017580): Train loss 0.483, Val loss 46.544\nEp 1 (Step 017585): Train loss 0.549, Val loss 46.616\nEp 1 (Step 017590): Train loss 0.476, Val loss 45.969\nEp 1 (Step 017595): Train loss 0.466, Val loss 47.332\nEp 1 (Step 017600): Train loss 0.499, Val loss 46.396\nEp 1 (Step 017605): Train loss 0.463, Val loss 46.264\nEp 1 (Step 017610): Train loss 0.557, Val loss 46.145\nEp 1 (Step 017615): Train loss 0.502, Val loss 46.396\nEp 1 (Step 017620): Train loss 0.545, Val loss 46.785\nEp 1 (Step 017625): Train loss 0.505, Val loss 46.540\nEp 1 (Step 017630): Train loss 0.526, Val loss 46.604\nEp 1 (Step 017635): Train loss 0.515, Val loss 46.805\nEp 1 (Step 017640): Train loss 0.525, Val loss 45.683\nEp 1 (Step 017645): Train loss 0.523, Val loss 45.868\nEp 1 (Step 017650): Train loss 0.497, Val loss 46.764\nEp 1 (Step 017655): Train loss 0.543, Val loss 46.532\nEp 1 (Step 017660): Train loss 0.530, Val loss 46.974\nEp 1 (Step 017665): Train loss 0.477, Val loss 46.234\nEp 1 (Step 017670): Train loss 0.533, Val loss 47.400\nEp 1 (Step 017675): Train loss 0.481, Val loss 45.905\nEp 1 (Step 017680): Train loss 0.493, Val loss 46.394\nEp 1 (Step 017685): Train loss 0.483, Val loss 45.961\nEp 1 (Step 017690): Train loss 0.527, Val loss 46.403\nEp 1 (Step 017695): Train loss 0.503, Val loss 46.653\nEp 1 (Step 017700): Train loss 0.497, Val loss 46.454\nEp 1 (Step 017705): Train loss 0.535, Val loss 46.912\nEp 1 (Step 017710): Train loss 0.498, Val loss 45.980\nEp 1 (Step 017715): Train loss 0.538, Val loss 45.718\nEp 1 (Step 017720): Train loss 0.506, Val loss 46.021\nEp 1 (Step 017725): Train loss 0.477, Val loss 47.223\nEp 1 (Step 017730): Train loss 0.489, Val loss 45.811\nEp 1 (Step 017735): Train loss 0.441, Val loss 47.112\nEp 1 (Step 017740): Train loss 0.547, Val loss 46.241\nEp 1 (Step 017745): Train loss 0.513, Val loss 46.345\nEp 1 (Step 017750): Train loss 0.474, Val loss 46.559\nEp 1 (Step 017755): Train loss 0.503, Val loss 46.285\nEp 1 (Step 017760): Train loss 0.577, Val loss 46.447\nEp 1 (Step 017765): Train loss 0.544, Val loss 46.347\nEp 1 (Step 017770): Train loss 0.481, Val loss 46.559\nEp 1 (Step 017775): Train loss 0.555, Val loss 46.380\nEp 1 (Step 017780): Train loss 0.553, Val loss 46.559\nEp 1 (Step 017785): Train loss 0.474, Val loss 46.515\nEp 1 (Step 017790): Train loss 0.484, Val loss 46.123\nEp 1 (Step 017795): Train loss 0.471, Val loss 47.035\nEp 1 (Step 017800): Train loss 0.481, Val loss 46.310\nEp 1 (Step 017805): Train loss 0.469, Val loss 46.708\nEp 1 (Step 017810): Train loss 0.503, Val loss 46.575\nEp 1 (Step 017815): Train loss 0.524, Val loss 46.678\nEp 1 (Step 017820): Train loss 0.525, Val loss 46.177\nEp 1 (Step 017825): Train loss 0.530, Val loss 47.042\nEp 1 (Step 017830): Train loss 0.493, Val loss 46.148\nEp 1 (Step 017835): Train loss 0.506, Val loss 45.566\nEp 1 (Step 017840): Train loss 0.466, Val loss 46.572\nEp 1 (Step 017845): Train loss 0.511, Val loss 45.984\nEp 1 (Step 017850): Train loss 0.492, Val loss 46.241\nEp 1 (Step 017855): Train loss 0.493, Val loss 46.536\nEp 1 (Step 017860): Train loss 0.530, Val loss 46.064\nEp 1 (Step 017865): Train loss 0.508, Val loss 46.761\nEp 1 (Step 017870): Train loss 0.470, Val loss 46.629\nEp 1 (Step 017875): Train loss 0.506, Val loss 46.052\nEp 1 (Step 017880): Train loss 0.471, Val loss 46.332\nEp 1 (Step 017885): Train loss 0.485, Val loss 45.866\nEp 1 (Step 017890): Train loss 0.477, Val loss 46.738\nEp 1 (Step 017895): Train loss 0.445, Val loss 46.571\nEp 1 (Step 017900): Train loss 0.567, Val loss 45.751\nEp 1 (Step 017905): Train loss 0.474, Val loss 46.621\nEp 1 (Step 017910): Train loss 0.470, Val loss 46.062\nEp 1 (Step 017915): Train loss 0.494, Val loss 46.765\nEp 1 (Step 017920): Train loss 0.558, Val loss 46.216\nEp 1 (Step 017925): Train loss 0.505, Val loss 47.448\nEp 1 (Step 017930): Train loss 0.516, Val loss 46.973\nEp 1 (Step 017935): Train loss 0.482, Val loss 45.833\nEp 1 (Step 017940): Train loss 0.493, Val loss 46.402\nEp 1 (Step 017945): Train loss 0.459, Val loss 46.389\nEp 1 (Step 017950): Train loss 0.523, Val loss 46.792\nEp 1 (Step 017955): Train loss 0.481, Val loss 46.397\nEp 1 (Step 017960): Train loss 0.487, Val loss 46.575\nEp 1 (Step 017965): Train loss 0.482, Val loss 46.029\nEp 1 (Step 017970): Train loss 0.535, Val loss 45.460\nEp 1 (Step 017975): Train loss 0.465, Val loss 47.286\nEp 1 (Step 017980): Train loss 0.573, Val loss 46.170\nEp 1 (Step 017985): Train loss 0.514, Val loss 46.202\nEp 1 (Step 017990): Train loss 0.490, Val loss 46.392\nEp 1 (Step 017995): Train loss 0.493, Val loss 45.914\nEp 1 (Step 018000): Train loss 0.560, Val loss 46.896\nEp 1 (Step 018005): Train loss 0.477, Val loss 46.170\nEp 1 (Step 018010): Train loss 0.576, Val loss 46.720\nEp 1 (Step 018015): Train loss 0.487, Val loss 46.178\nEp 1 (Step 018020): Train loss 0.529, Val loss 46.796\nEp 1 (Step 018025): Train loss 0.569, Val loss 46.463\nEp 1 (Step 018030): Train loss 0.506, Val loss 46.438\nEp 1 (Step 018035): Train loss 0.518, Val loss 46.842\nEp 1 (Step 018040): Train loss 0.618, Val loss 46.830\nEp 1 (Step 018045): Train loss 0.520, Val loss 46.531\nEp 1 (Step 018050): Train loss 0.530, Val loss 46.337\nEp 1 (Step 018055): Train loss 0.516, Val loss 46.223\nEp 1 (Step 018060): Train loss 0.479, Val loss 46.640\nEp 1 (Step 018065): Train loss 0.481, Val loss 46.109\nEp 1 (Step 018070): Train loss 0.511, Val loss 46.915\nEp 1 (Step 018075): Train loss 0.527, Val loss 47.167\nEp 1 (Step 018080): Train loss 0.501, Val loss 46.698\nEp 1 (Step 018085): Train loss 0.495, Val loss 46.832\nEp 1 (Step 018090): Train loss 0.499, Val loss 46.619\nEp 1 (Step 018095): Train loss 0.558, Val loss 47.108\nEp 1 (Step 018100): Train loss 0.579, Val loss 46.050\nEp 1 (Step 018105): Train loss 0.509, Val loss 46.943\nEp 1 (Step 018110): Train loss 0.466, Val loss 47.124\nEp 1 (Step 018115): Train loss 0.491, Val loss 46.931\nEp 1 (Step 018120): Train loss 0.438, Val loss 46.899\nEp 1 (Step 018125): Train loss 0.458, Val loss 46.512\nEp 1 (Step 018130): Train loss 0.474, Val loss 46.600\nEp 1 (Step 018135): Train loss 0.502, Val loss 45.974\nEp 1 (Step 018140): Train loss 0.424, Val loss 45.863\nEp 1 (Step 018145): Train loss 0.488, Val loss 46.816\nEp 1 (Step 018150): Train loss 0.462, Val loss 46.926\nEp 1 (Step 018155): Train loss 0.493, Val loss 47.345\nEp 1 (Step 018160): Train loss 0.509, Val loss 47.073\nEp 1 (Step 018165): Train loss 0.513, Val loss 46.943\nEp 1 (Step 018170): Train loss 0.462, Val loss 45.996\nEp 1 (Step 018175): Train loss 0.491, Val loss 46.115\nEp 1 (Step 018180): Train loss 0.509, Val loss 47.509\nEp 1 (Step 018185): Train loss 0.454, Val loss 46.806\nEp 1 (Step 018190): Train loss 0.535, Val loss 45.731\nEp 1 (Step 018195): Train loss 0.502, Val loss 46.386\nEp 1 (Step 018200): Train loss 0.507, Val loss 46.927\nEp 1 (Step 018205): Train loss 0.531, Val loss 46.005\nEp 1 (Step 018210): Train loss 0.518, Val loss 46.693\nEp 1 (Step 018215): Train loss 0.530, Val loss 46.559\nEp 1 (Step 018220): Train loss 0.490, Val loss 47.219\nEp 1 (Step 018225): Train loss 0.530, Val loss 46.434\nEp 1 (Step 018230): Train loss 0.466, Val loss 46.624\nEp 1 (Step 018235): Train loss 0.524, Val loss 46.330\nEp 1 (Step 018240): Train loss 0.494, Val loss 46.775\nEp 1 (Step 018245): Train loss 0.506, Val loss 46.372\nEp 1 (Step 018250): Train loss 0.495, Val loss 45.795\nEp 1 (Step 018255): Train loss 0.512, Val loss 47.142\nEp 1 (Step 018260): Train loss 0.504, Val loss 46.125\nEp 1 (Step 018265): Train loss 0.518, Val loss 46.309\nEp 1 (Step 018270): Train loss 0.530, Val loss 45.973\nEp 1 (Step 018275): Train loss 0.484, Val loss 46.798\nEp 1 (Step 018280): Train loss 0.505, Val loss 46.724\nEp 1 (Step 018285): Train loss 0.497, Val loss 46.214\nEp 1 (Step 018290): Train loss 0.495, Val loss 46.391\nEp 1 (Step 018295): Train loss 0.461, Val loss 45.357\nEp 1 (Step 018300): Train loss 0.447, Val loss 46.288\nEp 1 (Step 018305): Train loss 0.476, Val loss 45.711\nEp 1 (Step 018310): Train loss 0.521, Val loss 47.033\nEp 1 (Step 018315): Train loss 0.489, Val loss 46.725\nEp 1 (Step 018320): Train loss 0.644, Val loss 46.923\nEp 1 (Step 018325): Train loss 0.539, Val loss 46.923\nEp 1 (Step 018330): Train loss 0.519, Val loss 46.028\nEp 1 (Step 018335): Train loss 0.501, Val loss 45.993\nEp 1 (Step 018340): Train loss 0.478, Val loss 46.555\nEp 1 (Step 018345): Train loss 0.535, Val loss 46.465\nEp 1 (Step 018350): Train loss 0.557, Val loss 46.614\nEp 1 (Step 018355): Train loss 0.543, Val loss 46.617\nEp 1 (Step 018360): Train loss 0.494, Val loss 46.829\nEp 1 (Step 018365): Train loss 0.515, Val loss 46.896\nEp 1 (Step 018370): Train loss 0.498, Val loss 46.910\nEp 1 (Step 018375): Train loss 0.534, Val loss 46.283\nEp 1 (Step 018380): Train loss 0.462, Val loss 47.140\nEp 1 (Step 018385): Train loss 0.490, Val loss 46.829\nEp 1 (Step 018390): Train loss 0.483, Val loss 45.421\nEp 1 (Step 018395): Train loss 0.494, Val loss 47.112\nEp 1 (Step 018400): Train loss 0.531, Val loss 45.845\nEp 1 (Step 018405): Train loss 0.554, Val loss 46.734\nEp 1 (Step 018410): Train loss 0.481, Val loss 46.243\nEp 1 (Step 018415): Train loss 0.468, Val loss 46.876\nEp 1 (Step 018420): Train loss 0.502, Val loss 46.205\nEp 1 (Step 018425): Train loss 0.427, Val loss 46.188\nEp 1 (Step 018430): Train loss 0.474, Val loss 46.316\nEp 1 (Step 018435): Train loss 0.486, Val loss 46.329\nEp 1 (Step 018440): Train loss 0.485, Val loss 46.490\nEp 1 (Step 018445): Train loss 0.511, Val loss 46.076\nEp 1 (Step 018450): Train loss 0.512, Val loss 46.425\nEp 1 (Step 018455): Train loss 0.502, Val loss 46.754\nEp 1 (Step 018460): Train loss 0.527, Val loss 46.686\nEp 1 (Step 018465): Train loss 0.474, Val loss 46.633\nEp 1 (Step 018470): Train loss 0.494, Val loss 46.682\nEp 1 (Step 018475): Train loss 0.490, Val loss 46.206\nEp 1 (Step 018480): Train loss 0.495, Val loss 46.562\nEp 1 (Step 018485): Train loss 0.483, Val loss 46.604\nEp 1 (Step 018490): Train loss 0.530, Val loss 46.938\nEp 1 (Step 018495): Train loss 0.528, Val loss 46.913\nEp 1 (Step 018500): Train loss 0.536, Val loss 46.483\nEp 1 (Step 018505): Train loss 0.480, Val loss 46.785\nEp 1 (Step 018510): Train loss 0.473, Val loss 46.751\nEp 1 (Step 018515): Train loss 0.530, Val loss 47.131\nEp 1 (Step 018520): Train loss 0.496, Val loss 46.353\nEp 1 (Step 018525): Train loss 0.524, Val loss 45.903\nEp 1 (Step 018530): Train loss 0.455, Val loss 46.002\nEp 1 (Step 018535): Train loss 0.489, Val loss 47.294\nEp 1 (Step 018540): Train loss 0.475, Val loss 47.183\nEp 1 (Step 018545): Train loss 0.521, Val loss 46.640\nEp 1 (Step 018550): Train loss 0.478, Val loss 46.223\nEp 1 (Step 018555): Train loss 0.504, Val loss 46.727\nEp 1 (Step 018560): Train loss 0.466, Val loss 46.722\nEp 1 (Step 018565): Train loss 0.531, Val loss 46.093\nEp 1 (Step 018570): Train loss 0.488, Val loss 47.027\nEp 1 (Step 018575): Train loss 0.478, Val loss 46.023\nEp 1 (Step 018580): Train loss 0.490, Val loss 45.771\nEp 1 (Step 018585): Train loss 0.517, Val loss 46.524\nEp 1 (Step 018590): Train loss 0.480, Val loss 45.827\nEp 1 (Step 018595): Train loss 0.489, Val loss 46.624\nEp 1 (Step 018600): Train loss 0.478, Val loss 47.191\nEp 1 (Step 018605): Train loss 0.469, Val loss 46.602\nEp 1 (Step 018610): Train loss 0.535, Val loss 46.345\nEp 1 (Step 018615): Train loss 0.521, Val loss 45.990\nEp 1 (Step 018620): Train loss 0.508, Val loss 46.102\nEp 1 (Step 018625): Train loss 0.514, Val loss 46.129\nEp 1 (Step 018630): Train loss 0.493, Val loss 47.024\nEp 1 (Step 018635): Train loss 0.527, Val loss 46.147\nEp 1 (Step 018640): Train loss 0.511, Val loss 45.699\nEp 1 (Step 018645): Train loss 0.491, Val loss 46.082\nEp 1 (Step 018650): Train loss 0.485, Val loss 46.860\nEp 1 (Step 018655): Train loss 0.508, Val loss 46.883\nEp 1 (Step 018660): Train loss 0.492, Val loss 46.022\nEp 1 (Step 018665): Train loss 0.514, Val loss 46.383\nEp 1 (Step 018670): Train loss 0.450, Val loss 47.370\nEp 1 (Step 018675): Train loss 0.513, Val loss 46.635\nEp 1 (Step 018680): Train loss 0.555, Val loss 46.560\nEp 1 (Step 018685): Train loss 0.493, Val loss 46.163\nEp 1 (Step 018690): Train loss 0.551, Val loss 46.518\nEp 1 (Step 018695): Train loss 0.510, Val loss 46.298\nEp 1 (Step 018700): Train loss 0.492, Val loss 45.957\nEp 1 (Step 018705): Train loss 0.484, Val loss 46.425\nEp 1 (Step 018710): Train loss 0.511, Val loss 46.363\nEp 1 (Step 018715): Train loss 0.535, Val loss 46.483\nEp 1 (Step 018720): Train loss 0.474, Val loss 45.768\nEp 1 (Step 018725): Train loss 0.561, Val loss 46.336\nEp 1 (Step 018730): Train loss 0.503, Val loss 46.608\nEp 1 (Step 018735): Train loss 0.518, Val loss 45.619\nEp 1 (Step 018740): Train loss 0.496, Val loss 46.389\nEp 1 (Step 018745): Train loss 0.471, Val loss 46.603\nEp 1 (Step 018750): Train loss 0.472, Val loss 47.005\nEp 1 (Step 018755): Train loss 0.519, Val loss 46.610\nEp 1 (Step 018760): Train loss 0.451, Val loss 47.222\nEp 1 (Step 018765): Train loss 0.501, Val loss 47.388\nEp 1 (Step 018770): Train loss 0.482, Val loss 46.655\nEp 1 (Step 018775): Train loss 0.541, Val loss 46.709\nEp 1 (Step 018780): Train loss 0.496, Val loss 47.081\nEp 1 (Step 018785): Train loss 0.466, Val loss 46.532\nEp 1 (Step 018790): Train loss 0.479, Val loss 46.887\nEp 1 (Step 018795): Train loss 0.451, Val loss 46.929\nEp 1 (Step 018800): Train loss 0.487, Val loss 46.316\nEp 1 (Step 018805): Train loss 0.495, Val loss 46.244\nEp 1 (Step 018810): Train loss 0.484, Val loss 46.064\nEp 1 (Step 018815): Train loss 0.490, Val loss 47.357\nEp 1 (Step 018820): Train loss 0.508, Val loss 46.167\nEp 1 (Step 018825): Train loss 0.477, Val loss 46.672\nEp 1 (Step 018830): Train loss 0.494, Val loss 46.477\nEp 1 (Step 018835): Train loss 0.515, Val loss 46.405\nEp 1 (Step 018840): Train loss 0.495, Val loss 46.288\nEp 1 (Step 018845): Train loss 0.505, Val loss 46.868\nEp 1 (Step 018850): Train loss 0.482, Val loss 47.483\nEp 1 (Step 018855): Train loss 0.520, Val loss 46.138\nEp 1 (Step 018860): Train loss 0.480, Val loss 46.364\nEp 1 (Step 018865): Train loss 0.524, Val loss 45.878\nEp 1 (Step 018870): Train loss 0.478, Val loss 46.235\nEp 1 (Step 018875): Train loss 0.487, Val loss 46.666\nEp 1 (Step 018880): Train loss 0.471, Val loss 46.837\nEp 1 (Step 018885): Train loss 0.526, Val loss 46.745\nEp 1 (Step 018890): Train loss 0.467, Val loss 46.131\nEp 1 (Step 018895): Train loss 0.466, Val loss 46.338\nEp 1 (Step 018900): Train loss 0.496, Val loss 46.792\nEp 1 (Step 018905): Train loss 0.528, Val loss 46.124\nEp 1 (Step 018910): Train loss 0.504, Val loss 46.022\nEp 1 (Step 018915): Train loss 0.485, Val loss 47.103\nEp 1 (Step 018920): Train loss 0.512, Val loss 46.531\nEp 1 (Step 018925): Train loss 0.501, Val loss 46.757\nEp 1 (Step 018930): Train loss 0.512, Val loss 46.649\nEp 1 (Step 018935): Train loss 0.503, Val loss 46.277\nEp 1 (Step 018940): Train loss 0.465, Val loss 45.538\nEp 1 (Step 018945): Train loss 0.547, Val loss 46.486\nEp 1 (Step 018950): Train loss 0.505, Val loss 46.543\nEp 1 (Step 018955): Train loss 0.516, Val loss 46.420\nEp 1 (Step 018960): Train loss 0.510, Val loss 45.993\nEp 1 (Step 018965): Train loss 0.472, Val loss 46.588\nEp 1 (Step 018970): Train loss 0.499, Val loss 46.329\nEp 1 (Step 018975): Train loss 0.521, Val loss 46.376\nEp 1 (Step 018980): Train loss 0.435, Val loss 46.463\nEp 1 (Step 018985): Train loss 0.465, Val loss 46.952\nEp 1 (Step 018990): Train loss 0.437, Val loss 46.235\nEp 1 (Step 018995): Train loss 0.482, Val loss 46.441\nEp 1 (Step 019000): Train loss 0.457, Val loss 46.387\nEp 1 (Step 019005): Train loss 0.457, Val loss 46.714\nEp 1 (Step 019010): Train loss 0.494, Val loss 46.530\nEp 1 (Step 019015): Train loss 0.491, Val loss 46.110\nEp 1 (Step 019020): Train loss 0.525, Val loss 45.717\nEp 1 (Step 019025): Train loss 0.474, Val loss 46.150\nEp 1 (Step 019030): Train loss 0.478, Val loss 46.436\nEp 1 (Step 019035): Train loss 0.515, Val loss 46.633\nEp 1 (Step 019040): Train loss 0.463, Val loss 46.512\nEp 1 (Step 019045): Train loss 0.482, Val loss 46.831\nEp 1 (Step 019050): Train loss 0.458, Val loss 46.744\nEp 1 (Step 019055): Train loss 0.474, Val loss 46.709\nEp 1 (Step 019060): Train loss 0.466, Val loss 46.551\nEp 1 (Step 019065): Train loss 0.527, Val loss 45.961\nEp 1 (Step 019070): Train loss 0.524, Val loss 46.336\nEp 1 (Step 019075): Train loss 0.495, Val loss 46.636\nEp 1 (Step 019080): Train loss 0.508, Val loss 46.776\nEp 1 (Step 019085): Train loss 0.492, Val loss 46.773\nEp 1 (Step 019090): Train loss 0.498, Val loss 46.630\nEp 1 (Step 019095): Train loss 0.546, Val loss 46.747\nEp 1 (Step 019100): Train loss 0.550, Val loss 46.747\nEp 1 (Step 019105): Train loss 0.463, Val loss 45.664\nEp 1 (Step 019110): Train loss 0.517, Val loss 45.935\nEp 1 (Step 019115): Train loss 0.498, Val loss 46.538\nEp 1 (Step 019120): Train loss 0.458, Val loss 46.718\nEp 1 (Step 019125): Train loss 0.490, Val loss 46.218\nEp 1 (Step 019130): Train loss 0.491, Val loss 46.458\nEp 1 (Step 019135): Train loss 0.473, Val loss 46.613\nEp 1 (Step 019140): Train loss 0.474, Val loss 46.307\nEp 1 (Step 019145): Train loss 0.480, Val loss 46.915\nEp 1 (Step 019150): Train loss 0.455, Val loss 46.678\nEp 1 (Step 019155): Train loss 0.465, Val loss 46.553\nEp 1 (Step 019160): Train loss 0.476, Val loss 46.974\nEp 1 (Step 019165): Train loss 0.467, Val loss 46.209\nEp 1 (Step 019170): Train loss 0.472, Val loss 46.955\nEp 1 (Step 019175): Train loss 0.441, Val loss 46.731\nEp 1 (Step 019180): Train loss 0.448, Val loss 47.306\nEp 1 (Step 019185): Train loss 0.480, Val loss 46.654\nEp 1 (Step 019190): Train loss 0.484, Val loss 46.802\nEp 1 (Step 019195): Train loss 0.483, Val loss 46.595\nEp 1 (Step 019200): Train loss 0.489, Val loss 46.828\nEp 1 (Step 019205): Train loss 0.455, Val loss 46.336\nEp 1 (Step 019210): Train loss 0.512, Val loss 46.832\nEp 1 (Step 019215): Train loss 0.512, Val loss 46.475\nEp 1 (Step 019220): Train loss 0.479, Val loss 46.894\nEp 1 (Step 019225): Train loss 0.485, Val loss 45.844\nEp 1 (Step 019230): Train loss 0.520, Val loss 46.181\nEp 1 (Step 019235): Train loss 0.495, Val loss 46.128\nEp 1 (Step 019240): Train loss 0.442, Val loss 46.178\nEp 1 (Step 019245): Train loss 0.450, Val loss 46.000\nEp 1 (Step 019250): Train loss 0.508, Val loss 46.426\nEp 1 (Step 019255): Train loss 0.505, Val loss 46.189\nEp 1 (Step 019260): Train loss 0.479, Val loss 45.995\nEp 1 (Step 019265): Train loss 0.459, Val loss 45.775\nEp 1 (Step 019270): Train loss 0.444, Val loss 46.694\nEp 1 (Step 019275): Train loss 0.478, Val loss 46.212\nEp 1 (Step 019280): Train loss 0.496, Val loss 46.501\nEp 1 (Step 019285): Train loss 0.458, Val loss 47.081\nEp 1 (Step 019290): Train loss 0.473, Val loss 45.934\nEp 1 (Step 019295): Train loss 0.456, Val loss 45.978\nEp 1 (Step 019300): Train loss 0.517, Val loss 46.492\nEp 1 (Step 019305): Train loss 0.456, Val loss 46.712\nEp 1 (Step 019310): Train loss 0.503, Val loss 47.013\nEp 1 (Step 019315): Train loss 0.457, Val loss 46.089\nEp 1 (Step 019320): Train loss 0.459, Val loss 46.651\nEp 1 (Step 019325): Train loss 0.432, Val loss 46.826\nEp 1 (Step 019330): Train loss 0.513, Val loss 46.983\nEp 1 (Step 019335): Train loss 0.491, Val loss 46.169\nEp 1 (Step 019340): Train loss 0.494, Val loss 47.363\nEp 1 (Step 019345): Train loss 0.472, Val loss 46.382\nEp 1 (Step 019350): Train loss 0.501, Val loss 46.368\nEp 1 (Step 019355): Train loss 0.469, Val loss 46.057\nEp 1 (Step 019360): Train loss 0.477, Val loss 46.488\nEp 1 (Step 019365): Train loss 0.460, Val loss 47.090\nEp 1 (Step 019370): Train loss 0.488, Val loss 46.558\nEp 1 (Step 019375): Train loss 0.528, Val loss 45.837\nEp 1 (Step 019380): Train loss 0.446, Val loss 46.277\nEp 1 (Step 019385): Train loss 0.473, Val loss 46.277\nEp 1 (Step 019390): Train loss 0.491, Val loss 47.031\nEp 1 (Step 019395): Train loss 0.429, Val loss 46.533\nEp 1 (Step 019400): Train loss 0.456, Val loss 45.899\nEp 1 (Step 019405): Train loss 0.464, Val loss 46.057\nEp 1 (Step 019410): Train loss 0.420, Val loss 47.070\nEp 1 (Step 019415): Train loss 0.506, Val loss 45.849\nEp 1 (Step 019420): Train loss 0.493, Val loss 46.727\nEp 1 (Step 019425): Train loss 0.495, Val loss 46.564\nEp 1 (Step 019430): Train loss 0.480, Val loss 46.133\nEp 1 (Step 019435): Train loss 0.489, Val loss 47.018\nEp 1 (Step 019440): Train loss 0.500, Val loss 46.117\nEp 1 (Step 019445): Train loss 0.482, Val loss 46.740\nEp 1 (Step 019450): Train loss 0.467, Val loss 46.795\nEp 1 (Step 019455): Train loss 0.579, Val loss 46.642\nEp 1 (Step 019460): Train loss 0.490, Val loss 46.444\nEp 1 (Step 019465): Train loss 0.525, Val loss 46.765\nEp 1 (Step 019470): Train loss 0.480, Val loss 46.065\nEp 1 (Step 019475): Train loss 0.505, Val loss 46.680\nEp 1 (Step 019480): Train loss 0.497, Val loss 47.029\nEp 1 (Step 019485): Train loss 0.529, Val loss 46.731\nEp 1 (Step 019490): Train loss 0.518, Val loss 46.121\nEp 1 (Step 019495): Train loss 0.446, Val loss 46.818\nEp 1 (Step 019500): Train loss 0.460, Val loss 46.192\nEp 1 (Step 019505): Train loss 0.495, Val loss 47.036\nEp 1 (Step 019510): Train loss 0.476, Val loss 46.745\nEp 1 (Step 019515): Train loss 0.462, Val loss 46.688\nEp 1 (Step 019520): Train loss 0.450, Val loss 46.732\nEp 1 (Step 019525): Train loss 0.510, Val loss 47.349\nEp 1 (Step 019530): Train loss 0.462, Val loss 46.074\nEp 1 (Step 019535): Train loss 0.465, Val loss 46.744\nEp 1 (Step 019540): Train loss 0.437, Val loss 46.479\nEp 1 (Step 019545): Train loss 0.551, Val loss 45.991\nEp 1 (Step 019550): Train loss 0.462, Val loss 47.323\nEp 1 (Step 019555): Train loss 0.479, Val loss 46.150\nEp 1 (Step 019560): Train loss 0.489, Val loss 46.305\nEp 1 (Step 019565): Train loss 0.496, Val loss 46.591\nEp 1 (Step 019570): Train loss 0.519, Val loss 46.984\nEp 1 (Step 019575): Train loss 0.446, Val loss 46.308\nEp 1 (Step 019580): Train loss 0.525, Val loss 46.401\nEp 1 (Step 019585): Train loss 0.498, Val loss 46.999\nEp 1 (Step 019590): Train loss 0.488, Val loss 46.857\nEp 1 (Step 019595): Train loss 0.510, Val loss 46.822\nEp 1 (Step 019600): Train loss 0.464, Val loss 46.294\nEp 1 (Step 019605): Train loss 0.426, Val loss 46.561\nEp 1 (Step 019610): Train loss 0.491, Val loss 46.580\nEp 1 (Step 019615): Train loss 0.493, Val loss 47.243\nEp 1 (Step 019620): Train loss 0.557, Val loss 46.127\nEp 1 (Step 019625): Train loss 0.501, Val loss 45.949\nEp 1 (Step 019630): Train loss 0.490, Val loss 47.388\nEp 1 (Step 019635): Train loss 0.472, Val loss 47.249\nEp 1 (Step 019640): Train loss 0.477, Val loss 46.937\nEp 1 (Step 019645): Train loss 0.492, Val loss 45.703\nEp 1 (Step 019650): Train loss 0.478, Val loss 47.095\nEp 1 (Step 019655): Train loss 0.493, Val loss 46.263\nEp 1 (Step 019660): Train loss 0.458, Val loss 46.234\nEp 1 (Step 019665): Train loss 0.466, Val loss 46.195\nEp 1 (Step 019670): Train loss 0.513, Val loss 46.655\nEp 1 (Step 019675): Train loss 0.451, Val loss 46.381\nEp 1 (Step 019680): Train loss 0.542, Val loss 46.542\nEp 1 (Step 019685): Train loss 0.447, Val loss 46.116\nEp 1 (Step 019690): Train loss 0.513, Val loss 46.063\nEp 1 (Step 019695): Train loss 0.481, Val loss 46.160\nEp 1 (Step 019700): Train loss 0.463, Val loss 46.739\nEp 1 (Step 019705): Train loss 0.449, Val loss 46.434\nEp 1 (Step 019710): Train loss 0.470, Val loss 46.703\nEp 1 (Step 019715): Train loss 0.514, Val loss 46.918\nEp 1 (Step 019720): Train loss 0.485, Val loss 46.683\nEp 1 (Step 019725): Train loss 0.473, Val loss 47.206\nEp 1 (Step 019730): Train loss 0.447, Val loss 46.247\nEp 1 (Step 019735): Train loss 0.439, Val loss 46.475\nEp 1 (Step 019740): Train loss 0.516, Val loss 46.460\nEp 1 (Step 019745): Train loss 0.431, Val loss 47.042\nEp 1 (Step 019750): Train loss 0.449, Val loss 45.906\nEp 1 (Step 019755): Train loss 0.475, Val loss 46.658\nEp 1 (Step 019760): Train loss 0.482, Val loss 46.323\nEp 1 (Step 019765): Train loss 0.470, Val loss 46.769\nEp 1 (Step 019770): Train loss 0.484, Val loss 47.112\nEp 1 (Step 019775): Train loss 0.475, Val loss 47.713\nEp 1 (Step 019780): Train loss 0.511, Val loss 46.087\nEp 1 (Step 019785): Train loss 0.471, Val loss 46.712\nEp 1 (Step 019790): Train loss 0.552, Val loss 46.384\nEp 1 (Step 019795): Train loss 0.449, Val loss 46.347\nEp 1 (Step 019800): Train loss 0.495, Val loss 47.382\nEp 1 (Step 019805): Train loss 0.447, Val loss 46.481\nEp 1 (Step 019810): Train loss 0.468, Val loss 46.625\nEp 1 (Step 019815): Train loss 0.440, Val loss 46.704\nEp 1 (Step 019820): Train loss 0.500, Val loss 46.488\nEp 1 (Step 019825): Train loss 0.483, Val loss 47.097\nEp 1 (Step 019830): Train loss 0.540, Val loss 47.094\nEp 1 (Step 019835): Train loss 0.449, Val loss 45.928\nEp 1 (Step 019840): Train loss 0.523, Val loss 46.363\nEp 1 (Step 019845): Train loss 0.470, Val loss 46.665\nEp 1 (Step 019850): Train loss 0.507, Val loss 45.948\nEp 1 (Step 019855): Train loss 0.501, Val loss 46.658\nEp 1 (Step 019860): Train loss 0.460, Val loss 47.015\nEp 1 (Step 019865): Train loss 0.448, Val loss 46.711\nEp 1 (Step 019870): Train loss 0.474, Val loss 46.407\nEp 1 (Step 019875): Train loss 0.444, Val loss 46.564\nEp 1 (Step 019880): Train loss 0.451, Val loss 46.696\nEp 1 (Step 019885): Train loss 0.441, Val loss 46.939\nEp 1 (Step 019890): Train loss 0.459, Val loss 45.418\nEp 1 (Step 019895): Train loss 0.461, Val loss 47.143\nEp 1 (Step 019900): Train loss 0.445, Val loss 45.652\nEp 1 (Step 019905): Train loss 0.513, Val loss 46.941\nEp 1 (Step 019910): Train loss 0.500, Val loss 46.526\nEp 1 (Step 019915): Train loss 0.408, Val loss 45.798\nEp 1 (Step 019920): Train loss 0.479, Val loss 47.102\nEp 1 (Step 019925): Train loss 0.479, Val loss 46.639\nEp 1 (Step 019930): Train loss 0.443, Val loss 46.525\nEp 1 (Step 019935): Train loss 0.447, Val loss 46.100\nEp 1 (Step 019940): Train loss 0.470, Val loss 46.415\nEp 1 (Step 019945): Train loss 0.485, Val loss 46.064\nEp 1 (Step 019950): Train loss 0.484, Val loss 46.404\nEp 1 (Step 019955): Train loss 0.405, Val loss 46.473\nEp 1 (Step 019960): Train loss 0.501, Val loss 46.188\nEp 1 (Step 019965): Train loss 0.471, Val loss 47.522\nEp 1 (Step 019970): Train loss 0.492, Val loss 46.503\nEp 1 (Step 019975): Train loss 0.460, Val loss 46.640\nEp 1 (Step 019980): Train loss 0.467, Val loss 46.698\nEp 1 (Step 019985): Train loss 0.479, Val loss 46.380\nEp 1 (Step 019990): Train loss 0.454, Val loss 45.793\nEp 1 (Step 019995): Train loss 0.462, Val loss 46.800\nEp 1 (Step 020000): Train loss 0.478, Val loss 47.077\nEp 1 (Step 020005): Train loss 0.479, Val loss 46.709\nEp 1 (Step 020010): Train loss 0.476, Val loss 47.424\nEp 1 (Step 020015): Train loss 0.501, Val loss 47.029\nEp 1 (Step 020020): Train loss 0.476, Val loss 45.935\nEp 1 (Step 020025): Train loss 0.493, Val loss 46.124\nEp 1 (Step 020030): Train loss 0.506, Val loss 45.890\nEp 1 (Step 020035): Train loss 0.463, Val loss 46.359\nEp 1 (Step 020040): Train loss 0.483, Val loss 46.584\nEp 1 (Step 020045): Train loss 0.506, Val loss 45.540\nEp 1 (Step 020050): Train loss 0.448, Val loss 46.828\nEp 1 (Step 020055): Train loss 0.453, Val loss 47.244\nEp 1 (Step 020060): Train loss 0.472, Val loss 46.105\nEp 1 (Step 020065): Train loss 0.460, Val loss 46.082\nEp 1 (Step 020070): Train loss 0.492, Val loss 46.635\nEp 1 (Step 020075): Train loss 0.483, Val loss 46.681\nEp 1 (Step 020080): Train loss 0.492, Val loss 46.337\nEp 1 (Step 020085): Train loss 0.504, Val loss 46.433\nEp 1 (Step 020090): Train loss 0.467, Val loss 46.558\nEp 1 (Step 020095): Train loss 0.508, Val loss 46.732\nEp 1 (Step 020100): Train loss 0.501, Val loss 46.994\nEp 1 (Step 020105): Train loss 0.524, Val loss 46.893\nEp 1 (Step 020110): Train loss 0.461, Val loss 47.052\nEp 1 (Step 020115): Train loss 0.491, Val loss 46.587\nEp 1 (Step 020120): Train loss 0.439, Val loss 45.503\nEp 1 (Step 020125): Train loss 0.512, Val loss 46.492\nEp 1 (Step 020130): Train loss 0.452, Val loss 46.141\nEp 1 (Step 020135): Train loss 0.447, Val loss 46.746\nEp 1 (Step 020140): Train loss 0.439, Val loss 46.035\nEp 1 (Step 020145): Train loss 0.452, Val loss 46.300\nEp 1 (Step 020150): Train loss 0.457, Val loss 46.326\nEp 1 (Step 020155): Train loss 0.501, Val loss 46.218\nEp 1 (Step 020160): Train loss 0.476, Val loss 46.232\nEp 1 (Step 020165): Train loss 0.491, Val loss 47.227\nEp 1 (Step 020170): Train loss 0.459, Val loss 47.574\nEp 1 (Step 020175): Train loss 0.481, Val loss 47.185\nEp 1 (Step 020180): Train loss 0.483, Val loss 46.857\nEp 1 (Step 020185): Train loss 0.506, Val loss 46.568\nEp 1 (Step 020190): Train loss 0.486, Val loss 46.427\nEp 1 (Step 020195): Train loss 0.449, Val loss 46.416\nEp 1 (Step 020200): Train loss 0.472, Val loss 46.520\nEp 1 (Step 020205): Train loss 0.491, Val loss 47.419\nEp 1 (Step 020210): Train loss 0.455, Val loss 45.993\nEp 1 (Step 020215): Train loss 0.485, Val loss 47.016\nEp 1 (Step 020220): Train loss 0.447, Val loss 46.691\nEp 1 (Step 020225): Train loss 0.479, Val loss 46.983\nEp 1 (Step 020230): Train loss 0.507, Val loss 45.909\nEp 1 (Step 020235): Train loss 0.490, Val loss 46.050\nEp 1 (Step 020240): Train loss 0.451, Val loss 47.071\nEp 1 (Step 020245): Train loss 0.475, Val loss 47.524\nEp 1 (Step 020250): Train loss 0.540, Val loss 46.665\nEp 1 (Step 020255): Train loss 0.469, Val loss 46.760\nEp 1 (Step 020260): Train loss 0.434, Val loss 46.843\nEp 1 (Step 020265): Train loss 0.483, Val loss 47.277\nEp 1 (Step 020270): Train loss 0.506, Val loss 46.573\nEp 1 (Step 020275): Train loss 0.456, Val loss 46.283\nEp 1 (Step 020280): Train loss 0.474, Val loss 46.180\nEp 1 (Step 020285): Train loss 0.478, Val loss 46.663\nEp 1 (Step 020290): Train loss 0.465, Val loss 47.501\nEp 1 (Step 020295): Train loss 0.475, Val loss 46.555\nEp 1 (Step 020300): Train loss 0.489, Val loss 46.778\nEp 1 (Step 020305): Train loss 0.471, Val loss 47.321\nEp 1 (Step 020310): Train loss 0.491, Val loss 46.731\nEp 1 (Step 020315): Train loss 0.494, Val loss 46.607\nEp 1 (Step 020320): Train loss 0.429, Val loss 46.375\nEp 1 (Step 020325): Train loss 0.502, Val loss 46.395\nEp 1 (Step 020330): Train loss 0.505, Val loss 47.600\nEp 1 (Step 020335): Train loss 0.479, Val loss 46.831\nEp 1 (Step 020340): Train loss 0.447, Val loss 47.172\nEp 1 (Step 020345): Train loss 0.454, Val loss 46.600\nEp 1 (Step 020350): Train loss 0.455, Val loss 47.047\nEp 1 (Step 020355): Train loss 0.405, Val loss 46.852\nEp 1 (Step 020360): Train loss 0.430, Val loss 47.283\nEp 1 (Step 020365): Train loss 0.478, Val loss 46.169\nEp 1 (Step 020370): Train loss 0.475, Val loss 46.016\nEp 1 (Step 020375): Train loss 0.472, Val loss 46.041\nEp 1 (Step 020380): Train loss 0.464, Val loss 47.222\nEp 1 (Step 020385): Train loss 0.493, Val loss 46.392\nEp 1 (Step 020390): Train loss 0.452, Val loss 46.142\nEp 1 (Step 020395): Train loss 0.463, Val loss 46.384\nEp 1 (Step 020400): Train loss 0.475, Val loss 46.078\nEp 1 (Step 020405): Train loss 0.447, Val loss 46.311\nEp 1 (Step 020410): Train loss 0.452, Val loss 46.139\nEp 1 (Step 020415): Train loss 0.457, Val loss 46.401\nEp 1 (Step 020420): Train loss 0.469, Val loss 45.936\nEp 1 (Step 020425): Train loss 0.465, Val loss 46.483\nEp 1 (Step 020430): Train loss 0.465, Val loss 47.190\nEp 1 (Step 020435): Train loss 0.450, Val loss 46.036\nEp 1 (Step 020440): Train loss 0.499, Val loss 46.662\nEp 1 (Step 020445): Train loss 0.509, Val loss 46.252\nEp 1 (Step 020450): Train loss 0.499, Val loss 46.609\nEp 1 (Step 020455): Train loss 0.499, Val loss 47.334\nEp 1 (Step 020460): Train loss 0.552, Val loss 46.751\nEp 1 (Step 020465): Train loss 0.482, Val loss 46.605\nEp 1 (Step 020470): Train loss 0.489, Val loss 46.551\nEp 1 (Step 020475): Train loss 0.485, Val loss 47.151\nEp 1 (Step 020480): Train loss 0.471, Val loss 46.834\nEp 1 (Step 020485): Train loss 0.493, Val loss 46.072\nEp 1 (Step 020490): Train loss 0.454, Val loss 46.449\nEp 1 (Step 020495): Train loss 0.452, Val loss 46.662\nEp 1 (Step 020500): Train loss 0.504, Val loss 46.622\nEp 1 (Step 020505): Train loss 0.438, Val loss 46.667\nEp 1 (Step 020510): Train loss 0.466, Val loss 46.066\nEp 1 (Step 020515): Train loss 0.437, Val loss 46.866\nEp 1 (Step 020520): Train loss 0.467, Val loss 46.130\nEp 1 (Step 020525): Train loss 0.454, Val loss 46.350\nEp 1 (Step 020530): Train loss 0.467, Val loss 45.995\nEp 1 (Step 020535): Train loss 0.501, Val loss 46.597\nEp 1 (Step 020540): Train loss 0.480, Val loss 46.517\nEp 1 (Step 020545): Train loss 0.497, Val loss 46.452\nEp 1 (Step 020550): Train loss 0.464, Val loss 46.749\nEp 1 (Step 020555): Train loss 0.511, Val loss 46.609\nEp 1 (Step 020560): Train loss 0.447, Val loss 46.682\nEp 1 (Step 020565): Train loss 0.465, Val loss 46.419\nEp 1 (Step 020570): Train loss 0.514, Val loss 46.560\nEp 1 (Step 020575): Train loss 0.534, Val loss 46.340\nEp 1 (Step 020580): Train loss 0.441, Val loss 46.345\nEp 1 (Step 020585): Train loss 0.495, Val loss 46.241\nEp 1 (Step 020590): Train loss 0.472, Val loss 46.323\nEp 1 (Step 020595): Train loss 0.456, Val loss 46.917\nEp 1 (Step 020600): Train loss 0.512, Val loss 45.813\nEp 1 (Step 020605): Train loss 0.497, Val loss 46.011\nEp 1 (Step 020610): Train loss 0.469, Val loss 46.347\nEp 1 (Step 020615): Train loss 0.450, Val loss 45.693\nEp 1 (Step 020620): Train loss 0.485, Val loss 46.195\nEp 1 (Step 020625): Train loss 0.493, Val loss 46.318\nEp 1 (Step 020630): Train loss 0.491, Val loss 46.712\nEp 1 (Step 020635): Train loss 0.488, Val loss 45.847\nEp 1 (Step 020640): Train loss 0.477, Val loss 45.912\nEp 1 (Step 020645): Train loss 0.473, Val loss 47.003\nEp 1 (Step 020650): Train loss 0.499, Val loss 46.335\nEp 1 (Step 020655): Train loss 0.490, Val loss 47.513\nEp 1 (Step 020660): Train loss 0.436, Val loss 45.845\nEp 1 (Step 020665): Train loss 0.470, Val loss 45.685\nEp 1 (Step 020670): Train loss 0.456, Val loss 46.781\nEp 1 (Step 020675): Train loss 0.480, Val loss 46.597\nEp 1 (Step 020680): Train loss 0.468, Val loss 46.406\nEp 1 (Step 020685): Train loss 0.465, Val loss 46.191\nEp 1 (Step 020690): Train loss 0.459, Val loss 46.796\nEp 1 (Step 020695): Train loss 0.483, Val loss 47.039\nEp 1 (Step 020700): Train loss 0.419, Val loss 46.673\nEp 1 (Step 020705): Train loss 0.501, Val loss 46.476\nEp 1 (Step 020710): Train loss 0.508, Val loss 46.668\nEp 1 (Step 020715): Train loss 0.470, Val loss 46.812\nEp 1 (Step 020720): Train loss 0.466, Val loss 46.470\nEp 1 (Step 020725): Train loss 0.468, Val loss 46.670\nEp 1 (Step 020730): Train loss 0.465, Val loss 46.031\nEp 1 (Step 020735): Train loss 0.459, Val loss 46.184\nEp 1 (Step 020740): Train loss 0.435, Val loss 47.304\nEp 1 (Step 020745): Train loss 0.456, Val loss 46.007\nEp 1 (Step 020750): Train loss 0.468, Val loss 46.096\nEp 1 (Step 020755): Train loss 0.478, Val loss 47.007\nEp 1 (Step 020760): Train loss 0.442, Val loss 47.238\nEp 1 (Step 020765): Train loss 0.468, Val loss 47.365\nEp 1 (Step 020770): Train loss 0.449, Val loss 46.798\nEp 1 (Step 020775): Train loss 0.472, Val loss 46.101\nEp 1 (Step 020780): Train loss 0.480, Val loss 46.253\nEp 1 (Step 020785): Train loss 0.466, Val loss 46.375\nEp 1 (Step 020790): Train loss 0.476, Val loss 46.572\nEp 1 (Step 020795): Train loss 0.438, Val loss 46.025\nEp 1 (Step 020800): Train loss 0.469, Val loss 46.174\nEp 1 (Step 020805): Train loss 0.447, Val loss 46.186\nEp 1 (Step 020810): Train loss 0.466, Val loss 47.091\nEp 1 (Step 020815): Train loss 0.447, Val loss 46.533\nEp 1 (Step 020820): Train loss 0.494, Val loss 46.596\nEp 1 (Step 020825): Train loss 0.490, Val loss 47.358\nEp 1 (Step 020830): Train loss 0.437, Val loss 47.119\nEp 1 (Step 020835): Train loss 0.440, Val loss 45.528\nEp 1 (Step 020840): Train loss 0.451, Val loss 46.823\nEp 1 (Step 020845): Train loss 0.457, Val loss 46.760\nEp 1 (Step 020850): Train loss 0.468, Val loss 46.790\nEp 1 (Step 020855): Train loss 0.472, Val loss 45.793\nEp 1 (Step 020860): Train loss 0.457, Val loss 45.703\nEp 1 (Step 020865): Train loss 0.454, Val loss 46.291\nEp 1 (Step 020870): Train loss 0.535, Val loss 46.000\nEp 1 (Step 020875): Train loss 0.508, Val loss 45.865\nEp 1 (Step 020880): Train loss 0.466, Val loss 45.738\nEp 1 (Step 020885): Train loss 0.429, Val loss 46.361\nEp 1 (Step 020890): Train loss 0.447, Val loss 46.381\nEp 1 (Step 020895): Train loss 0.484, Val loss 47.371\nEp 1 (Step 020900): Train loss 0.458, Val loss 46.673\nEp 1 (Step 020905): Train loss 0.456, Val loss 46.415\nEp 1 (Step 020910): Train loss 0.436, Val loss 46.497\nEp 1 (Step 020915): Train loss 0.433, Val loss 46.453\nEp 1 (Step 020920): Train loss 0.444, Val loss 47.145\nEp 1 (Step 020925): Train loss 0.461, Val loss 46.040\nEp 1 (Step 020930): Train loss 0.451, Val loss 46.255\nEp 1 (Step 020935): Train loss 0.481, Val loss 46.364\nEp 1 (Step 020940): Train loss 0.454, Val loss 46.712\nEp 1 (Step 020945): Train loss 0.421, Val loss 46.653\nEp 1 (Step 020950): Train loss 0.496, Val loss 46.093\nEp 1 (Step 020955): Train loss 0.492, Val loss 47.105\nEp 1 (Step 020960): Train loss 0.463, Val loss 46.664\nEp 1 (Step 020965): Train loss 0.487, Val loss 47.134\nEp 1 (Step 020970): Train loss 0.486, Val loss 46.257\nEp 1 (Step 020975): Train loss 0.436, Val loss 46.275\nEp 1 (Step 020980): Train loss 0.486, Val loss 46.277\nEp 1 (Step 020985): Train loss 0.439, Val loss 46.129\nEp 1 (Step 020990): Train loss 0.482, Val loss 46.725\nEp 1 (Step 020995): Train loss 0.480, Val loss 46.871\nEp 1 (Step 021000): Train loss 0.485, Val loss 46.195\nEp 1 (Step 021005): Train loss 0.443, Val loss 46.650\nEp 1 (Step 021010): Train loss 0.437, Val loss 46.535\nEp 1 (Step 021015): Train loss 0.443, Val loss 46.492\nEp 1 (Step 021020): Train loss 0.506, Val loss 46.661\nEp 1 (Step 021025): Train loss 0.482, Val loss 46.020\nEp 1 (Step 021030): Train loss 0.485, Val loss 46.047\nEp 1 (Step 021035): Train loss 0.441, Val loss 46.169\nEp 1 (Step 021040): Train loss 0.466, Val loss 46.266\nEp 1 (Step 021045): Train loss 0.515, Val loss 46.498\nEp 1 (Step 021050): Train loss 0.450, Val loss 46.951\nEp 1 (Step 021055): Train loss 0.475, Val loss 46.588\nEp 1 (Step 021060): Train loss 0.490, Val loss 46.649\nEp 1 (Step 021065): Train loss 0.432, Val loss 44.944\nEp 1 (Step 021070): Train loss 0.528, Val loss 47.316\nEp 1 (Step 021075): Train loss 0.472, Val loss 46.219\nEp 1 (Step 021080): Train loss 0.442, Val loss 46.431\nEp 1 (Step 021085): Train loss 0.505, Val loss 46.397\nEp 1 (Step 021090): Train loss 0.460, Val loss 47.205\nEp 1 (Step 021095): Train loss 0.470, Val loss 47.074\nEp 1 (Step 021100): Train loss 0.440, Val loss 46.919\nEp 1 (Step 021105): Train loss 0.457, Val loss 46.593\nEp 1 (Step 021110): Train loss 0.511, Val loss 45.968\nEp 1 (Step 021115): Train loss 0.469, Val loss 46.086\nEp 1 (Step 021120): Train loss 0.499, Val loss 47.146\nEp 1 (Step 021125): Train loss 0.437, Val loss 46.774\nEp 1 (Step 021130): Train loss 0.453, Val loss 46.277\nEp 1 (Step 021135): Train loss 0.462, Val loss 45.760\nEp 1 (Step 021140): Train loss 0.461, Val loss 46.513\nEp 1 (Step 021145): Train loss 0.404, Val loss 46.275\nEp 1 (Step 021150): Train loss 0.473, Val loss 46.450\nEp 1 (Step 021155): Train loss 0.425, Val loss 46.627\nEp 1 (Step 021160): Train loss 0.470, Val loss 46.606\nEp 1 (Step 021165): Train loss 0.483, Val loss 46.034\nEp 1 (Step 021170): Train loss 0.455, Val loss 46.813\nEp 1 (Step 021175): Train loss 0.449, Val loss 46.519\nEp 1 (Step 021180): Train loss 0.443, Val loss 46.352\nEp 1 (Step 021185): Train loss 0.454, Val loss 46.566\nEp 1 (Step 021190): Train loss 0.448, Val loss 46.466\nEp 1 (Step 021195): Train loss 0.456, Val loss 46.243\nEp 1 (Step 021200): Train loss 0.454, Val loss 46.984\nEp 1 (Step 021205): Train loss 0.447, Val loss 46.448\nEp 1 (Step 021210): Train loss 0.461, Val loss 46.206\nEp 1 (Step 021215): Train loss 0.497, Val loss 46.406\nEp 1 (Step 021220): Train loss 0.456, Val loss 46.566\nEp 1 (Step 021225): Train loss 0.481, Val loss 46.771\nEp 1 (Step 021230): Train loss 0.449, Val loss 46.679\nEp 1 (Step 021235): Train loss 0.481, Val loss 47.384\nEp 1 (Step 021240): Train loss 0.459, Val loss 46.247\nEp 1 (Step 021245): Train loss 0.459, Val loss 46.237\nEp 1 (Step 021250): Train loss 0.502, Val loss 46.729\nEp 1 (Step 021255): Train loss 0.472, Val loss 46.541\nEp 1 (Step 021260): Train loss 0.476, Val loss 46.160\nEp 1 (Step 021265): Train loss 0.447, Val loss 46.453\nEp 1 (Step 021270): Train loss 0.428, Val loss 46.418\nEp 1 (Step 021275): Train loss 0.466, Val loss 46.956\nEp 1 (Step 021280): Train loss 0.474, Val loss 46.911\nEp 1 (Step 021285): Train loss 0.437, Val loss 46.347\nEp 1 (Step 021290): Train loss 0.479, Val loss 46.782\nEp 1 (Step 021295): Train loss 0.455, Val loss 46.392\nEp 1 (Step 021300): Train loss 0.452, Val loss 46.557\nEp 1 (Step 021305): Train loss 0.450, Val loss 46.951\nEp 1 (Step 021310): Train loss 0.457, Val loss 46.053\nEp 1 (Step 021315): Train loss 0.437, Val loss 46.156\nEp 1 (Step 021320): Train loss 0.442, Val loss 46.324\nEp 1 (Step 021325): Train loss 0.415, Val loss 47.070\nEp 1 (Step 021330): Train loss 0.499, Val loss 46.915\nEp 1 (Step 021335): Train loss 0.524, Val loss 46.554\nEp 1 (Step 021340): Train loss 0.452, Val loss 46.472\nEp 1 (Step 021345): Train loss 0.496, Val loss 46.966\nEp 1 (Step 021350): Train loss 0.457, Val loss 46.306\nEp 1 (Step 021355): Train loss 0.471, Val loss 46.289\nEp 1 (Step 021360): Train loss 0.517, Val loss 46.522\nEp 1 (Step 021365): Train loss 0.489, Val loss 46.730\nEp 1 (Step 021370): Train loss 0.471, Val loss 46.764\nEp 1 (Step 021375): Train loss 0.419, Val loss 46.029\nEp 1 (Step 021380): Train loss 0.443, Val loss 45.799\nEp 1 (Step 021385): Train loss 0.469, Val loss 46.199\nEp 1 (Step 021390): Train loss 0.498, Val loss 46.769\nEp 1 (Step 021395): Train loss 0.473, Val loss 47.543\nEp 1 (Step 021400): Train loss 0.470, Val loss 46.237\nEp 1 (Step 021405): Train loss 0.488, Val loss 46.868\nEp 1 (Step 021410): Train loss 0.478, Val loss 45.726\nEp 1 (Step 021415): Train loss 0.458, Val loss 46.097\nEp 1 (Step 021420): Train loss 0.502, Val loss 46.873\nEp 1 (Step 021425): Train loss 0.468, Val loss 47.377\nEp 1 (Step 021430): Train loss 0.448, Val loss 46.565\nEp 1 (Step 021435): Train loss 0.440, Val loss 46.753\nEp 1 (Step 021440): Train loss 0.441, Val loss 45.591\nEp 1 (Step 021445): Train loss 0.477, Val loss 46.542\nEp 1 (Step 021450): Train loss 0.483, Val loss 46.148\nEp 1 (Step 021455): Train loss 0.442, Val loss 47.086\nEp 1 (Step 021460): Train loss 0.411, Val loss 46.622\nEp 1 (Step 021465): Train loss 0.450, Val loss 46.028\nEp 1 (Step 021470): Train loss 0.480, Val loss 46.762\nEp 1 (Step 021475): Train loss 0.462, Val loss 46.138\nEp 1 (Step 021480): Train loss 0.471, Val loss 47.061\nEp 1 (Step 021485): Train loss 0.467, Val loss 46.043\nEp 1 (Step 021490): Train loss 0.470, Val loss 46.678\nEp 1 (Step 021495): Train loss 0.443, Val loss 46.696\nEp 1 (Step 021500): Train loss 0.450, Val loss 46.386\nEp 1 (Step 021505): Train loss 0.450, Val loss 46.435\nEp 1 (Step 021510): Train loss 0.497, Val loss 46.514\nEp 1 (Step 021515): Train loss 0.492, Val loss 46.473\nEp 1 (Step 021520): Train loss 0.496, Val loss 45.809\nEp 1 (Step 021525): Train loss 0.447, Val loss 46.062\nEp 1 (Step 021530): Train loss 0.465, Val loss 46.614\nEp 1 (Step 021535): Train loss 0.459, Val loss 46.043\nEp 1 (Step 021540): Train loss 0.457, Val loss 46.298\nEp 1 (Step 021545): Train loss 0.475, Val loss 46.792\nEp 1 (Step 021550): Train loss 0.487, Val loss 46.510\nEp 1 (Step 021555): Train loss 0.488, Val loss 45.596\nEp 1 (Step 021560): Train loss 0.462, Val loss 47.163\nEp 1 (Step 021565): Train loss 0.476, Val loss 47.130\nEp 1 (Step 021570): Train loss 0.479, Val loss 46.260\nEp 1 (Step 021575): Train loss 0.458, Val loss 46.257\nEp 1 (Step 021580): Train loss 0.477, Val loss 45.832\nEp 1 (Step 021585): Train loss 0.451, Val loss 46.494\nEp 1 (Step 021590): Train loss 0.451, Val loss 46.771\nEp 1 (Step 021595): Train loss 0.463, Val loss 46.598\nEp 1 (Step 021600): Train loss 0.486, Val loss 46.368\nEp 1 (Step 021605): Train loss 0.435, Val loss 46.300\nEp 1 (Step 021610): Train loss 0.409, Val loss 46.405\nEp 1 (Step 021615): Train loss 0.483, Val loss 47.072\nEp 1 (Step 021620): Train loss 0.511, Val loss 45.592\nEp 1 (Step 021625): Train loss 0.458, Val loss 46.417\nEp 1 (Step 021630): Train loss 0.470, Val loss 46.440\nEp 1 (Step 021635): Train loss 0.500, Val loss 45.564\nEp 1 (Step 021640): Train loss 0.469, Val loss 46.396\nEp 1 (Step 021645): Train loss 0.478, Val loss 47.125\nEp 1 (Step 021650): Train loss 0.465, Val loss 46.682\nEp 1 (Step 021655): Train loss 0.510, Val loss 46.864\nEp 1 (Step 021660): Train loss 0.512, Val loss 46.119\nEp 1 (Step 021665): Train loss 0.445, Val loss 46.357\nEp 1 (Step 021670): Train loss 0.458, Val loss 46.400\nEp 1 (Step 021675): Train loss 0.470, Val loss 46.570\nEp 1 (Step 021680): Train loss 0.451, Val loss 46.365\nEp 1 (Step 021685): Train loss 0.492, Val loss 46.885\nEp 1 (Step 021690): Train loss 0.512, Val loss 46.486\nEp 1 (Step 021695): Train loss 0.447, Val loss 46.427\nEp 1 (Step 021700): Train loss 0.446, Val loss 46.463\nEp 1 (Step 021705): Train loss 0.467, Val loss 46.638\nEp 1 (Step 021710): Train loss 0.409, Val loss 47.069\nEp 1 (Step 021715): Train loss 0.489, Val loss 47.364\nEp 1 (Step 021720): Train loss 0.440, Val loss 46.155\nEp 1 (Step 021725): Train loss 0.470, Val loss 46.955\nEp 1 (Step 021730): Train loss 0.484, Val loss 46.502\nEp 1 (Step 021735): Train loss 0.459, Val loss 47.072\nEp 1 (Step 021740): Train loss 0.466, Val loss 46.596\nEp 1 (Step 021745): Train loss 0.432, Val loss 45.976\nEp 1 (Step 021750): Train loss 0.523, Val loss 45.805\nEp 1 (Step 021755): Train loss 0.452, Val loss 46.550\nEp 1 (Step 021760): Train loss 0.455, Val loss 45.938\nEp 1 (Step 021765): Train loss 0.505, Val loss 46.050\nEp 1 (Step 021770): Train loss 0.472, Val loss 46.570\nEp 1 (Step 021775): Train loss 0.444, Val loss 46.299\nEp 1 (Step 021780): Train loss 0.454, Val loss 46.921\nEp 1 (Step 021785): Train loss 0.444, Val loss 47.077\nEp 1 (Step 021790): Train loss 0.452, Val loss 46.569\nEp 1 (Step 021795): Train loss 0.425, Val loss 46.251\nEp 1 (Step 021800): Train loss 0.500, Val loss 47.081\nEp 1 (Step 021805): Train loss 0.443, Val loss 47.587\nEp 1 (Step 021810): Train loss 0.449, Val loss 46.184\nEp 1 (Step 021815): Train loss 0.419, Val loss 46.081\nEp 1 (Step 021820): Train loss 0.424, Val loss 47.284\nEp 1 (Step 021825): Train loss 0.447, Val loss 46.129\nEp 1 (Step 021830): Train loss 0.439, Val loss 47.092\nEp 1 (Step 021835): Train loss 0.446, Val loss 46.959\nEp 1 (Step 021840): Train loss 0.451, Val loss 45.592\nEp 1 (Step 021845): Train loss 0.458, Val loss 46.896\nEp 1 (Step 021850): Train loss 0.425, Val loss 46.572\nEp 1 (Step 021855): Train loss 0.433, Val loss 46.893\nEp 1 (Step 021860): Train loss 0.447, Val loss 46.366\nEp 1 (Step 021865): Train loss 0.481, Val loss 46.577\nEp 1 (Step 021870): Train loss 0.397, Val loss 46.510\nEp 1 (Step 021875): Train loss 0.462, Val loss 46.425\nEp 1 (Step 021880): Train loss 0.455, Val loss 46.084\nEp 1 (Step 021885): Train loss 0.438, Val loss 47.216\nEp 1 (Step 021890): Train loss 0.515, Val loss 46.523\nEp 1 (Step 021895): Train loss 0.453, Val loss 47.202\nEp 1 (Step 021900): Train loss 0.466, Val loss 46.748\nEp 1 (Step 021905): Train loss 0.488, Val loss 47.245\nEp 1 (Step 021910): Train loss 0.418, Val loss 46.202\nEp 1 (Step 021915): Train loss 0.453, Val loss 46.897\nEp 1 (Step 021920): Train loss 0.412, Val loss 45.860\nEp 1 (Step 021925): Train loss 0.453, Val loss 46.769\nEp 1 (Step 021930): Train loss 0.419, Val loss 46.363\nEp 1 (Step 021935): Train loss 0.502, Val loss 47.039\nEp 1 (Step 021940): Train loss 0.484, Val loss 46.214\nEp 1 (Step 021945): Train loss 0.416, Val loss 47.064\nEp 1 (Step 021950): Train loss 0.433, Val loss 46.905\nEp 1 (Step 021955): Train loss 0.466, Val loss 46.226\nEp 1 (Step 021960): Train loss 0.455, Val loss 46.255\nEp 1 (Step 021965): Train loss 0.468, Val loss 46.996\nEp 1 (Step 021970): Train loss 0.458, Val loss 46.349\nEp 1 (Step 021975): Train loss 0.461, Val loss 46.779\nEp 1 (Step 021980): Train loss 0.457, Val loss 46.696\nEp 1 (Step 021985): Train loss 0.454, Val loss 46.938\nEp 1 (Step 021990): Train loss 0.457, Val loss 46.404\nEp 1 (Step 021995): Train loss 0.478, Val loss 46.500\nEp 1 (Step 022000): Train loss 0.498, Val loss 46.412\nEp 1 (Step 022005): Train loss 0.377, Val loss 46.610\nEp 1 (Step 022010): Train loss 0.453, Val loss 47.622\nEp 1 (Step 022015): Train loss 0.438, Val loss 47.159\nEp 1 (Step 022020): Train loss 0.464, Val loss 46.889\nEp 1 (Step 022025): Train loss 0.467, Val loss 47.095\nEp 1 (Step 022030): Train loss 0.478, Val loss 47.286\nEp 1 (Step 022035): Train loss 0.456, Val loss 46.437\nEp 1 (Step 022040): Train loss 0.452, Val loss 47.165\nEp 1 (Step 022045): Train loss 0.472, Val loss 46.460\nEp 1 (Step 022050): Train loss 0.457, Val loss 46.621\nEp 1 (Step 022055): Train loss 0.423, Val loss 46.622\nEp 1 (Step 022060): Train loss 0.439, Val loss 46.588\nEp 1 (Step 022065): Train loss 0.446, Val loss 46.509\nEp 1 (Step 022070): Train loss 0.429, Val loss 46.552\nEp 1 (Step 022075): Train loss 0.474, Val loss 46.679\nEp 1 (Step 022080): Train loss 0.443, Val loss 46.415\nEp 1 (Step 022085): Train loss 0.447, Val loss 46.292\nEp 1 (Step 022090): Train loss 0.480, Val loss 47.103\nEp 1 (Step 022095): Train loss 0.465, Val loss 46.770\nEp 1 (Step 022100): Train loss 0.452, Val loss 46.685\nEp 1 (Step 022105): Train loss 0.458, Val loss 46.605\nEp 1 (Step 022110): Train loss 0.453, Val loss 46.871\nEp 1 (Step 022115): Train loss 0.429, Val loss 46.098\nEp 1 (Step 022120): Train loss 0.420, Val loss 45.863\nEp 1 (Step 022125): Train loss 0.431, Val loss 46.552\nEp 1 (Step 022130): Train loss 0.419, Val loss 46.638\nEp 1 (Step 022135): Train loss 0.437, Val loss 46.011\nEp 1 (Step 022140): Train loss 0.469, Val loss 46.335\nEp 1 (Step 022145): Train loss 0.447, Val loss 47.228\nEp 1 (Step 022150): Train loss 0.461, Val loss 46.879\nEp 1 (Step 022155): Train loss 0.488, Val loss 47.496\nEp 1 (Step 022160): Train loss 0.486, Val loss 47.296\nEp 1 (Step 022165): Train loss 0.463, Val loss 46.736\nEp 1 (Step 022170): Train loss 0.455, Val loss 46.182\nEp 1 (Step 022175): Train loss 0.487, Val loss 46.833\nEp 1 (Step 022180): Train loss 0.480, Val loss 46.537\nEp 1 (Step 022185): Train loss 0.438, Val loss 46.977\nEp 1 (Step 022190): Train loss 0.442, Val loss 46.989\nEp 1 (Step 022195): Train loss 0.448, Val loss 46.865\nEp 1 (Step 022200): Train loss 0.467, Val loss 46.496\nEp 1 (Step 022205): Train loss 0.471, Val loss 46.339\nEp 1 (Step 022210): Train loss 0.498, Val loss 46.784\nEp 1 (Step 022215): Train loss 0.442, Val loss 46.937\nEp 1 (Step 022220): Train loss 0.415, Val loss 47.498\nEp 1 (Step 022225): Train loss 0.448, Val loss 46.425\nEp 1 (Step 022230): Train loss 0.468, Val loss 46.810\nEp 1 (Step 022235): Train loss 0.426, Val loss 46.124\nEp 1 (Step 022240): Train loss 0.424, Val loss 46.752\nEp 1 (Step 022245): Train loss 0.424, Val loss 46.349\nEp 1 (Step 022250): Train loss 0.473, Val loss 45.980\nEp 1 (Step 022255): Train loss 0.473, Val loss 46.865\nEp 1 (Step 022260): Train loss 0.452, Val loss 46.319\nEp 1 (Step 022265): Train loss 0.450, Val loss 46.991\nEp 1 (Step 022270): Train loss 0.489, Val loss 46.608\nEp 1 (Step 022275): Train loss 0.454, Val loss 46.315\nEp 1 (Step 022280): Train loss 0.481, Val loss 46.117\nEp 1 (Step 022285): Train loss 0.461, Val loss 46.897\nEp 1 (Step 022290): Train loss 0.443, Val loss 46.140\nEp 1 (Step 022295): Train loss 0.473, Val loss 46.132\nEp 1 (Step 022300): Train loss 0.450, Val loss 46.957\nEp 1 (Step 022305): Train loss 0.430, Val loss 46.139\nEp 1 (Step 022310): Train loss 0.459, Val loss 46.974\nEp 1 (Step 022315): Train loss 0.461, Val loss 46.300\nEp 1 (Step 022320): Train loss 0.477, Val loss 46.725\nEp 1 (Step 022325): Train loss 0.474, Val loss 46.214\nEp 1 (Step 022330): Train loss 0.465, Val loss 46.575\nEp 1 (Step 022335): Train loss 0.436, Val loss 46.683\nEp 1 (Step 022340): Train loss 0.482, Val loss 46.342\nEp 1 (Step 022345): Train loss 0.542, Val loss 46.945\nEp 1 (Step 022350): Train loss 0.467, Val loss 46.667\nEp 1 (Step 022355): Train loss 0.468, Val loss 47.233\nEp 1 (Step 022360): Train loss 0.446, Val loss 46.719\nEp 1 (Step 022365): Train loss 0.432, Val loss 46.673\nEp 1 (Step 022370): Train loss 0.449, Val loss 46.332\nEp 1 (Step 022375): Train loss 0.434, Val loss 47.070\nEp 1 (Step 022380): Train loss 0.481, Val loss 46.352\nEp 1 (Step 022385): Train loss 0.450, Val loss 46.884\nEp 1 (Step 022390): Train loss 0.454, Val loss 46.253\nEp 1 (Step 022395): Train loss 0.454, Val loss 46.865\nEp 1 (Step 022400): Train loss 0.441, Val loss 47.346\nEp 1 (Step 022405): Train loss 0.483, Val loss 46.649\nEp 1 (Step 022410): Train loss 0.441, Val loss 47.512\nEp 1 (Step 022415): Train loss 0.484, Val loss 46.518\nEp 1 (Step 022420): Train loss 0.460, Val loss 46.916\nEp 1 (Step 022425): Train loss 0.438, Val loss 47.156\nEp 1 (Step 022430): Train loss 0.449, Val loss 45.781\nEp 1 (Step 022435): Train loss 0.445, Val loss 46.743\nEp 1 (Step 022440): Train loss 0.419, Val loss 46.913\nEp 1 (Step 022445): Train loss 0.438, Val loss 46.761\nEp 1 (Step 022450): Train loss 0.444, Val loss 47.225\nEp 1 (Step 022455): Train loss 0.450, Val loss 46.567\nEp 1 (Step 022460): Train loss 0.413, Val loss 46.964\nEp 1 (Step 022465): Train loss 0.447, Val loss 47.274\nEp 1 (Step 022470): Train loss 0.446, Val loss 47.123\nEp 1 (Step 022475): Train loss 0.436, Val loss 46.490\nEp 1 (Step 022480): Train loss 0.465, Val loss 46.424\nEp 1 (Step 022485): Train loss 0.459, Val loss 46.245\nEp 1 (Step 022490): Train loss 0.470, Val loss 46.553\nEp 1 (Step 022495): Train loss 0.467, Val loss 47.049\nEp 1 (Step 022500): Train loss 0.472, Val loss 47.271\nEp 1 (Step 022505): Train loss 0.459, Val loss 46.522\nEp 1 (Step 022510): Train loss 0.430, Val loss 47.068\nEp 1 (Step 022515): Train loss 0.421, Val loss 47.261\nEp 1 (Step 022520): Train loss 0.457, Val loss 48.100\nEp 1 (Step 022525): Train loss 0.417, Val loss 46.017\nEp 1 (Step 022530): Train loss 0.437, Val loss 47.049\nEp 1 (Step 022535): Train loss 0.424, Val loss 46.507\nEp 1 (Step 022540): Train loss 0.448, Val loss 47.154\nEp 1 (Step 022545): Train loss 0.451, Val loss 46.913\nEp 1 (Step 022550): Train loss 0.467, Val loss 46.774\nEp 1 (Step 022555): Train loss 0.418, Val loss 46.230\nEp 1 (Step 022560): Train loss 0.479, Val loss 46.802\nEp 1 (Step 022565): Train loss 0.436, Val loss 47.012\nEp 1 (Step 022570): Train loss 0.411, Val loss 46.966\nEp 1 (Step 022575): Train loss 0.460, Val loss 47.173\nEp 1 (Step 022580): Train loss 0.472, Val loss 46.960\nEp 1 (Step 022585): Train loss 0.442, Val loss 46.799\nEp 1 (Step 022590): Train loss 0.410, Val loss 46.405\nEp 1 (Step 022595): Train loss 0.483, Val loss 46.805\nEp 1 (Step 022600): Train loss 0.453, Val loss 47.137\nEp 1 (Step 022605): Train loss 0.469, Val loss 46.663\nEp 1 (Step 022610): Train loss 0.424, Val loss 46.947\nEp 1 (Step 022615): Train loss 0.460, Val loss 46.835\nEp 1 (Step 022620): Train loss 0.442, Val loss 46.064\nEp 1 (Step 022625): Train loss 0.452, Val loss 46.892\nEp 1 (Step 022630): Train loss 0.462, Val loss 46.466\nEp 1 (Step 022635): Train loss 0.440, Val loss 46.672\nEp 1 (Step 022640): Train loss 0.425, Val loss 46.875\nEp 1 (Step 022645): Train loss 0.441, Val loss 46.650\nEp 1 (Step 022650): Train loss 0.428, Val loss 45.729\nEp 1 (Step 022655): Train loss 0.455, Val loss 46.456\nEp 1 (Step 022660): Train loss 0.471, Val loss 45.876\nEp 1 (Step 022665): Train loss 0.463, Val loss 47.244\nEp 1 (Step 022670): Train loss 0.444, Val loss 46.732\nEp 1 (Step 022675): Train loss 0.461, Val loss 47.050\nEp 1 (Step 022680): Train loss 0.462, Val loss 46.807\nEp 1 (Step 022685): Train loss 0.432, Val loss 46.867\nEp 1 (Step 022690): Train loss 0.486, Val loss 47.399\nEp 1 (Step 022695): Train loss 0.470, Val loss 47.053\nEp 1 (Step 022700): Train loss 0.471, Val loss 46.611\nEp 1 (Step 022705): Train loss 0.447, Val loss 46.689\nEp 1 (Step 022710): Train loss 0.452, Val loss 46.352\nEp 1 (Step 022715): Train loss 0.426, Val loss 46.179\nEp 1 (Step 022720): Train loss 0.456, Val loss 46.512\nEp 1 (Step 022725): Train loss 0.467, Val loss 46.227\nEp 1 (Step 022730): Train loss 0.431, Val loss 46.011\nEp 1 (Step 022735): Train loss 0.440, Val loss 45.830\nEp 1 (Step 022740): Train loss 0.474, Val loss 46.858\nEp 1 (Step 022745): Train loss 0.476, Val loss 46.344\nEp 1 (Step 022750): Train loss 0.456, Val loss 46.986\nEp 1 (Step 022755): Train loss 0.489, Val loss 46.625\nEp 1 (Step 022760): Train loss 0.467, Val loss 46.737\nEp 1 (Step 022765): Train loss 0.426, Val loss 46.691\nEp 1 (Step 022770): Train loss 0.489, Val loss 45.891\nEp 1 (Step 022775): Train loss 0.463, Val loss 46.411\nEp 1 (Step 022780): Train loss 0.476, Val loss 46.411\nEp 1 (Step 022785): Train loss 0.489, Val loss 46.178\nEp 1 (Step 022790): Train loss 0.487, Val loss 47.291\nEp 1 (Step 022795): Train loss 0.464, Val loss 46.788\nEp 1 (Step 022800): Train loss 0.515, Val loss 47.164\nEp 1 (Step 022805): Train loss 0.473, Val loss 46.387\nEp 1 (Step 022810): Train loss 0.456, Val loss 46.628\nEp 1 (Step 022815): Train loss 0.407, Val loss 46.546\nEp 1 (Step 022820): Train loss 0.458, Val loss 46.954\nEp 1 (Step 022825): Train loss 0.454, Val loss 47.585\nEp 1 (Step 022830): Train loss 0.446, Val loss 46.513\nEp 1 (Step 022835): Train loss 0.457, Val loss 47.585\nEp 1 (Step 022840): Train loss 0.438, Val loss 46.887\nEp 1 (Step 022845): Train loss 0.462, Val loss 46.259\nEp 1 (Step 022850): Train loss 0.419, Val loss 45.674\nEp 1 (Step 022855): Train loss 0.418, Val loss 46.891\nEp 1 (Step 022860): Train loss 0.464, Val loss 46.429\nEp 1 (Step 022865): Train loss 0.425, Val loss 46.532\nEp 1 (Step 022870): Train loss 0.427, Val loss 46.428\nEp 1 (Step 022875): Train loss 0.490, Val loss 46.080\nEp 1 (Step 022880): Train loss 0.430, Val loss 46.312\nEp 1 (Step 022885): Train loss 0.470, Val loss 46.815\nEp 1 (Step 022890): Train loss 0.441, Val loss 46.290\nEp 1 (Step 022895): Train loss 0.457, Val loss 46.916\nEp 1 (Step 022900): Train loss 0.441, Val loss 45.745\nEp 1 (Step 022905): Train loss 0.465, Val loss 46.647\nEp 1 (Step 022910): Train loss 0.431, Val loss 46.600\nEp 1 (Step 022915): Train loss 0.463, Val loss 46.747\nEp 1 (Step 022920): Train loss 0.479, Val loss 46.611\nEp 1 (Step 022925): Train loss 0.434, Val loss 47.860\nEp 1 (Step 022930): Train loss 0.414, Val loss 46.551\nEp 1 (Step 022935): Train loss 0.444, Val loss 46.468\nEp 1 (Step 022940): Train loss 0.465, Val loss 46.847\nEp 1 (Step 022945): Train loss 0.436, Val loss 46.616\nEp 1 (Step 022950): Train loss 0.421, Val loss 46.877\nEp 1 (Step 022955): Train loss 0.496, Val loss 47.076\nEp 1 (Step 022960): Train loss 0.464, Val loss 46.813\nEp 1 (Step 022965): Train loss 0.475, Val loss 46.648\nEp 1 (Step 022970): Train loss 0.454, Val loss 46.938\nEp 1 (Step 022975): Train loss 0.438, Val loss 47.195\nEp 1 (Step 022980): Train loss 0.460, Val loss 46.396\nEp 1 (Step 022985): Train loss 0.470, Val loss 46.479\nEp 1 (Step 022990): Train loss 0.501, Val loss 46.484\nEp 1 (Step 022995): Train loss 0.507, Val loss 46.750\nEp 1 (Step 023000): Train loss 0.438, Val loss 46.805\nEp 1 (Step 023005): Train loss 0.429, Val loss 46.734\nEp 1 (Step 023010): Train loss 0.437, Val loss 45.703\nEp 1 (Step 023015): Train loss 0.443, Val loss 46.147\nEp 1 (Step 023020): Train loss 0.457, Val loss 47.027\nEp 1 (Step 023025): Train loss 0.472, Val loss 46.920\nEp 1 (Step 023030): Train loss 0.468, Val loss 46.996\nEp 1 (Step 023035): Train loss 0.452, Val loss 47.210\nEp 1 (Step 023040): Train loss 0.446, Val loss 46.097\nEp 1 (Step 023045): Train loss 0.424, Val loss 46.061\nEp 1 (Step 023050): Train loss 0.432, Val loss 45.303\nEp 1 (Step 023055): Train loss 0.472, Val loss 46.221\nEp 1 (Step 023060): Train loss 0.443, Val loss 46.704\nEp 1 (Step 023065): Train loss 0.478, Val loss 46.841\nEp 1 (Step 023070): Train loss 0.490, Val loss 46.421\nEp 1 (Step 023075): Train loss 0.437, Val loss 46.456\nEp 1 (Step 023080): Train loss 0.457, Val loss 46.687\nEp 1 (Step 023085): Train loss 0.479, Val loss 46.621\nEp 1 (Step 023090): Train loss 0.475, Val loss 46.561\nEp 1 (Step 023095): Train loss 0.401, Val loss 46.754\nEp 1 (Step 023100): Train loss 0.474, Val loss 46.917\nEp 1 (Step 023105): Train loss 0.465, Val loss 46.359\nEp 1 (Step 023110): Train loss 0.429, Val loss 46.913\nEp 1 (Step 023115): Train loss 0.458, Val loss 46.345\nEp 1 (Step 023120): Train loss 0.443, Val loss 46.234\nEp 1 (Step 023125): Train loss 0.449, Val loss 47.101\nEp 1 (Step 023130): Train loss 0.481, Val loss 46.182\nEp 1 (Step 023135): Train loss 0.455, Val loss 46.692\nEp 1 (Step 023140): Train loss 0.444, Val loss 46.301\nEp 1 (Step 023145): Train loss 0.478, Val loss 46.569\nEp 1 (Step 023150): Train loss 0.450, Val loss 46.224\nEp 1 (Step 023155): Train loss 0.407, Val loss 46.189\nEp 1 (Step 023160): Train loss 0.485, Val loss 46.896\nEp 1 (Step 023165): Train loss 0.483, Val loss 46.151\nEp 1 (Step 023170): Train loss 0.445, Val loss 46.418\nEp 1 (Step 023175): Train loss 0.471, Val loss 47.201\nEp 1 (Step 023180): Train loss 0.443, Val loss 47.190\nEp 1 (Step 023185): Train loss 0.462, Val loss 46.144\nEp 1 (Step 023190): Train loss 0.451, Val loss 46.415\nEp 1 (Step 023195): Train loss 0.423, Val loss 46.679\nEp 1 (Step 023200): Train loss 0.444, Val loss 47.306\nEp 1 (Step 023205): Train loss 0.404, Val loss 45.869\nEp 1 (Step 023210): Train loss 0.448, Val loss 46.988\nEp 1 (Step 023215): Train loss 0.395, Val loss 46.274\nEp 1 (Step 023220): Train loss 0.442, Val loss 46.468\nEp 1 (Step 023225): Train loss 0.423, Val loss 45.735\nEp 1 (Step 023230): Train loss 0.471, Val loss 46.841\nEp 1 (Step 023235): Train loss 0.443, Val loss 46.264\nEp 1 (Step 023240): Train loss 0.416, Val loss 46.204\nEp 1 (Step 023245): Train loss 0.472, Val loss 46.767\nEp 1 (Step 023250): Train loss 0.462, Val loss 45.922\nEp 1 (Step 023255): Train loss 0.445, Val loss 46.511\nEp 1 (Step 023260): Train loss 0.463, Val loss 47.045\nEp 1 (Step 023265): Train loss 0.441, Val loss 46.125\nEp 1 (Step 023270): Train loss 0.429, Val loss 46.565\nEp 1 (Step 023275): Train loss 0.437, Val loss 45.777\nEp 1 (Step 023280): Train loss 0.460, Val loss 47.373\nEp 1 (Step 023285): Train loss 0.466, Val loss 47.045\nEp 1 (Step 023290): Train loss 0.472, Val loss 46.782\nEp 1 (Step 023295): Train loss 0.513, Val loss 46.110\nEp 1 (Step 023300): Train loss 0.411, Val loss 46.496\nEp 1 (Step 023305): Train loss 0.393, Val loss 45.251\nEp 1 (Step 023310): Train loss 0.453, Val loss 46.228\nEp 1 (Step 023315): Train loss 0.439, Val loss 46.185\nEp 1 (Step 023320): Train loss 0.438, Val loss 46.013\nEp 1 (Step 023325): Train loss 0.439, Val loss 46.846\nEp 1 (Step 023330): Train loss 0.475, Val loss 46.533\nEp 1 (Step 023335): Train loss 0.448, Val loss 46.568\nEp 1 (Step 023340): Train loss 0.437, Val loss 46.906\nEp 1 (Step 023345): Train loss 0.445, Val loss 46.804\nEp 1 (Step 023350): Train loss 0.441, Val loss 46.575\nEp 1 (Step 023355): Train loss 0.456, Val loss 46.390\nEp 1 (Step 023360): Train loss 0.492, Val loss 46.746\nEp 1 (Step 023365): Train loss 0.464, Val loss 47.517\nEp 1 (Step 023370): Train loss 0.450, Val loss 46.557\nEp 1 (Step 023375): Train loss 0.430, Val loss 45.782\nEp 1 (Step 023380): Train loss 0.394, Val loss 46.209\nEp 1 (Step 023385): Train loss 0.440, Val loss 46.227\nEp 1 (Step 023390): Train loss 0.463, Val loss 46.789\nEp 1 (Step 023395): Train loss 0.484, Val loss 46.583\nEp 1 (Step 023400): Train loss 0.462, Val loss 45.814\nEp 1 (Step 023405): Train loss 0.462, Val loss 46.057\nEp 1 (Step 023410): Train loss 0.446, Val loss 46.133\nEp 1 (Step 023415): Train loss 0.470, Val loss 46.876\nEp 1 (Step 023420): Train loss 0.453, Val loss 46.005\nEp 1 (Step 023425): Train loss 0.426, Val loss 46.006\nEp 1 (Step 023430): Train loss 0.481, Val loss 47.533\nEp 1 (Step 023435): Train loss 0.464, Val loss 46.223\nEp 1 (Step 023440): Train loss 0.429, Val loss 46.491\nEp 1 (Step 023445): Train loss 0.443, Val loss 46.596\nEp 1 (Step 023450): Train loss 0.416, Val loss 47.013\nEp 1 (Step 023455): Train loss 0.448, Val loss 47.069\nEp 1 (Step 023460): Train loss 0.462, Val loss 46.268\nEp 1 (Step 023465): Train loss 0.449, Val loss 46.640\nEp 1 (Step 023470): Train loss 0.480, Val loss 47.418\nEp 1 (Step 023475): Train loss 0.428, Val loss 46.219\nEp 1 (Step 023480): Train loss 0.505, Val loss 46.202\nEp 1 (Step 023485): Train loss 0.423, Val loss 46.546\nEp 1 (Step 023490): Train loss 0.451, Val loss 45.985\nEp 1 (Step 023495): Train loss 0.461, Val loss 45.867\nEp 1 (Step 023500): Train loss 0.434, Val loss 47.002\nEp 1 (Step 023505): Train loss 0.420, Val loss 46.377\nEp 1 (Step 023510): Train loss 0.415, Val loss 47.249\nEp 1 (Step 023515): Train loss 0.449, Val loss 46.711\nEp 1 (Step 023520): Train loss 0.462, Val loss 46.471\nEp 1 (Step 023525): Train loss 0.448, Val loss 47.343\nEp 1 (Step 023530): Train loss 0.436, Val loss 46.300\nEp 1 (Step 023535): Train loss 0.470, Val loss 46.214\nEp 1 (Step 023540): Train loss 0.438, Val loss 46.677\nEp 1 (Step 023545): Train loss 0.461, Val loss 46.983\nEp 1 (Step 023550): Train loss 0.430, Val loss 46.747\nEp 1 (Step 023555): Train loss 0.452, Val loss 46.999\nEp 1 (Step 023560): Train loss 0.448, Val loss 45.842\nEp 1 (Step 023565): Train loss 0.437, Val loss 47.315\nEp 1 (Step 023570): Train loss 0.464, Val loss 47.003\nEp 1 (Step 023575): Train loss 0.430, Val loss 46.617\nEp 1 (Step 023580): Train loss 0.451, Val loss 46.737\nEp 1 (Step 023585): Train loss 0.469, Val loss 46.315\nEp 1 (Step 023590): Train loss 0.397, Val loss 46.554\nEp 1 (Step 023595): Train loss 0.456, Val loss 46.408\nEp 1 (Step 023600): Train loss 0.478, Val loss 46.482\nEp 1 (Step 023605): Train loss 0.433, Val loss 46.477\nEp 1 (Step 023610): Train loss 0.454, Val loss 46.519\nEp 1 (Step 023615): Train loss 0.412, Val loss 45.944\nEp 1 (Step 023620): Train loss 0.442, Val loss 46.365\nEp 1 (Step 023625): Train loss 0.460, Val loss 46.778\nEp 1 (Step 023630): Train loss 0.442, Val loss 46.279\nEp 1 (Step 023635): Train loss 0.419, Val loss 46.079\nEp 1 (Step 023640): Train loss 0.434, Val loss 46.795\nEp 1 (Step 023645): Train loss 0.414, Val loss 46.257\nEp 1 (Step 023650): Train loss 0.471, Val loss 46.281\nEp 1 (Step 023655): Train loss 0.466, Val loss 46.925\nEp 1 (Step 023660): Train loss 0.456, Val loss 46.674\nEp 1 (Step 023665): Train loss 0.449, Val loss 46.653\nEp 1 (Step 023670): Train loss 0.392, Val loss 46.015\nEp 1 (Step 023675): Train loss 0.453, Val loss 46.487\nEp 1 (Step 023680): Train loss 0.461, Val loss 45.637\nEp 1 (Step 023685): Train loss 0.466, Val loss 46.535\nEp 1 (Step 023690): Train loss 0.427, Val loss 46.957\nEp 1 (Step 023695): Train loss 0.432, Val loss 46.393\nEp 1 (Step 023700): Train loss 0.410, Val loss 46.095\nEp 1 (Step 023705): Train loss 0.434, Val loss 46.089\nEp 1 (Step 023710): Train loss 0.469, Val loss 46.581\nEp 1 (Step 023715): Train loss 0.444, Val loss 46.285\nEp 1 (Step 023720): Train loss 0.459, Val loss 46.390\nEp 1 (Step 023725): Train loss 0.434, Val loss 46.400\nEp 1 (Step 023730): Train loss 0.442, Val loss 46.218\nEp 1 (Step 023735): Train loss 0.559, Val loss 46.196\nEp 1 (Step 023740): Train loss 0.471, Val loss 46.576\nEp 1 (Step 023745): Train loss 0.442, Val loss 46.295\nEp 1 (Step 023750): Train loss 0.408, Val loss 46.568\nEp 1 (Step 023755): Train loss 0.451, Val loss 46.956\nEp 1 (Step 023760): Train loss 0.418, Val loss 45.557\nEp 1 (Step 023765): Train loss 0.443, Val loss 45.919\nEp 1 (Step 023770): Train loss 0.419, Val loss 46.750\nEp 1 (Step 023775): Train loss 0.434, Val loss 46.501\nEp 1 (Step 023780): Train loss 0.442, Val loss 47.416\nEp 1 (Step 023785): Train loss 0.388, Val loss 46.095\nEp 1 (Step 023790): Train loss 0.459, Val loss 46.112\nEp 1 (Step 023795): Train loss 0.463, Val loss 46.782\nEp 1 (Step 023800): Train loss 0.442, Val loss 46.003\nEp 1 (Step 023805): Train loss 0.469, Val loss 46.876\nEp 1 (Step 023810): Train loss 0.428, Val loss 46.232\nEp 1 (Step 023815): Train loss 0.446, Val loss 46.632\nEp 1 (Step 023820): Train loss 0.417, Val loss 46.483\nEp 1 (Step 023825): Train loss 0.413, Val loss 46.973\nEp 1 (Step 023830): Train loss 0.469, Val loss 46.858\nEp 1 (Step 023835): Train loss 0.439, Val loss 46.866\nEp 1 (Step 023840): Train loss 0.405, Val loss 46.645\nEp 1 (Step 023845): Train loss 0.435, Val loss 46.483\nEp 1 (Step 023850): Train loss 0.433, Val loss 47.233\nEp 1 (Step 023855): Train loss 0.441, Val loss 46.022\nEp 1 (Step 023860): Train loss 0.444, Val loss 46.690\nEp 1 (Step 023865): Train loss 0.442, Val loss 46.388\nEp 1 (Step 023870): Train loss 0.419, Val loss 46.673\nEp 1 (Step 023875): Train loss 0.410, Val loss 46.444\nEp 1 (Step 023880): Train loss 0.537, Val loss 46.476\nEp 1 (Step 023885): Train loss 0.467, Val loss 45.908\nEp 1 (Step 023890): Train loss 0.552, Val loss 46.254\nEp 1 (Step 023895): Train loss 0.459, Val loss 45.722\nEp 1 (Step 023900): Train loss 0.450, Val loss 46.526\nEp 1 (Step 023905): Train loss 0.454, Val loss 46.632\nEp 1 (Step 023910): Train loss 0.486, Val loss 46.734\nEp 1 (Step 023915): Train loss 0.448, Val loss 46.338\nEp 1 (Step 023920): Train loss 0.418, Val loss 46.071\nEp 1 (Step 023925): Train loss 0.501, Val loss 46.552\nEp 1 (Step 023930): Train loss 0.403, Val loss 46.235\nEp 1 (Step 023935): Train loss 0.426, Val loss 46.188\nEp 1 (Step 023940): Train loss 0.445, Val loss 46.726\nEp 1 (Step 023945): Train loss 0.401, Val loss 46.621\nEp 1 (Step 023950): Train loss 0.426, Val loss 46.117\nEp 1 (Step 023955): Train loss 0.431, Val loss 46.910\nEp 1 (Step 023960): Train loss 0.442, Val loss 46.857\nEp 1 (Step 023965): Train loss 0.434, Val loss 46.728\nEp 1 (Step 023970): Train loss 0.468, Val loss 46.790\nEp 1 (Step 023975): Train loss 0.480, Val loss 46.828\nEp 1 (Step 023980): Train loss 0.474, Val loss 46.204\nEp 1 (Step 023985): Train loss 0.457, Val loss 46.351\nEp 1 (Step 023990): Train loss 0.428, Val loss 46.640\nEp 1 (Step 023995): Train loss 0.456, Val loss 46.900\nEp 1 (Step 024000): Train loss 0.428, Val loss 46.397\nEp 1 (Step 024005): Train loss 0.437, Val loss 46.590\nEp 1 (Step 024010): Train loss 0.450, Val loss 46.738\nEp 1 (Step 024015): Train loss 0.513, Val loss 45.847\nEp 1 (Step 024020): Train loss 0.411, Val loss 46.758\nEp 1 (Step 024025): Train loss 0.438, Val loss 46.959\nEp 1 (Step 024030): Train loss 0.432, Val loss 46.642\nEp 1 (Step 024035): Train loss 0.460, Val loss 46.837\nEp 1 (Step 024040): Train loss 0.394, Val loss 47.043\nEp 1 (Step 024045): Train loss 0.382, Val loss 46.885\nEp 1 (Step 024050): Train loss 0.401, Val loss 46.523\nEp 1 (Step 024055): Train loss 0.425, Val loss 46.874\nEp 1 (Step 024060): Train loss 0.444, Val loss 46.212\nEp 1 (Step 024065): Train loss 0.415, Val loss 46.288\nEp 1 (Step 024070): Train loss 0.470, Val loss 46.616\nEp 1 (Step 024075): Train loss 0.446, Val loss 46.481\nEp 1 (Step 024080): Train loss 0.445, Val loss 46.337\nEp 1 (Step 024085): Train loss 0.429, Val loss 46.874\nEp 1 (Step 024090): Train loss 0.434, Val loss 46.660\nEp 1 (Step 024095): Train loss 0.419, Val loss 47.554\nEp 1 (Step 024100): Train loss 0.452, Val loss 46.683\nEp 1 (Step 024105): Train loss 0.418, Val loss 46.916\nEp 1 (Step 024110): Train loss 0.394, Val loss 46.698\nEp 1 (Step 024115): Train loss 0.459, Val loss 46.580\nEp 1 (Step 024120): Train loss 0.441, Val loss 46.288\nEp 1 (Step 024125): Train loss 0.447, Val loss 46.792\nEp 1 (Step 024130): Train loss 0.475, Val loss 46.551\nEp 1 (Step 024135): Train loss 0.460, Val loss 47.234\nEp 1 (Step 024140): Train loss 0.469, Val loss 47.109\nEp 1 (Step 024145): Train loss 0.436, Val loss 46.795\nEp 1 (Step 024150): Train loss 0.397, Val loss 46.684\nEp 1 (Step 024155): Train loss 0.478, Val loss 46.613\nEp 1 (Step 024160): Train loss 0.403, Val loss 46.341\nEp 1 (Step 024165): Train loss 0.454, Val loss 46.045\nEp 1 (Step 024170): Train loss 0.445, Val loss 46.681\nEp 1 (Step 024175): Train loss 0.459, Val loss 46.585\nEp 1 (Step 024180): Train loss 0.423, Val loss 46.509\nEp 1 (Step 024185): Train loss 0.398, Val loss 47.185\nEp 1 (Step 024190): Train loss 0.434, Val loss 46.215\nEp 1 (Step 024195): Train loss 0.484, Val loss 46.868\nEp 1 (Step 024200): Train loss 0.432, Val loss 46.758\nEp 1 (Step 024205): Train loss 0.468, Val loss 46.828\nEp 1 (Step 024210): Train loss 0.448, Val loss 46.727\nEp 1 (Step 024215): Train loss 0.472, Val loss 46.403\nEp 1 (Step 024220): Train loss 0.449, Val loss 46.505\nEp 1 (Step 024225): Train loss 0.445, Val loss 46.391\nEp 1 (Step 024230): Train loss 0.403, Val loss 46.374\nEp 1 (Step 024235): Train loss 0.443, Val loss 46.844\nEp 1 (Step 024240): Train loss 0.411, Val loss 46.704\nEp 1 (Step 024245): Train loss 0.443, Val loss 46.574\nEp 1 (Step 024250): Train loss 0.396, Val loss 46.088\nEp 1 (Step 024255): Train loss 0.405, Val loss 45.431\nEp 1 (Step 024260): Train loss 0.427, Val loss 47.178\nEp 1 (Step 024265): Train loss 0.411, Val loss 45.978\nEp 1 (Step 024270): Train loss 0.449, Val loss 46.469\nEp 1 (Step 024275): Train loss 0.430, Val loss 46.527\nEp 1 (Step 024280): Train loss 0.440, Val loss 45.980\nEp 1 (Step 024285): Train loss 0.455, Val loss 47.240\nEp 1 (Step 024290): Train loss 0.465, Val loss 46.646\nEp 1 (Step 024295): Train loss 0.473, Val loss 47.136\nEp 1 (Step 024300): Train loss 0.428, Val loss 46.559\nEp 1 (Step 024305): Train loss 0.429, Val loss 46.481\nEp 1 (Step 024310): Train loss 0.449, Val loss 47.016\nEp 1 (Step 024315): Train loss 0.423, Val loss 46.949\nEp 1 (Step 024320): Train loss 0.404, Val loss 46.258\nEp 1 (Step 024325): Train loss 0.445, Val loss 46.343\nEp 1 (Step 024330): Train loss 0.375, Val loss 46.849\nEp 1 (Step 024335): Train loss 0.453, Val loss 47.324\nEp 1 (Step 024340): Train loss 0.397, Val loss 47.124\nEp 1 (Step 024345): Train loss 0.419, Val loss 45.859\nEp 1 (Step 024350): Train loss 0.453, Val loss 47.245\nEp 1 (Step 024355): Train loss 0.430, Val loss 46.522\nEp 1 (Step 024360): Train loss 0.400, Val loss 45.601\nEp 1 (Step 024365): Train loss 0.394, Val loss 47.145\nEp 1 (Step 024370): Train loss 0.430, Val loss 46.956\nEp 1 (Step 024375): Train loss 0.425, Val loss 46.157\nEp 1 (Step 024380): Train loss 0.456, Val loss 45.986\nEp 1 (Step 024385): Train loss 0.437, Val loss 47.162\nEp 1 (Step 024390): Train loss 0.440, Val loss 46.073\nEp 1 (Step 024395): Train loss 0.451, Val loss 46.770\nEp 1 (Step 024400): Train loss 0.483, Val loss 46.768\nEp 1 (Step 024405): Train loss 0.439, Val loss 47.811\nEp 1 (Step 024410): Train loss 0.431, Val loss 46.859\nEp 1 (Step 024415): Train loss 0.465, Val loss 45.931\nEp 1 (Step 024420): Train loss 0.513, Val loss 46.364\nEp 1 (Step 024425): Train loss 0.475, Val loss 47.327\nEp 1 (Step 024430): Train loss 0.466, Val loss 46.477\nEp 1 (Step 024435): Train loss 0.435, Val loss 46.321\nEp 1 (Step 024440): Train loss 0.401, Val loss 46.210\nEp 1 (Step 024445): Train loss 0.465, Val loss 46.530\nEp 1 (Step 024450): Train loss 0.460, Val loss 46.778\nEp 1 (Step 024455): Train loss 0.471, Val loss 47.615\nEp 1 (Step 024460): Train loss 0.460, Val loss 46.082\nEp 1 (Step 024465): Train loss 0.445, Val loss 46.819\nEp 1 (Step 024470): Train loss 0.443, Val loss 46.977\nEp 1 (Step 024475): Train loss 0.437, Val loss 46.504\nEp 1 (Step 024480): Train loss 0.458, Val loss 46.376\nEp 1 (Step 024485): Train loss 0.431, Val loss 47.260\nEp 1 (Step 024490): Train loss 0.449, Val loss 46.441\nEp 1 (Step 024495): Train loss 0.425, Val loss 46.594\nEp 1 (Step 024500): Train loss 0.472, Val loss 46.559\nEp 1 (Step 024505): Train loss 0.452, Val loss 45.995\nEp 1 (Step 024510): Train loss 0.466, Val loss 46.332\nEp 1 (Step 024515): Train loss 0.403, Val loss 46.397\nEp 1 (Step 024520): Train loss 0.429, Val loss 46.247\nEp 1 (Step 024525): Train loss 0.430, Val loss 46.439\nEp 1 (Step 024530): Train loss 0.419, Val loss 46.137\nEp 1 (Step 024535): Train loss 0.427, Val loss 46.802\nEp 1 (Step 024540): Train loss 0.436, Val loss 46.220\nEp 1 (Step 024545): Train loss 0.415, Val loss 46.470\nEp 1 (Step 024550): Train loss 0.428, Val loss 46.215\nEp 1 (Step 024555): Train loss 0.446, Val loss 46.497\nEp 1 (Step 024560): Train loss 0.443, Val loss 46.532\nEp 1 (Step 024565): Train loss 0.435, Val loss 46.259\nEp 1 (Step 024570): Train loss 0.448, Val loss 46.404\nEp 1 (Step 024575): Train loss 0.470, Val loss 45.622\nEp 1 (Step 024580): Train loss 0.416, Val loss 46.369\nEp 1 (Step 024585): Train loss 0.472, Val loss 46.741\nEp 1 (Step 024590): Train loss 0.460, Val loss 46.814\nEp 1 (Step 024595): Train loss 0.429, Val loss 46.256\nEp 1 (Step 024600): Train loss 0.430, Val loss 45.934\nEp 1 (Step 024605): Train loss 0.462, Val loss 46.431\nEp 1 (Step 024610): Train loss 0.412, Val loss 47.328\nEp 1 (Step 024615): Train loss 0.428, Val loss 46.699\nEp 1 (Step 024620): Train loss 0.396, Val loss 46.142\nEp 1 (Step 024625): Train loss 0.429, Val loss 46.504\nEp 1 (Step 024630): Train loss 0.371, Val loss 46.483\nEp 1 (Step 024635): Train loss 0.458, Val loss 46.531\nEp 1 (Step 024640): Train loss 0.438, Val loss 46.959\nEp 1 (Step 024645): Train loss 0.417, Val loss 46.757\nEp 1 (Step 024650): Train loss 0.396, Val loss 46.649\nEp 1 (Step 024655): Train loss 0.410, Val loss 46.738\nEp 1 (Step 024660): Train loss 0.395, Val loss 46.572\nEp 1 (Step 024665): Train loss 0.408, Val loss 46.042\nEp 1 (Step 024670): Train loss 0.406, Val loss 46.346\nEp 1 (Step 024675): Train loss 0.396, Val loss 46.902\nEp 1 (Step 024680): Train loss 0.417, Val loss 46.756\nEp 1 (Step 024685): Train loss 0.406, Val loss 46.668\nEp 1 (Step 024690): Train loss 0.455, Val loss 47.178\nEp 1 (Step 024695): Train loss 0.436, Val loss 46.871\nEp 1 (Step 024700): Train loss 0.472, Val loss 46.355\nEp 1 (Step 024705): Train loss 0.447, Val loss 46.916\nEp 1 (Step 024710): Train loss 0.470, Val loss 46.541\nEp 1 (Step 024715): Train loss 0.397, Val loss 46.611\nEp 1 (Step 024720): Train loss 0.448, Val loss 45.786\nEp 1 (Step 024725): Train loss 0.417, Val loss 46.211\nEp 1 (Step 024730): Train loss 0.436, Val loss 46.641\nEp 1 (Step 024735): Train loss 0.444, Val loss 47.004\nEp 1 (Step 024740): Train loss 0.419, Val loss 46.449\nEp 1 (Step 024745): Train loss 0.402, Val loss 46.018\nEp 1 (Step 024750): Train loss 0.404, Val loss 46.721\nEp 1 (Step 024755): Train loss 0.424, Val loss 46.403\nEp 1 (Step 024760): Train loss 0.399, Val loss 46.963\nEp 1 (Step 024765): Train loss 0.438, Val loss 46.523\nEp 1 (Step 024770): Train loss 0.416, Val loss 45.910\nEp 1 (Step 024775): Train loss 0.491, Val loss 46.666\nEp 1 (Step 024780): Train loss 0.443, Val loss 46.118\nEp 1 (Step 024785): Train loss 0.479, Val loss 46.084\nEp 1 (Step 024790): Train loss 0.400, Val loss 46.400\nEp 1 (Step 024795): Train loss 0.449, Val loss 46.983\nEp 1 (Step 024800): Train loss 0.424, Val loss 46.463\nEp 1 (Step 024805): Train loss 0.450, Val loss 46.006\nEp 1 (Step 024810): Train loss 0.382, Val loss 46.338\nEp 1 (Step 024815): Train loss 0.391, Val loss 46.940\nEp 1 (Step 024820): Train loss 0.411, Val loss 46.465\nEp 1 (Step 024825): Train loss 0.460, Val loss 46.604\nEp 1 (Step 024830): Train loss 0.415, Val loss 46.236\nEp 1 (Step 024835): Train loss 0.419, Val loss 46.302\nEp 1 (Step 024840): Train loss 0.429, Val loss 47.133\nEp 1 (Step 024845): Train loss 0.409, Val loss 47.507\nEp 1 (Step 024850): Train loss 0.442, Val loss 46.976\nEp 1 (Step 024855): Train loss 0.430, Val loss 46.837\nEp 1 (Step 024860): Train loss 0.413, Val loss 46.363\nEp 1 (Step 024865): Train loss 0.419, Val loss 46.153\nEp 1 (Step 024870): Train loss 0.424, Val loss 46.977\nEp 1 (Step 024875): Train loss 0.402, Val loss 46.307\nEp 1 (Step 024880): Train loss 0.453, Val loss 46.614\nEp 1 (Step 024885): Train loss 0.464, Val loss 46.383\nEp 1 (Step 024890): Train loss 0.407, Val loss 46.816\nEp 1 (Step 024895): Train loss 0.455, Val loss 45.881\nEp 1 (Step 024900): Train loss 0.414, Val loss 46.442\nEp 1 (Step 024905): Train loss 0.407, Val loss 46.650\nEp 1 (Step 024910): Train loss 0.454, Val loss 46.393\nEp 1 (Step 024915): Train loss 0.410, Val loss 47.421\nEp 1 (Step 024920): Train loss 0.397, Val loss 45.857\nEp 1 (Step 024925): Train loss 0.435, Val loss 46.434\nEp 1 (Step 024930): Train loss 0.408, Val loss 46.747\nEp 1 (Step 024935): Train loss 0.425, Val loss 46.609\nEp 1 (Step 024940): Train loss 0.404, Val loss 46.685\nEp 1 (Step 024945): Train loss 0.436, Val loss 46.798\nEp 1 (Step 024950): Train loss 0.404, Val loss 46.444\nEp 1 (Step 024955): Train loss 0.443, Val loss 46.869\nEp 1 (Step 024960): Train loss 0.402, Val loss 46.540\nEp 1 (Step 024965): Train loss 0.448, Val loss 46.236\nEp 1 (Step 024970): Train loss 0.436, Val loss 46.207\nEp 1 (Step 024975): Train loss 0.491, Val loss 47.275\nEp 1 (Step 024980): Train loss 0.466, Val loss 47.559\nEp 1 (Step 024985): Train loss 0.436, Val loss 46.738\nEp 1 (Step 024990): Train loss 0.443, Val loss 47.103\nEp 1 (Step 024995): Train loss 0.437, Val loss 46.263\nEp 1 (Step 025000): Train loss 0.444, Val loss 46.164\nEp 1 (Step 025005): Train loss 0.397, Val loss 46.954\nEp 1 (Step 025010): Train loss 0.413, Val loss 46.810\nEp 1 (Step 025015): Train loss 0.375, Val loss 46.781\nEp 1 (Step 025020): Train loss 0.400, Val loss 46.864\nEp 1 (Step 025025): Train loss 0.419, Val loss 46.593\nEp 1 (Step 025030): Train loss 0.434, Val loss 46.697\nEp 1 (Step 025035): Train loss 0.432, Val loss 46.389\nEp 1 (Step 025040): Train loss 0.439, Val loss 46.280\nEp 1 (Step 025045): Train loss 0.441, Val loss 46.786\nEp 1 (Step 025050): Train loss 0.405, Val loss 46.660\nEp 1 (Step 025055): Train loss 0.402, Val loss 45.643\nEp 1 (Step 025060): Train loss 0.384, Val loss 46.543\nEp 1 (Step 025065): Train loss 0.497, Val loss 46.501\nEp 1 (Step 025070): Train loss 0.444, Val loss 46.854\nEp 1 (Step 025075): Train loss 0.431, Val loss 46.971\nEp 1 (Step 025080): Train loss 0.446, Val loss 47.072\nEp 1 (Step 025085): Train loss 0.426, Val loss 47.007\nEp 1 (Step 025090): Train loss 0.408, Val loss 46.720\nEp 1 (Step 025095): Train loss 0.437, Val loss 45.783\nEp 1 (Step 025100): Train loss 0.459, Val loss 46.624\nEp 1 (Step 025105): Train loss 0.435, Val loss 46.260\nEp 1 (Step 025110): Train loss 0.424, Val loss 46.379\nEp 1 (Step 025115): Train loss 0.382, Val loss 46.627\nEp 1 (Step 025120): Train loss 0.464, Val loss 47.735\nEp 1 (Step 025125): Train loss 0.469, Val loss 46.461\nEp 1 (Step 025130): Train loss 0.412, Val loss 46.808\nEp 1 (Step 025135): Train loss 0.448, Val loss 46.552\nEp 1 (Step 025140): Train loss 0.423, Val loss 47.072\nEp 1 (Step 025145): Train loss 0.449, Val loss 46.920\nEp 1 (Step 025150): Train loss 0.444, Val loss 47.039\nEp 1 (Step 025155): Train loss 0.441, Val loss 46.951\nEp 1 (Step 025160): Train loss 0.430, Val loss 47.091\nEp 1 (Step 025165): Train loss 0.436, Val loss 46.612\nEp 1 (Step 025170): Train loss 0.445, Val loss 46.673\nEp 1 (Step 025175): Train loss 0.393, Val loss 46.512\nEp 1 (Step 025180): Train loss 0.425, Val loss 46.139\nEp 1 (Step 025185): Train loss 0.451, Val loss 46.505\nEp 1 (Step 025190): Train loss 0.444, Val loss 46.129\nEp 1 (Step 025195): Train loss 0.465, Val loss 46.849\nEp 1 (Step 025200): Train loss 0.433, Val loss 46.569\nEp 1 (Step 025205): Train loss 0.398, Val loss 46.701\nEp 1 (Step 025210): Train loss 0.426, Val loss 46.030\nEp 1 (Step 025215): Train loss 0.432, Val loss 46.141\nEp 1 (Step 025220): Train loss 0.424, Val loss 46.729\nEp 1 (Step 025225): Train loss 0.457, Val loss 46.303\nEp 1 (Step 025230): Train loss 0.445, Val loss 46.044\nEp 1 (Step 025235): Train loss 0.455, Val loss 46.882\nEp 1 (Step 025240): Train loss 0.400, Val loss 46.491\nEp 1 (Step 025245): Train loss 0.449, Val loss 47.044\nEp 1 (Step 025250): Train loss 0.468, Val loss 46.834\nEp 1 (Step 025255): Train loss 0.440, Val loss 46.775\nEp 1 (Step 025260): Train loss 0.446, Val loss 46.965\nEp 1 (Step 025265): Train loss 0.413, Val loss 46.055\nEp 1 (Step 025270): Train loss 0.470, Val loss 46.421\nEp 1 (Step 025275): Train loss 0.438, Val loss 45.672\nEp 1 (Step 025280): Train loss 0.458, Val loss 46.257\nEp 1 (Step 025285): Train loss 0.456, Val loss 46.242\nEp 1 (Step 025290): Train loss 0.451, Val loss 46.839\nEp 1 (Step 025295): Train loss 0.443, Val loss 47.376\nEp 1 (Step 025300): Train loss 0.418, Val loss 45.732\nEp 1 (Step 025305): Train loss 0.480, Val loss 47.356\nEp 1 (Step 025310): Train loss 0.443, Val loss 47.113\nEp 1 (Step 025315): Train loss 0.409, Val loss 46.015\nEp 1 (Step 025320): Train loss 0.437, Val loss 46.068\nEp 1 (Step 025325): Train loss 0.417, Val loss 46.216\nEp 1 (Step 025330): Train loss 0.452, Val loss 46.437\nEp 1 (Step 025335): Train loss 0.432, Val loss 46.307\nEp 1 (Step 025340): Train loss 0.432, Val loss 46.583\nEp 1 (Step 025345): Train loss 0.425, Val loss 46.105\nEp 1 (Step 025350): Train loss 0.411, Val loss 46.977\nEp 1 (Step 025355): Train loss 0.409, Val loss 46.529\nEp 1 (Step 025360): Train loss 0.419, Val loss 45.587\nEp 1 (Step 025365): Train loss 0.385, Val loss 46.381\nEp 1 (Step 025370): Train loss 0.405, Val loss 46.614\nEp 1 (Step 025375): Train loss 0.405, Val loss 46.871\nEp 1 (Step 025380): Train loss 0.426, Val loss 47.033\nEp 1 (Step 025385): Train loss 0.404, Val loss 46.594\nEp 1 (Step 025390): Train loss 0.427, Val loss 46.153\nEp 1 (Step 025395): Train loss 0.438, Val loss 46.176\nEp 1 (Step 025400): Train loss 0.422, Val loss 46.430\nEp 1 (Step 025405): Train loss 0.425, Val loss 46.383\nEp 1 (Step 025410): Train loss 0.446, Val loss 47.095\nEp 1 (Step 025415): Train loss 0.408, Val loss 46.734\nEp 1 (Step 025420): Train loss 0.385, Val loss 46.406\nEp 1 (Step 025425): Train loss 0.413, Val loss 46.830\nEp 1 (Step 025430): Train loss 0.396, Val loss 46.483\nEp 1 (Step 025435): Train loss 0.497, Val loss 46.721\nEp 1 (Step 025440): Train loss 0.431, Val loss 45.858\nEp 1 (Step 025445): Train loss 0.420, Val loss 46.780\nEp 1 (Step 025450): Train loss 0.447, Val loss 47.293\nEp 1 (Step 025455): Train loss 0.454, Val loss 46.585\nEp 1 (Step 025460): Train loss 0.447, Val loss 46.231\nEp 1 (Step 025465): Train loss 0.410, Val loss 45.626\nEp 1 (Step 025470): Train loss 0.467, Val loss 46.888\nEp 1 (Step 025475): Train loss 0.440, Val loss 47.137\nEp 1 (Step 025480): Train loss 0.429, Val loss 45.816\nEp 1 (Step 025485): Train loss 0.397, Val loss 46.167\nEp 1 (Step 025490): Train loss 0.386, Val loss 45.871\nEp 1 (Step 025495): Train loss 0.411, Val loss 46.407\nEp 1 (Step 025500): Train loss 0.408, Val loss 46.145\nEp 1 (Step 025505): Train loss 0.415, Val loss 46.865\nEp 1 (Step 025510): Train loss 0.427, Val loss 46.406\nEp 1 (Step 025515): Train loss 0.445, Val loss 46.270\nEp 1 (Step 025520): Train loss 0.454, Val loss 47.323\nEp 1 (Step 025525): Train loss 0.425, Val loss 46.803\nEp 1 (Step 025530): Train loss 0.418, Val loss 46.835\nEp 1 (Step 025535): Train loss 0.451, Val loss 45.802\nEp 1 (Step 025540): Train loss 0.500, Val loss 46.776\nEp 1 (Step 025545): Train loss 0.459, Val loss 47.064\nEp 1 (Step 025550): Train loss 0.396, Val loss 46.879\nEp 1 (Step 025555): Train loss 0.451, Val loss 46.605\nEp 1 (Step 025560): Train loss 0.407, Val loss 46.124\nEp 1 (Step 025565): Train loss 0.414, Val loss 45.971\nEp 1 (Step 025570): Train loss 0.438, Val loss 46.550\nEp 1 (Step 025575): Train loss 0.450, Val loss 45.477\nEp 1 (Step 025580): Train loss 0.454, Val loss 46.377\nEp 1 (Step 025585): Train loss 0.406, Val loss 46.528\nEp 1 (Step 025590): Train loss 0.417, Val loss 46.390\nEp 1 (Step 025595): Train loss 0.436, Val loss 46.911\nEp 1 (Step 025600): Train loss 0.453, Val loss 46.856\nEp 1 (Step 025605): Train loss 0.453, Val loss 47.414\nEp 1 (Step 025610): Train loss 0.431, Val loss 46.875\nEp 1 (Step 025615): Train loss 0.452, Val loss 46.211\nEp 1 (Step 025620): Train loss 0.445, Val loss 46.483\nEp 1 (Step 025625): Train loss 0.403, Val loss 45.967\nEp 1 (Step 025630): Train loss 0.421, Val loss 45.953\nEp 1 (Step 025635): Train loss 0.431, Val loss 47.384\nEp 1 (Step 025640): Train loss 0.437, Val loss 46.565\nEp 1 (Step 025645): Train loss 0.404, Val loss 47.872\nEp 1 (Step 025650): Train loss 0.442, Val loss 46.417\nEp 1 (Step 025655): Train loss 0.422, Val loss 46.207\nEp 1 (Step 025660): Train loss 0.410, Val loss 46.664\nEp 1 (Step 025665): Train loss 0.461, Val loss 46.028\nEp 1 (Step 025670): Train loss 0.422, Val loss 46.615\nEp 1 (Step 025675): Train loss 0.448, Val loss 46.365\nEp 1 (Step 025680): Train loss 0.461, Val loss 46.496\nEp 1 (Step 025685): Train loss 0.425, Val loss 46.767\nEp 1 (Step 025690): Train loss 0.420, Val loss 46.245\nEp 1 (Step 025695): Train loss 0.392, Val loss 46.138\nEp 1 (Step 025700): Train loss 0.399, Val loss 45.975\nEp 1 (Step 025705): Train loss 0.439, Val loss 47.130\nEp 1 (Step 025710): Train loss 0.424, Val loss 46.362\nEp 1 (Step 025715): Train loss 0.467, Val loss 46.458\nEp 1 (Step 025720): Train loss 0.443, Val loss 46.384\nEp 1 (Step 025725): Train loss 0.435, Val loss 46.462\nEp 1 (Step 025730): Train loss 0.456, Val loss 45.879\nEp 1 (Step 025735): Train loss 0.408, Val loss 46.505\nEp 1 (Step 025740): Train loss 0.420, Val loss 46.615\nEp 1 (Step 025745): Train loss 0.417, Val loss 47.326\nEp 1 (Step 025750): Train loss 0.505, Val loss 46.797\nEp 1 (Step 025755): Train loss 0.425, Val loss 46.233\nEp 1 (Step 025760): Train loss 0.447, Val loss 46.614\nEp 1 (Step 025765): Train loss 0.373, Val loss 46.614\nEp 1 (Step 025770): Train loss 0.410, Val loss 47.250\nEp 1 (Step 025775): Train loss 0.420, Val loss 46.501\nEp 1 (Step 025780): Train loss 0.481, Val loss 46.580\nEp 1 (Step 025785): Train loss 0.409, Val loss 46.549\nEp 1 (Step 025790): Train loss 0.432, Val loss 46.333\nEp 1 (Step 025795): Train loss 0.451, Val loss 47.077\nEp 1 (Step 025800): Train loss 0.408, Val loss 47.026\nEp 1 (Step 025805): Train loss 0.449, Val loss 47.006\nEp 1 (Step 025810): Train loss 0.427, Val loss 46.858\nEp 1 (Step 025815): Train loss 0.429, Val loss 46.621\nEp 1 (Step 025820): Train loss 0.438, Val loss 47.026\nEp 1 (Step 025825): Train loss 0.425, Val loss 46.237\nEp 1 (Step 025830): Train loss 0.430, Val loss 46.168\nEp 1 (Step 025835): Train loss 0.448, Val loss 46.654\nEp 1 (Step 025840): Train loss 0.443, Val loss 46.800\nEp 1 (Step 025845): Train loss 0.431, Val loss 46.484\nEp 1 (Step 025850): Train loss 0.409, Val loss 45.986\nEp 1 (Step 025855): Train loss 0.454, Val loss 46.301\nEp 1 (Step 025860): Train loss 0.431, Val loss 46.065\nEp 1 (Step 025865): Train loss 0.412, Val loss 46.505\nEp 1 (Step 025870): Train loss 0.415, Val loss 46.602\nEp 1 (Step 025875): Train loss 0.428, Val loss 46.440\nEp 1 (Step 025880): Train loss 0.410, Val loss 47.013\nEp 1 (Step 025885): Train loss 0.440, Val loss 46.702\nEp 1 (Step 025890): Train loss 0.461, Val loss 47.147\nEp 1 (Step 025895): Train loss 0.481, Val loss 46.064\nEp 1 (Step 025900): Train loss 0.436, Val loss 47.038\nEp 1 (Step 025905): Train loss 0.457, Val loss 46.495\nEp 1 (Step 025910): Train loss 0.436, Val loss 46.254\nEp 1 (Step 025915): Train loss 0.411, Val loss 46.241\nEp 1 (Step 025920): Train loss 0.405, Val loss 46.326\nEp 1 (Step 025925): Train loss 0.478, Val loss 46.472\nEp 1 (Step 025930): Train loss 0.413, Val loss 45.975\nEp 1 (Step 025935): Train loss 0.400, Val loss 47.177\nEp 1 (Step 025940): Train loss 0.425, Val loss 46.727\nEp 1 (Step 025945): Train loss 0.416, Val loss 46.307\nEp 1 (Step 025950): Train loss 0.450, Val loss 47.075\nEp 1 (Step 025955): Train loss 0.439, Val loss 46.727\nEp 1 (Step 025960): Train loss 0.510, Val loss 46.408\nEp 1 (Step 025965): Train loss 0.424, Val loss 46.545\nEp 1 (Step 025970): Train loss 0.448, Val loss 46.690\nEp 1 (Step 025975): Train loss 0.425, Val loss 47.134\nEp 1 (Step 025980): Train loss 0.431, Val loss 46.224\nEp 1 (Step 025985): Train loss 0.442, Val loss 46.871\nEp 1 (Step 025990): Train loss 0.421, Val loss 46.885\nEp 1 (Step 025995): Train loss 0.415, Val loss 46.850\nEp 1 (Step 026000): Train loss 0.497, Val loss 46.907\nEp 1 (Step 026005): Train loss 0.490, Val loss 46.261\nEp 1 (Step 026010): Train loss 0.416, Val loss 46.152\nEp 1 (Step 026015): Train loss 0.419, Val loss 46.486\nEp 1 (Step 026020): Train loss 0.439, Val loss 45.758\nEp 1 (Step 026025): Train loss 0.424, Val loss 46.306\nEp 1 (Step 026030): Train loss 0.460, Val loss 46.650\nEp 1 (Step 026035): Train loss 0.432, Val loss 46.062\nEp 1 (Step 026040): Train loss 0.411, Val loss 46.444\nEp 1 (Step 026045): Train loss 0.423, Val loss 46.586\nEp 1 (Step 026050): Train loss 0.430, Val loss 47.102\nEp 1 (Step 026055): Train loss 0.420, Val loss 46.843\nEp 1 (Step 026060): Train loss 0.458, Val loss 46.689\nEp 1 (Step 026065): Train loss 0.424, Val loss 46.417\nEp 1 (Step 026070): Train loss 0.429, Val loss 47.081\nEp 1 (Step 026075): Train loss 0.437, Val loss 46.018\nEp 1 (Step 026080): Train loss 0.396, Val loss 46.829\nEp 1 (Step 026085): Train loss 0.405, Val loss 45.953\nEp 1 (Step 026090): Train loss 0.413, Val loss 46.292\nEp 1 (Step 026095): Train loss 0.492, Val loss 45.996\nEp 1 (Step 026100): Train loss 0.419, Val loss 46.307\nEp 1 (Step 026105): Train loss 0.416, Val loss 46.797\nEp 1 (Step 026110): Train loss 0.400, Val loss 46.778\nEp 1 (Step 026115): Train loss 0.408, Val loss 46.815\nEp 1 (Step 026120): Train loss 0.395, Val loss 46.412\nEp 1 (Step 026125): Train loss 0.446, Val loss 46.559\nEp 1 (Step 026130): Train loss 0.451, Val loss 46.685\nEp 1 (Step 026135): Train loss 0.406, Val loss 46.179\nEp 1 (Step 026140): Train loss 0.450, Val loss 46.369\nEp 1 (Step 026145): Train loss 0.412, Val loss 46.042\nEp 1 (Step 026150): Train loss 0.408, Val loss 47.346\nEp 1 (Step 026155): Train loss 0.451, Val loss 46.230\nEp 1 (Step 026160): Train loss 0.439, Val loss 46.522\nEp 1 (Step 026165): Train loss 0.401, Val loss 46.119\nEp 1 (Step 026170): Train loss 0.399, Val loss 46.422\nEp 1 (Step 026175): Train loss 0.403, Val loss 46.508\nEp 1 (Step 026180): Train loss 0.391, Val loss 46.554\nEp 1 (Step 026185): Train loss 0.439, Val loss 46.411\nEp 1 (Step 026190): Train loss 0.415, Val loss 46.928\nEp 1 (Step 026195): Train loss 0.419, Val loss 46.056\nEp 1 (Step 026200): Train loss 0.390, Val loss 47.341\nEp 1 (Step 026205): Train loss 0.432, Val loss 46.817\nEp 1 (Step 026210): Train loss 0.428, Val loss 45.914\nEp 1 (Step 026215): Train loss 0.467, Val loss 46.621\nEp 1 (Step 026220): Train loss 0.413, Val loss 46.754\nEp 1 (Step 026225): Train loss 0.428, Val loss 46.111\nEp 1 (Step 026230): Train loss 0.435, Val loss 46.268\nEp 1 (Step 026235): Train loss 0.407, Val loss 46.449\nEp 1 (Step 026240): Train loss 0.414, Val loss 46.367\nEp 1 (Step 026245): Train loss 0.397, Val loss 46.336\nEp 1 (Step 026250): Train loss 0.401, Val loss 47.342\nEp 1 (Step 026255): Train loss 0.428, Val loss 46.442\nEp 1 (Step 026260): Train loss 0.394, Val loss 46.836\nEp 1 (Step 026265): Train loss 0.388, Val loss 46.274\nEp 1 (Step 026270): Train loss 0.417, Val loss 46.112\nEp 1 (Step 026275): Train loss 0.419, Val loss 46.840\nEp 1 (Step 026280): Train loss 0.390, Val loss 46.498\nEp 1 (Step 026285): Train loss 0.391, Val loss 46.428\nEp 1 (Step 026290): Train loss 0.432, Val loss 46.360\nEp 1 (Step 026295): Train loss 0.427, Val loss 47.337\nEp 1 (Step 026300): Train loss 0.402, Val loss 46.872\nEp 1 (Step 026305): Train loss 0.410, Val loss 46.249\nEp 1 (Step 026310): Train loss 0.432, Val loss 46.470\nEp 1 (Step 026315): Train loss 0.404, Val loss 47.117\nEp 1 (Step 026320): Train loss 0.484, Val loss 46.352\nEp 1 (Step 026325): Train loss 0.434, Val loss 47.192\nEp 1 (Step 026330): Train loss 0.400, Val loss 46.349\nEp 1 (Step 026335): Train loss 0.431, Val loss 46.287\nEp 1 (Step 026340): Train loss 0.407, Val loss 47.028\nEp 1 (Step 026345): Train loss 0.461, Val loss 46.461\nEp 1 (Step 026350): Train loss 0.411, Val loss 47.195\nEp 1 (Step 026355): Train loss 0.487, Val loss 47.113\nEp 1 (Step 026360): Train loss 0.418, Val loss 46.396\nEp 1 (Step 026365): Train loss 0.396, Val loss 46.351\nEp 1 (Step 026370): Train loss 0.424, Val loss 47.231\nEp 1 (Step 026375): Train loss 0.405, Val loss 46.951\nEp 1 (Step 026380): Train loss 0.428, Val loss 46.524\nEp 1 (Step 026385): Train loss 0.441, Val loss 46.916\nEp 1 (Step 026390): Train loss 0.438, Val loss 46.134\nEp 1 (Step 026395): Train loss 0.392, Val loss 45.664\nEp 1 (Step 026400): Train loss 0.422, Val loss 46.768\nEp 1 (Step 026405): Train loss 0.405, Val loss 46.490\nEp 1 (Step 026410): Train loss 0.404, Val loss 46.836\nEp 1 (Step 026415): Train loss 0.420, Val loss 46.402\nEp 1 (Step 026420): Train loss 0.412, Val loss 46.546\nEp 1 (Step 026425): Train loss 0.400, Val loss 46.191\nEp 1 (Step 026430): Train loss 0.419, Val loss 45.684\nEp 1 (Step 026435): Train loss 0.393, Val loss 46.688\nEp 1 (Step 026440): Train loss 0.455, Val loss 46.609\nEp 1 (Step 026445): Train loss 0.479, Val loss 46.739\nEp 1 (Step 026450): Train loss 0.412, Val loss 47.182\nEp 1 (Step 026455): Train loss 0.425, Val loss 45.993\nEp 1 (Step 026460): Train loss 0.430, Val loss 46.629\nEp 1 (Step 026465): Train loss 0.422, Val loss 46.094\nEp 1 (Step 026470): Train loss 0.421, Val loss 46.556\nEp 1 (Step 026475): Train loss 0.404, Val loss 46.858\nEp 1 (Step 026480): Train loss 0.425, Val loss 47.042\nEp 1 (Step 026485): Train loss 0.451, Val loss 47.156\nEp 1 (Step 026490): Train loss 0.376, Val loss 46.106\nEp 1 (Step 026495): Train loss 0.383, Val loss 46.549\nEp 1 (Step 026500): Train loss 0.393, Val loss 46.914\nEp 1 (Step 026505): Train loss 0.434, Val loss 46.551\nEp 1 (Step 026510): Train loss 0.410, Val loss 46.784\nEp 1 (Step 026515): Train loss 0.420, Val loss 46.974\nEp 1 (Step 026520): Train loss 0.418, Val loss 46.898\nEp 1 (Step 026525): Train loss 0.453, Val loss 47.209\nEp 1 (Step 026530): Train loss 0.416, Val loss 46.113\nEp 1 (Step 026535): Train loss 0.443, Val loss 47.065\nEp 1 (Step 026540): Train loss 0.443, Val loss 46.739\nEp 1 (Step 026545): Train loss 0.427, Val loss 46.355\nEp 1 (Step 026550): Train loss 0.475, Val loss 46.885\nEp 1 (Step 026555): Train loss 0.401, Val loss 46.988\nEp 1 (Step 026560): Train loss 0.403, Val loss 46.109\nEp 1 (Step 026565): Train loss 0.447, Val loss 47.022\nEp 1 (Step 026570): Train loss 0.390, Val loss 45.994\nEp 1 (Step 026575): Train loss 0.444, Val loss 46.917\nEp 1 (Step 026580): Train loss 0.425, Val loss 45.977\nEp 1 (Step 026585): Train loss 0.406, Val loss 47.076\nEp 1 (Step 026590): Train loss 0.476, Val loss 46.005\nEp 1 (Step 026595): Train loss 0.419, Val loss 46.177\nEp 1 (Step 026600): Train loss 0.446, Val loss 46.423\nEp 1 (Step 026605): Train loss 0.417, Val loss 46.690\nEp 1 (Step 026610): Train loss 0.416, Val loss 47.261\nEp 1 (Step 026615): Train loss 0.415, Val loss 46.513\nEp 1 (Step 026620): Train loss 0.429, Val loss 46.713\nEp 1 (Step 026625): Train loss 0.417, Val loss 46.288\nEp 1 (Step 026630): Train loss 0.434, Val loss 46.521\nEp 1 (Step 026635): Train loss 0.475, Val loss 46.357\nEp 1 (Step 026640): Train loss 0.402, Val loss 46.561\nEp 1 (Step 026645): Train loss 0.413, Val loss 45.773\nEp 1 (Step 026650): Train loss 0.445, Val loss 46.813\nEp 1 (Step 026655): Train loss 0.454, Val loss 46.334\nEp 1 (Step 026660): Train loss 0.426, Val loss 46.512\nEp 1 (Step 026665): Train loss 0.413, Val loss 46.464\nEp 1 (Step 026670): Train loss 0.428, Val loss 46.530\nEp 1 (Step 026675): Train loss 0.473, Val loss 45.695\nEp 1 (Step 026680): Train loss 0.408, Val loss 46.407\nEp 1 (Step 026685): Train loss 0.444, Val loss 46.917\nEp 1 (Step 026690): Train loss 0.427, Val loss 46.150\nEp 1 (Step 026695): Train loss 0.400, Val loss 46.283\nEp 1 (Step 026700): Train loss 0.395, Val loss 46.486\nEp 1 (Step 026705): Train loss 0.403, Val loss 45.625\nEp 1 (Step 026710): Train loss 0.433, Val loss 46.240\nEp 1 (Step 026715): Train loss 0.446, Val loss 46.734\nEp 1 (Step 026720): Train loss 0.431, Val loss 46.605\nEp 1 (Step 026725): Train loss 0.412, Val loss 46.187\nEp 1 (Step 026730): Train loss 0.377, Val loss 46.188\nEp 1 (Step 026735): Train loss 0.429, Val loss 46.577\nEp 1 (Step 026740): Train loss 0.432, Val loss 46.106\nEp 1 (Step 026745): Train loss 0.464, Val loss 46.622\nEp 1 (Step 026750): Train loss 0.416, Val loss 46.196\nEp 1 (Step 026755): Train loss 0.407, Val loss 47.351\nEp 1 (Step 026760): Train loss 0.459, Val loss 46.642\nEp 1 (Step 026765): Train loss 0.437, Val loss 46.940\nEp 1 (Step 026770): Train loss 0.403, Val loss 46.455\nEp 1 (Step 026775): Train loss 0.429, Val loss 46.998\nEp 1 (Step 026780): Train loss 0.413, Val loss 46.209\nEp 1 (Step 026785): Train loss 0.404, Val loss 46.083\nEp 1 (Step 026790): Train loss 0.456, Val loss 46.265\nEp 1 (Step 026795): Train loss 0.417, Val loss 45.981\nEp 1 (Step 026800): Train loss 0.421, Val loss 46.532\nEp 1 (Step 026805): Train loss 0.457, Val loss 47.320\nEp 1 (Step 026810): Train loss 0.403, Val loss 46.023\nEp 1 (Step 026815): Train loss 0.431, Val loss 46.478\nEp 1 (Step 026820): Train loss 0.440, Val loss 47.211\nEp 1 (Step 026825): Train loss 0.429, Val loss 46.435\nEp 1 (Step 026830): Train loss 0.424, Val loss 46.777\nEp 1 (Step 026835): Train loss 0.426, Val loss 46.885\nEp 1 (Step 026840): Train loss 0.426, Val loss 46.922\nEp 1 (Step 026845): Train loss 0.415, Val loss 46.175\nEp 1 (Step 026850): Train loss 0.463, Val loss 46.637\nEp 1 (Step 026855): Train loss 0.428, Val loss 46.106\nEp 1 (Step 026860): Train loss 0.403, Val loss 45.418\nEp 1 (Step 026865): Train loss 0.459, Val loss 46.863\nEp 1 (Step 026870): Train loss 0.456, Val loss 46.433\nEp 1 (Step 026875): Train loss 0.473, Val loss 45.616\nEp 1 (Step 026880): Train loss 0.433, Val loss 46.316\nEp 1 (Step 026885): Train loss 0.421, Val loss 46.272\nEp 1 (Step 026890): Train loss 0.436, Val loss 46.791\nEp 1 (Step 026895): Train loss 0.473, Val loss 46.371\nEp 1 (Step 026900): Train loss 0.432, Val loss 46.665\nEp 1 (Step 026905): Train loss 0.429, Val loss 46.302\nEp 1 (Step 026910): Train loss 0.467, Val loss 46.053\nEp 1 (Step 026915): Train loss 0.478, Val loss 46.978\nEp 1 (Step 026920): Train loss 0.433, Val loss 46.437\nEp 1 (Step 026925): Train loss 0.438, Val loss 46.577\nEp 1 (Step 026930): Train loss 0.397, Val loss 46.965\nEp 1 (Step 026935): Train loss 0.427, Val loss 47.873\nEp 1 (Step 026940): Train loss 0.444, Val loss 47.629\nEp 1 (Step 026945): Train loss 0.435, Val loss 46.875\nEp 1 (Step 026950): Train loss 0.413, Val loss 46.704\nEp 1 (Step 026955): Train loss 0.455, Val loss 46.830\nEp 1 (Step 026960): Train loss 0.536, Val loss 46.912\nEp 1 (Step 026965): Train loss 0.461, Val loss 46.187\nEp 1 (Step 026970): Train loss 0.452, Val loss 45.936\nEp 1 (Step 026975): Train loss 0.433, Val loss 47.023\nEp 1 (Step 026980): Train loss 0.414, Val loss 46.754\nEp 1 (Step 026985): Train loss 0.397, Val loss 46.868\nEp 1 (Step 026990): Train loss 0.462, Val loss 46.400\nEp 1 (Step 026995): Train loss 0.413, Val loss 46.488\nEp 1 (Step 027000): Train loss 0.425, Val loss 46.624\nEp 1 (Step 027005): Train loss 0.406, Val loss 46.233\nEp 1 (Step 027010): Train loss 0.388, Val loss 46.406\nEp 1 (Step 027015): Train loss 0.407, Val loss 46.220\nEp 1 (Step 027020): Train loss 0.452, Val loss 47.141\nEp 1 (Step 027025): Train loss 0.471, Val loss 46.020\nEp 1 (Step 027030): Train loss 0.438, Val loss 46.807\nEp 1 (Step 027035): Train loss 0.436, Val loss 45.805\nEp 1 (Step 027040): Train loss 0.428, Val loss 46.250\nEp 1 (Step 027045): Train loss 0.432, Val loss 46.476\nEp 1 (Step 027050): Train loss 0.418, Val loss 46.355\nEp 1 (Step 027055): Train loss 0.439, Val loss 46.531\nEp 1 (Step 027060): Train loss 0.425, Val loss 46.719\nEp 1 (Step 027065): Train loss 0.438, Val loss 46.236\nEp 1 (Step 027070): Train loss 0.431, Val loss 46.645\nEp 1 (Step 027075): Train loss 0.436, Val loss 45.888\nEp 1 (Step 027080): Train loss 0.402, Val loss 47.241\nEp 1 (Step 027085): Train loss 0.450, Val loss 45.970\nEp 1 (Step 027090): Train loss 0.442, Val loss 47.796\nEp 1 (Step 027095): Train loss 0.426, Val loss 45.920\nEp 1 (Step 027100): Train loss 0.431, Val loss 46.430\nEp 1 (Step 027105): Train loss 0.431, Val loss 46.485\nEp 1 (Step 027110): Train loss 0.438, Val loss 45.985\nEp 1 (Step 027115): Train loss 0.416, Val loss 46.885\nEp 1 (Step 027120): Train loss 0.418, Val loss 46.536\nEp 1 (Step 027125): Train loss 0.395, Val loss 45.999\nEp 1 (Step 027130): Train loss 0.422, Val loss 46.015\nEp 1 (Step 027135): Train loss 0.417, Val loss 46.619\nEp 1 (Step 027140): Train loss 0.429, Val loss 46.192\nEp 1 (Step 027145): Train loss 0.388, Val loss 46.596\nEp 1 (Step 027150): Train loss 0.388, Val loss 46.136\nEp 1 (Step 027155): Train loss 0.406, Val loss 47.242\nEp 1 (Step 027160): Train loss 0.392, Val loss 46.456\nEp 1 (Step 027165): Train loss 0.423, Val loss 46.855\nEp 1 (Step 027170): Train loss 0.427, Val loss 45.797\nEp 1 (Step 027175): Train loss 0.405, Val loss 46.596\nEp 1 (Step 027180): Train loss 0.421, Val loss 46.585\nEp 1 (Step 027185): Train loss 0.417, Val loss 46.079\nEp 1 (Step 027190): Train loss 0.463, Val loss 47.143\nEp 1 (Step 027195): Train loss 0.431, Val loss 46.493\nEp 1 (Step 027200): Train loss 0.430, Val loss 46.454\nEp 1 (Step 027205): Train loss 0.439, Val loss 46.642\nEp 1 (Step 027210): Train loss 0.404, Val loss 46.032\nEp 1 (Step 027215): Train loss 0.420, Val loss 46.948\nEp 1 (Step 027220): Train loss 0.420, Val loss 46.306\nEp 1 (Step 027225): Train loss 0.447, Val loss 46.751\nEp 1 (Step 027230): Train loss 0.401, Val loss 46.321\nEp 1 (Step 027235): Train loss 0.416, Val loss 46.604\nEp 1 (Step 027240): Train loss 0.451, Val loss 46.266\nEp 1 (Step 027245): Train loss 0.426, Val loss 46.811\nEp 1 (Step 027250): Train loss 0.422, Val loss 46.780\nEp 1 (Step 027255): Train loss 0.371, Val loss 46.175\nEp 1 (Step 027260): Train loss 0.430, Val loss 46.291\nEp 1 (Step 027265): Train loss 0.426, Val loss 46.170\nEp 1 (Step 027270): Train loss 0.401, Val loss 45.979\nEp 1 (Step 027275): Train loss 0.417, Val loss 46.454\nEp 1 (Step 027280): Train loss 0.372, Val loss 46.697\nEp 1 (Step 027285): Train loss 0.432, Val loss 46.625\nEp 1 (Step 027290): Train loss 0.386, Val loss 46.247\nEp 1 (Step 027295): Train loss 0.412, Val loss 46.128\nEp 1 (Step 027300): Train loss 0.431, Val loss 46.601\nEp 1 (Step 027305): Train loss 0.372, Val loss 46.967\nEp 1 (Step 027310): Train loss 0.408, Val loss 46.894\nEp 1 (Step 027315): Train loss 0.396, Val loss 45.882\nEp 1 (Step 027320): Train loss 0.412, Val loss 46.524\nEp 1 (Step 027325): Train loss 0.409, Val loss 46.736\nEp 1 (Step 027330): Train loss 0.386, Val loss 46.082\nEp 1 (Step 027335): Train loss 0.388, Val loss 46.621\nEp 1 (Step 027340): Train loss 0.412, Val loss 46.979\nEp 1 (Step 027345): Train loss 0.373, Val loss 46.801\nEp 1 (Step 027350): Train loss 0.373, Val loss 46.812\nEp 1 (Step 027355): Train loss 0.421, Val loss 46.524\nEp 1 (Step 027360): Train loss 0.404, Val loss 46.015\nEp 1 (Step 027365): Train loss 0.438, Val loss 46.948\nEp 1 (Step 027370): Train loss 0.419, Val loss 46.887\nEp 1 (Step 027375): Train loss 0.404, Val loss 46.749\nEp 1 (Step 027380): Train loss 0.396, Val loss 46.613\nEp 1 (Step 027385): Train loss 0.414, Val loss 46.532\nEp 1 (Step 027390): Train loss 0.386, Val loss 46.604\nEp 1 (Step 027395): Train loss 0.379, Val loss 47.056\nEp 1 (Step 027400): Train loss 0.392, Val loss 46.616\nEp 1 (Step 027405): Train loss 0.426, Val loss 45.440\nEp 1 (Step 027410): Train loss 0.436, Val loss 46.698\nEp 1 (Step 027415): Train loss 0.405, Val loss 46.683\nEp 1 (Step 027420): Train loss 0.406, Val loss 46.413\nEp 1 (Step 027425): Train loss 0.399, Val loss 46.623\nEp 1 (Step 027430): Train loss 0.405, Val loss 46.412\nEp 1 (Step 027435): Train loss 0.436, Val loss 45.503\nEp 1 (Step 027440): Train loss 0.368, Val loss 46.327\nEp 1 (Step 027445): Train loss 0.404, Val loss 47.310\nEp 1 (Step 027450): Train loss 0.413, Val loss 46.301\nEp 1 (Step 027455): Train loss 0.383, Val loss 46.467\nEp 1 (Step 027460): Train loss 0.411, Val loss 46.872\nEp 1 (Step 027465): Train loss 0.406, Val loss 46.977\nEp 1 (Step 027470): Train loss 0.398, Val loss 46.670\nEp 1 (Step 027475): Train loss 0.373, Val loss 46.629\nEp 1 (Step 027480): Train loss 0.393, Val loss 47.446\nEp 1 (Step 027485): Train loss 0.368, Val loss 46.269\nEp 1 (Step 027490): Train loss 0.422, Val loss 46.632\nEp 1 (Step 027495): Train loss 0.419, Val loss 46.603\nEp 1 (Step 027500): Train loss 0.395, Val loss 46.808\nEp 1 (Step 027505): Train loss 0.425, Val loss 46.542\nEp 1 (Step 027510): Train loss 0.411, Val loss 46.044\nEp 1 (Step 027515): Train loss 0.437, Val loss 46.864\nEp 1 (Step 027520): Train loss 0.397, Val loss 46.342\nEp 1 (Step 027525): Train loss 0.415, Val loss 47.313\nEp 1 (Step 027530): Train loss 0.392, Val loss 46.263\nEp 1 (Step 027535): Train loss 0.429, Val loss 46.901\nEp 1 (Step 027540): Train loss 0.420, Val loss 46.637\nEp 1 (Step 027545): Train loss 0.414, Val loss 46.398\nEp 1 (Step 027550): Train loss 0.410, Val loss 46.375\nEp 1 (Step 027555): Train loss 0.398, Val loss 46.871\nEp 1 (Step 027560): Train loss 0.422, Val loss 46.470\nEp 1 (Step 027565): Train loss 0.398, Val loss 46.033\nEp 1 (Step 027570): Train loss 0.388, Val loss 47.355\nEp 1 (Step 027575): Train loss 0.391, Val loss 46.835\nEp 1 (Step 027580): Train loss 0.425, Val loss 46.533\nEp 1 (Step 027585): Train loss 0.449, Val loss 46.311\nEp 1 (Step 027590): Train loss 0.398, Val loss 46.694\nEp 1 (Step 027595): Train loss 0.403, Val loss 46.579\nEp 1 (Step 027600): Train loss 0.408, Val loss 46.477\nEp 1 (Step 027605): Train loss 0.411, Val loss 46.804\nEp 1 (Step 027610): Train loss 0.416, Val loss 45.914\nEp 1 (Step 027615): Train loss 0.390, Val loss 45.811\nEp 1 (Step 027620): Train loss 0.395, Val loss 45.426\nEp 1 (Step 027625): Train loss 0.394, Val loss 45.681\nEp 1 (Step 027630): Train loss 0.389, Val loss 47.052\nEp 1 (Step 027635): Train loss 0.380, Val loss 46.731\nEp 1 (Step 027640): Train loss 0.407, Val loss 46.682\nEp 1 (Step 027645): Train loss 0.408, Val loss 46.858\nEp 1 (Step 027650): Train loss 0.375, Val loss 46.749\nEp 1 (Step 027655): Train loss 0.389, Val loss 45.801\nEp 1 (Step 027660): Train loss 0.378, Val loss 46.301\nEp 1 (Step 027665): Train loss 0.392, Val loss 46.293\nEp 1 (Step 027670): Train loss 0.368, Val loss 46.243\nEp 1 (Step 027675): Train loss 0.408, Val loss 47.020\nEp 1 (Step 027680): Train loss 0.425, Val loss 47.027\nEp 1 (Step 027685): Train loss 0.426, Val loss 47.077\nEp 1 (Step 027690): Train loss 0.397, Val loss 46.977\nEp 1 (Step 027695): Train loss 0.400, Val loss 46.350\nEp 1 (Step 027700): Train loss 0.444, Val loss 46.749\nEp 1 (Step 027705): Train loss 0.411, Val loss 46.079\nEp 1 (Step 027710): Train loss 0.391, Val loss 46.378\nEp 1 (Step 027715): Train loss 0.408, Val loss 45.871\nEp 1 (Step 027720): Train loss 0.366, Val loss 46.504\nEp 1 (Step 027725): Train loss 0.404, Val loss 46.481\nEp 1 (Step 027730): Train loss 0.401, Val loss 45.987\nEp 1 (Step 027735): Train loss 0.415, Val loss 46.686\nEp 1 (Step 027740): Train loss 0.386, Val loss 47.040\nEp 1 (Step 027745): Train loss 0.399, Val loss 46.795\nEp 1 (Step 027750): Train loss 0.474, Val loss 46.776\nEp 1 (Step 027755): Train loss 0.384, Val loss 45.839\nEp 1 (Step 027760): Train loss 0.416, Val loss 46.317\nEp 1 (Step 027765): Train loss 0.389, Val loss 46.511\nEp 1 (Step 027770): Train loss 0.407, Val loss 46.622\nEp 1 (Step 027775): Train loss 0.398, Val loss 46.584\nEp 1 (Step 027780): Train loss 0.414, Val loss 46.385\nEp 1 (Step 027785): Train loss 0.399, Val loss 46.283\nEp 1 (Step 027790): Train loss 0.426, Val loss 46.634\nEp 1 (Step 027795): Train loss 0.381, Val loss 46.824\nEp 1 (Step 027800): Train loss 0.386, Val loss 46.741\nEp 1 (Step 027805): Train loss 0.392, Val loss 45.894\nEp 1 (Step 027810): Train loss 0.446, Val loss 46.448\nEp 1 (Step 027815): Train loss 0.449, Val loss 47.035\nEp 1 (Step 027820): Train loss 0.400, Val loss 46.475\nEp 1 (Step 027825): Train loss 0.370, Val loss 46.793\nEp 1 (Step 027830): Train loss 0.398, Val loss 46.083\nEp 1 (Step 027835): Train loss 0.425, Val loss 46.434\nEp 1 (Step 027840): Train loss 0.392, Val loss 46.674\nEp 1 (Step 027845): Train loss 0.414, Val loss 46.753\nEp 1 (Step 027850): Train loss 0.403, Val loss 46.455\nEp 1 (Step 027855): Train loss 0.411, Val loss 46.438\nEp 1 (Step 027860): Train loss 0.408, Val loss 47.081\nEp 1 (Step 027865): Train loss 0.441, Val loss 46.805\nEp 1 (Step 027870): Train loss 0.380, Val loss 45.873\nEp 1 (Step 027875): Train loss 0.409, Val loss 46.453\nEp 1 (Step 027880): Train loss 0.402, Val loss 46.552\nEp 1 (Step 027885): Train loss 0.417, Val loss 46.633\nEp 1 (Step 027890): Train loss 0.395, Val loss 46.092\nEp 1 (Step 027895): Train loss 0.410, Val loss 46.694\nEp 1 (Step 027900): Train loss 0.385, Val loss 46.782\nEp 1 (Step 027905): Train loss 0.396, Val loss 46.232\nEp 1 (Step 027910): Train loss 0.402, Val loss 47.346\nEp 1 (Step 027915): Train loss 0.390, Val loss 47.191\nEp 1 (Step 027920): Train loss 0.397, Val loss 46.588\nEp 1 (Step 027925): Train loss 0.408, Val loss 46.443\nEp 1 (Step 027930): Train loss 0.411, Val loss 46.279\nEp 1 (Step 027935): Train loss 0.349, Val loss 46.235\nEp 1 (Step 027940): Train loss 0.458, Val loss 46.464\nEp 1 (Step 027945): Train loss 0.444, Val loss 46.153\nEp 1 (Step 027950): Train loss 0.437, Val loss 46.923\nEp 1 (Step 027955): Train loss 0.404, Val loss 46.719\nEp 1 (Step 027960): Train loss 0.415, Val loss 46.782\nEp 1 (Step 027965): Train loss 0.422, Val loss 46.234\nEp 1 (Step 027970): Train loss 0.432, Val loss 46.313\nEp 1 (Step 027975): Train loss 0.382, Val loss 46.578\nEp 1 (Step 027980): Train loss 0.426, Val loss 46.891\nEp 1 (Step 027985): Train loss 0.407, Val loss 46.690\nEp 1 (Step 027990): Train loss 0.451, Val loss 47.205\nEp 1 (Step 027995): Train loss 0.404, Val loss 45.934\nEp 1 (Step 028000): Train loss 0.420, Val loss 46.304\nEp 1 (Step 028005): Train loss 0.390, Val loss 46.344\nEp 1 (Step 028010): Train loss 0.448, Val loss 45.817\nEp 1 (Step 028015): Train loss 0.421, Val loss 47.088\nEp 1 (Step 028020): Train loss 0.452, Val loss 45.757\nEp 1 (Step 028025): Train loss 0.369, Val loss 46.703\nEp 1 (Step 028030): Train loss 0.416, Val loss 46.246\nEp 1 (Step 028035): Train loss 0.391, Val loss 45.539\nEp 1 (Step 028040): Train loss 0.406, Val loss 46.463\nEp 1 (Step 028045): Train loss 0.431, Val loss 46.470\nEp 1 (Step 028050): Train loss 0.410, Val loss 46.478\nEp 1 (Step 028055): Train loss 0.426, Val loss 46.357\nEp 1 (Step 028060): Train loss 0.407, Val loss 46.443\nEp 1 (Step 028065): Train loss 0.407, Val loss 46.626\nEp 1 (Step 028070): Train loss 0.420, Val loss 46.521\nEp 1 (Step 028075): Train loss 0.411, Val loss 46.257\nEp 1 (Step 028080): Train loss 0.410, Val loss 46.748\nEp 1 (Step 028085): Train loss 0.476, Val loss 45.697\nEp 1 (Step 028090): Train loss 0.405, Val loss 46.405\nEp 1 (Step 028095): Train loss 0.428, Val loss 45.942\nEp 1 (Step 028100): Train loss 0.424, Val loss 47.324\nEp 1 (Step 028105): Train loss 0.388, Val loss 46.690\nEp 1 (Step 028110): Train loss 0.408, Val loss 47.107\nEp 1 (Step 028115): Train loss 0.411, Val loss 46.230\nEp 1 (Step 028120): Train loss 0.429, Val loss 46.595\nEp 1 (Step 028125): Train loss 0.376, Val loss 45.974\nEp 1 (Step 028130): Train loss 0.390, Val loss 46.225\nEp 1 (Step 028135): Train loss 0.387, Val loss 45.962\nEp 1 (Step 028140): Train loss 0.423, Val loss 45.715\nEp 1 (Step 028145): Train loss 0.385, Val loss 46.196\nEp 1 (Step 028150): Train loss 0.428, Val loss 46.175\nEp 1 (Step 028155): Train loss 0.367, Val loss 46.525\nEp 1 (Step 028160): Train loss 0.426, Val loss 46.685\nEp 1 (Step 028165): Train loss 0.358, Val loss 46.399\nEp 1 (Step 028170): Train loss 0.432, Val loss 46.185\nEp 1 (Step 028175): Train loss 0.430, Val loss 46.146\nEp 1 (Step 028180): Train loss 0.376, Val loss 45.867\nEp 1 (Step 028185): Train loss 0.399, Val loss 46.709\nEp 1 (Step 028190): Train loss 0.427, Val loss 46.678\nEp 1 (Step 028195): Train loss 0.405, Val loss 45.945\nEp 1 (Step 028200): Train loss 0.416, Val loss 46.932\nEp 1 (Step 028205): Train loss 0.412, Val loss 46.757\nEp 1 (Step 028210): Train loss 0.374, Val loss 45.887\nEp 1 (Step 028215): Train loss 0.412, Val loss 46.582\nEp 1 (Step 028220): Train loss 0.421, Val loss 46.290\nEp 1 (Step 028225): Train loss 0.423, Val loss 46.872\nEp 1 (Step 028230): Train loss 0.405, Val loss 46.683\nEp 1 (Step 028235): Train loss 0.444, Val loss 46.386\nEp 1 (Step 028240): Train loss 0.392, Val loss 46.078\nEp 1 (Step 028245): Train loss 0.376, Val loss 45.971\nEp 1 (Step 028250): Train loss 0.394, Val loss 47.535\nEp 1 (Step 028255): Train loss 0.405, Val loss 46.462\nEp 1 (Step 028260): Train loss 0.399, Val loss 46.065\nEp 1 (Step 028265): Train loss 0.421, Val loss 46.752\nEp 1 (Step 028270): Train loss 0.380, Val loss 45.709\nEp 1 (Step 028275): Train loss 0.416, Val loss 45.439\nEp 1 (Step 028280): Train loss 0.404, Val loss 46.829\nEp 1 (Step 028285): Train loss 0.421, Val loss 46.283\nEp 1 (Step 028290): Train loss 0.430, Val loss 45.844\nEp 1 (Step 028295): Train loss 0.399, Val loss 45.982\nEp 1 (Step 028300): Train loss 0.410, Val loss 46.072\nEp 1 (Step 028305): Train loss 0.428, Val loss 46.317\nEp 1 (Step 028310): Train loss 0.452, Val loss 46.206\nEp 1 (Step 028315): Train loss 0.433, Val loss 47.116\nEp 1 (Step 028320): Train loss 0.423, Val loss 46.651\nEp 1 (Step 028325): Train loss 0.414, Val loss 46.137\nEp 1 (Step 028330): Train loss 0.411, Val loss 46.309\nEp 1 (Step 028335): Train loss 0.379, Val loss 46.333\nEp 1 (Step 028340): Train loss 0.381, Val loss 45.849\nEp 1 (Step 028345): Train loss 0.401, Val loss 46.390\nEp 1 (Step 028350): Train loss 0.412, Val loss 46.328\nEp 1 (Step 028355): Train loss 0.435, Val loss 46.984\nEp 1 (Step 028360): Train loss 0.451, Val loss 46.276\nEp 1 (Step 028365): Train loss 0.420, Val loss 46.766\nEp 1 (Step 028370): Train loss 0.423, Val loss 46.557\nEp 1 (Step 028375): Train loss 0.433, Val loss 45.549\nEp 1 (Step 028380): Train loss 0.450, Val loss 47.421\nEp 1 (Step 028385): Train loss 0.400, Val loss 46.939\nEp 1 (Step 028390): Train loss 0.401, Val loss 46.730\nEp 1 (Step 028395): Train loss 0.418, Val loss 46.571\nEp 1 (Step 028400): Train loss 0.408, Val loss 46.912\nEp 1 (Step 028405): Train loss 0.414, Val loss 46.490\nEp 1 (Step 028410): Train loss 0.388, Val loss 45.994\nEp 1 (Step 028415): Train loss 0.447, Val loss 46.472\nEp 1 (Step 028420): Train loss 0.412, Val loss 46.323\nEp 1 (Step 028425): Train loss 0.379, Val loss 47.276\nEp 1 (Step 028430): Train loss 0.409, Val loss 46.384\nEp 1 (Step 028435): Train loss 0.411, Val loss 46.052\nEp 1 (Step 028440): Train loss 0.425, Val loss 47.136\nEp 1 (Step 028445): Train loss 0.368, Val loss 46.651\nEp 1 (Step 028450): Train loss 0.405, Val loss 46.470\nEp 1 (Step 028455): Train loss 0.389, Val loss 45.619\nEp 1 (Step 028460): Train loss 0.388, Val loss 45.741\nEp 1 (Step 028465): Train loss 0.395, Val loss 46.045\nEp 1 (Step 028470): Train loss 0.408, Val loss 46.104\nEp 1 (Step 028475): Train loss 0.397, Val loss 46.490\nEp 1 (Step 028480): Train loss 0.409, Val loss 46.007\nEp 1 (Step 028485): Train loss 0.406, Val loss 47.158\nEp 1 (Step 028490): Train loss 0.438, Val loss 46.688\nEp 1 (Step 028495): Train loss 0.399, Val loss 47.416\nEp 1 (Step 028500): Train loss 0.390, Val loss 46.327\nEp 1 (Step 028505): Train loss 0.398, Val loss 46.180\nEp 1 (Step 028510): Train loss 0.450, Val loss 46.095\nEp 1 (Step 028515): Train loss 0.392, Val loss 46.188\nEp 1 (Step 028520): Train loss 0.442, Val loss 46.693\nEp 1 (Step 028525): Train loss 0.410, Val loss 46.835\nEp 1 (Step 028530): Train loss 0.442, Val loss 45.788\nEp 1 (Step 028535): Train loss 0.473, Val loss 46.886\nEp 1 (Step 028540): Train loss 0.416, Val loss 46.108\nEp 1 (Step 028545): Train loss 0.456, Val loss 46.629\nEp 1 (Step 028550): Train loss 0.404, Val loss 46.385\nEp 1 (Step 028555): Train loss 0.395, Val loss 46.531\nEp 1 (Step 028560): Train loss 0.393, Val loss 46.812\nEp 1 (Step 028565): Train loss 0.428, Val loss 46.357\nEp 1 (Step 028570): Train loss 0.403, Val loss 47.118\nEp 1 (Step 028575): Train loss 0.411, Val loss 46.624\nEp 1 (Step 028580): Train loss 0.437, Val loss 46.713\nEp 1 (Step 028585): Train loss 0.390, Val loss 47.487\nEp 1 (Step 028590): Train loss 0.426, Val loss 46.156\nEp 1 (Step 028595): Train loss 0.416, Val loss 45.876\nEp 1 (Step 028600): Train loss 0.382, Val loss 45.999\nEp 1 (Step 028605): Train loss 0.438, Val loss 46.111\nEp 1 (Step 028610): Train loss 0.436, Val loss 47.371\nEp 1 (Step 028615): Train loss 0.395, Val loss 46.119\nEp 1 (Step 028620): Train loss 0.404, Val loss 45.899\nEp 1 (Step 028625): Train loss 0.430, Val loss 46.290\nEp 1 (Step 028630): Train loss 0.414, Val loss 46.113\nEp 1 (Step 028635): Train loss 0.439, Val loss 46.630\nEp 1 (Step 028640): Train loss 0.415, Val loss 46.116\nEp 1 (Step 028645): Train loss 0.436, Val loss 46.688\nEp 1 (Step 028650): Train loss 0.346, Val loss 45.919\nEp 1 (Step 028655): Train loss 0.373, Val loss 46.051\nEp 1 (Step 028660): Train loss 0.381, Val loss 46.219\nEp 1 (Step 028665): Train loss 0.380, Val loss 46.358\nEp 1 (Step 028670): Train loss 0.434, Val loss 46.772\nEp 1 (Step 028675): Train loss 0.407, Val loss 46.500\nEp 1 (Step 028680): Train loss 0.403, Val loss 46.368\nEp 1 (Step 028685): Train loss 0.420, Val loss 47.224\nEp 1 (Step 028690): Train loss 0.374, Val loss 46.274\nEp 1 (Step 028695): Train loss 0.441, Val loss 46.432\nEp 1 (Step 028700): Train loss 0.399, Val loss 47.054\nEp 1 (Step 028705): Train loss 0.432, Val loss 46.541\nEp 1 (Step 028710): Train loss 0.391, Val loss 46.997\nEp 1 (Step 028715): Train loss 0.388, Val loss 47.207\nEp 1 (Step 028720): Train loss 0.390, Val loss 46.223\nEp 1 (Step 028725): Train loss 0.388, Val loss 46.720\nEp 1 (Step 028730): Train loss 0.423, Val loss 45.995\nEp 1 (Step 028735): Train loss 0.389, Val loss 46.665\nEp 1 (Step 028740): Train loss 0.373, Val loss 46.686\nEp 1 (Step 028745): Train loss 0.379, Val loss 47.135\nEp 1 (Step 028750): Train loss 0.395, Val loss 45.951\nEp 1 (Step 028755): Train loss 0.395, Val loss 46.203\nEp 1 (Step 028760): Train loss 0.418, Val loss 46.570\nEp 1 (Step 028765): Train loss 0.448, Val loss 46.558\nEp 1 (Step 028770): Train loss 0.390, Val loss 46.521\nEp 1 (Step 028775): Train loss 0.401, Val loss 46.246\nEp 1 (Step 028780): Train loss 0.432, Val loss 46.947\nEp 1 (Step 028785): Train loss 0.364, Val loss 46.219\nEp 1 (Step 028790): Train loss 0.389, Val loss 46.414\n\nEXEMPLO DE GERA√á√ÉO:\nBom dia! √© preciso acabar o seminario, a entornei, mas disposto a fazel-a cair pela guela abaixo, caso o sabor lhe repugnasse, ou a temperatura, porque o caf√© estava frio... Mas n√£o sei que senti\n\nGR√ÅFICO DE PERDA DURANTE O TREINO:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 500x300 with 2 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"\nDESEMPENHO:\nTempo total: 18131.60 s\nTokens/s: 3969.73\nMem√≥ria m√°xima: 11157.89 MB\n\nTempo total de treino: 18095.66 s (301.59 min)\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"generate_and_print_sample(model, tokenizer, device=device, start_context=\"Bom dia!\")","metadata":{"trusted":true,"id":"r4C_QNH15t00","execution":{"iopub.status.busy":"2025-09-03T07:14:50.344357Z","iopub.execute_input":"2025-09-03T07:14:50.344975Z","iopub.status.idle":"2025-09-03T07:14:50.750849Z","shell.execute_reply.started":"2025-09-03T07:14:50.344950Z","shell.execute_reply":"2025-09-03T07:14:50.749982Z"}},"outputs":[{"name":"stdout","text":"Bom dia! √© preciso acabar o seminario, a entornei, mas disposto a fazel-a cair pela guela abaixo, caso o sabor lhe repugnasse, ou a temperatura, porque o caf√© estava frio... Mas n√£o sei que senti\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"generate_and_print_sample(model, tokenizer, device=device, start_context=\"Deus\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T07:14:50.751942Z","iopub.execute_input":"2025-09-03T07:14:50.752352Z","iopub.status.idle":"2025-09-03T07:15:02.932887Z","shell.execute_reply.started":"2025-09-03T07:14:50.752307Z","shell.execute_reply":"2025-09-03T07:15:02.931906Z"}},"outputs":[{"name":"stdout","text":"Deus quisesse. Era a nossa sorte amar-nos; se assim n√£o fora, como explicariamos a valsa e o resto? Virgilia pensava a mesma cousa. Um dia, depois de me confessar que tinha momentos\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"generate_and_print_sample(model, tokenizer, device=device, start_context=\"Crie um poema\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T07:15:02.933977Z","iopub.execute_input":"2025-09-03T07:15:02.934278Z","iopub.status.idle":"2025-09-03T07:15:03.723660Z","shell.execute_reply.started":"2025-09-03T07:15:02.934252Z","shell.execute_reply":"2025-09-03T07:15:03.722837Z"}},"outputs":[{"name":"stdout","text":"Crie um poema e outro poema.  Chegai, folgai, cantai. √â esta, √© esta De Lindoya, que a voz suave e forte Do vate celebrou, a alegre festa.  Al√©m do amavel, gracioso porte\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"generate_and_print_sample(model, tokenizer, device=device, start_context=\"O brasileiro √© um\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T07:15:03.724633Z","iopub.execute_input":"2025-09-03T07:15:03.724851Z","iopub.status.idle":"2025-09-03T07:15:04.628544Z","shell.execute_reply.started":"2025-09-03T07:15:03.724827Z","shell.execute_reply":"2025-09-03T07:15:04.627150Z"}},"outputs":[{"name":"stdout","text":"O brasileiro √© um grande e trajada ao bizarro, como diria o padre Bernardes,--esta id√©a come√ßou uma vertigem de cabriolas e eu deixei-me estar com os olhos nella, a achar-lhe gra√ßa. E n√£o\n","output_type":"stream"}],"execution_count":43},{"cell_type":"markdown","source":"# **Resultados ‚ú®**","metadata":{"id":"d45s6PQOQxVT"}},{"cell_type":"markdown","source":"## Perplexidade","metadata":{"id":"fGB69jVAi7m4"}},{"cell_type":"code","source":"def compute_perplexity(model, data_loader, device):\n    model.eval()\n    total_loss = 0\n    n_batches = 0\n\n    with torch.no_grad():\n        for batch_idx, (x, y) in enumerate(data_loader):\n            x, y = x.to(device), y.to(device)\n            loss = calc_loss_batch_by_cross_entropy(model, x, y, device)\n\n            print(f\"Batch {batch_idx+1}/{len(data_loader)}, Loss: {loss.item():.4f}\")\n\n            total_loss += loss.item()\n            n_batches += 1\n\n            if (batch_idx + 1) % 10 == 0:\n                avg_loss_partial = total_loss / n_batches\n                print(f\"  M√©dia parcial at√© aqui: {avg_loss_partial:.4f}\")\n\n    avg_loss = total_loss / n_batches\n    print(f\"Loss m√©dia final no dataset de teste: {avg_loss:.4f}\")\n\n    perplexity = torch.exp(torch.tensor(avg_loss))\n    print(f\"Perplexidade calculada: {perplexity.item():.2f}\")\n\n    return perplexity.item(), avg_loss\n\nperplexity_test, avg_loss = compute_perplexity(model, test_loader, device)","metadata":{"id":"A1k_UPSkfwrQ","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T07:56:27.580055Z","iopub.execute_input":"2025-09-03T07:56:27.580625Z","iopub.status.idle":"2025-09-03T08:02:55.306514Z","shell.execute_reply.started":"2025-09-03T07:56:27.580597Z","shell.execute_reply":"2025-09-03T08:02:55.305555Z"}},"outputs":[{"name":"stdout","text":"Batch 1/3471, Loss: 8.6219\nBatch 2/3471, Loss: 8.7489\nBatch 3/3471, Loss: 8.5763\nBatch 4/3471, Loss: 8.4102\nBatch 5/3471, Loss: 9.4475\nBatch 6/3471, Loss: 7.9522\nBatch 7/3471, Loss: 6.6053\nBatch 8/3471, Loss: 7.5220\nBatch 9/3471, Loss: 9.9397\nBatch 10/3471, Loss: 9.6543\n  M√©dia parcial at√© aqui: 8.5478\nBatch 11/3471, Loss: 9.5931\nBatch 12/3471, Loss: 7.9068\nBatch 13/3471, Loss: 8.6396\nBatch 14/3471, Loss: 7.6648\nBatch 15/3471, Loss: 9.3244\nBatch 16/3471, Loss: 8.7862\nBatch 17/3471, Loss: 7.6297\nBatch 18/3471, Loss: 7.5229\nBatch 19/3471, Loss: 7.5897\nBatch 20/3471, Loss: 9.2150\n  M√©dia parcial at√© aqui: 8.4675\nBatch 21/3471, Loss: 8.8581\nBatch 22/3471, Loss: 7.1290\nBatch 23/3471, Loss: 8.9023\nBatch 24/3471, Loss: 7.1482\nBatch 25/3471, Loss: 7.7077\nBatch 26/3471, Loss: 9.0135\nBatch 27/3471, Loss: 7.8497\nBatch 28/3471, Loss: 7.5939\nBatch 29/3471, Loss: 7.6320\nBatch 30/3471, Loss: 7.5169\n  M√©dia parcial at√© aqui: 8.2901\nBatch 31/3471, Loss: 6.9963\nBatch 32/3471, Loss: 8.5725\nBatch 33/3471, Loss: 6.4487\nBatch 34/3471, Loss: 9.6575\nBatch 35/3471, Loss: 8.4017\nBatch 36/3471, Loss: 8.6188\nBatch 37/3471, Loss: 8.6697\nBatch 38/3471, Loss: 9.7585\nBatch 39/3471, Loss: 9.3809\nBatch 40/3471, Loss: 8.5710\n  M√©dia parcial at√© aqui: 8.3444\nBatch 41/3471, Loss: 7.6097\nBatch 42/3471, Loss: 7.4555\nBatch 43/3471, Loss: 7.3900\nBatch 44/3471, Loss: 8.6212\nBatch 45/3471, Loss: 8.6682\nBatch 46/3471, Loss: 6.5294\nBatch 47/3471, Loss: 9.3696\nBatch 48/3471, Loss: 9.2478\nBatch 49/3471, Loss: 7.8449\nBatch 50/3471, Loss: 7.5215\n  M√©dia parcial at√© aqui: 8.2807\nBatch 51/3471, Loss: 8.4425\nBatch 52/3471, Loss: 8.6682\nBatch 53/3471, Loss: 9.6810\nBatch 54/3471, Loss: 8.3406\nBatch 55/3471, Loss: 8.1167\nBatch 56/3471, Loss: 6.6949\nBatch 57/3471, Loss: 9.3880\nBatch 58/3471, Loss: 9.5240\nBatch 59/3471, Loss: 8.5188\nBatch 60/3471, Loss: 9.3018\n  M√©dia parcial at√© aqui: 8.3452\nBatch 61/3471, Loss: 9.9322\nBatch 62/3471, Loss: 9.2362\nBatch 63/3471, Loss: 7.6458\nBatch 64/3471, Loss: 9.6379\nBatch 65/3471, Loss: 8.5774\nBatch 66/3471, Loss: 7.0207\nBatch 67/3471, Loss: 8.4012\nBatch 68/3471, Loss: 8.4140\nBatch 69/3471, Loss: 9.4082\nBatch 70/3471, Loss: 7.8378\n  M√©dia parcial at√© aqui: 8.3832\nBatch 71/3471, Loss: 8.6825\nBatch 72/3471, Loss: 9.4260\nBatch 73/3471, Loss: 8.7452\nBatch 74/3471, Loss: 6.8057\nBatch 75/3471, Loss: 8.1956\nBatch 76/3471, Loss: 5.7374\nBatch 77/3471, Loss: 8.2511\nBatch 78/3471, Loss: 7.9455\nBatch 79/3471, Loss: 9.0457\nBatch 80/3471, Loss: 8.9340\n  M√©dia parcial at√© aqui: 8.3574\nBatch 81/3471, Loss: 7.5949\nBatch 82/3471, Loss: 7.8800\nBatch 83/3471, Loss: 9.4729\nBatch 84/3471, Loss: 8.5870\nBatch 85/3471, Loss: 9.7715\nBatch 86/3471, Loss: 8.9486\nBatch 87/3471, Loss: 6.0485\nBatch 88/3471, Loss: 7.6704\nBatch 89/3471, Loss: 8.8348\nBatch 90/3471, Loss: 9.6792\n  M√©dia parcial at√© aqui: 8.3676\nBatch 91/3471, Loss: 7.3637\nBatch 92/3471, Loss: 9.5111\nBatch 93/3471, Loss: 7.5150\nBatch 94/3471, Loss: 8.6143\nBatch 95/3471, Loss: 7.3205\nBatch 96/3471, Loss: 10.0447\nBatch 97/3471, Loss: 8.1258\nBatch 98/3471, Loss: 8.2436\nBatch 99/3471, Loss: 6.8516\nBatch 100/3471, Loss: 9.5414\n  M√©dia parcial at√© aqui: 8.3621\nBatch 101/3471, Loss: 9.0295\nBatch 102/3471, Loss: 9.2990\nBatch 103/3471, Loss: 9.0414\nBatch 104/3471, Loss: 9.4742\nBatch 105/3471, Loss: 9.6685\nBatch 106/3471, Loss: 8.9211\nBatch 107/3471, Loss: 9.8095\nBatch 108/3471, Loss: 9.3474\nBatch 109/3471, Loss: 6.9354\nBatch 110/3471, Loss: 9.3394\n  M√©dia parcial at√© aqui: 8.4280\nBatch 111/3471, Loss: 9.0319\nBatch 112/3471, Loss: 6.7412\nBatch 113/3471, Loss: 6.8411\nBatch 114/3471, Loss: 7.7792\nBatch 115/3471, Loss: 8.6158\nBatch 116/3471, Loss: 8.4684\nBatch 117/3471, Loss: 8.4474\nBatch 118/3471, Loss: 9.4943\nBatch 119/3471, Loss: 7.7182\nBatch 120/3471, Loss: 7.7645\n  M√©dia parcial at√© aqui: 8.3998\nBatch 121/3471, Loss: 8.5936\nBatch 122/3471, Loss: 7.5055\nBatch 123/3471, Loss: 8.8037\nBatch 124/3471, Loss: 9.2754\nBatch 125/3471, Loss: 7.9759\nBatch 126/3471, Loss: 9.7161\nBatch 127/3471, Loss: 9.4688\nBatch 128/3471, Loss: 7.8599\nBatch 129/3471, Loss: 8.8821\nBatch 130/3471, Loss: 8.8972\n  M√©dia parcial at√© aqui: 8.4227\nBatch 131/3471, Loss: 8.2571\nBatch 132/3471, Loss: 7.7122\nBatch 133/3471, Loss: 7.9845\nBatch 134/3471, Loss: 7.5664\nBatch 135/3471, Loss: 9.3695\nBatch 136/3471, Loss: 8.5225\nBatch 137/3471, Loss: 10.1118\nBatch 138/3471, Loss: 5.3835\nBatch 139/3471, Loss: 9.5008\nBatch 140/3471, Loss: 8.4419\n  M√©dia parcial at√© aqui: 8.4129\nBatch 141/3471, Loss: 8.4750\nBatch 142/3471, Loss: 7.1560\nBatch 143/3471, Loss: 9.8127\nBatch 144/3471, Loss: 7.5433\nBatch 145/3471, Loss: 9.2973\nBatch 146/3471, Loss: 9.5913\nBatch 147/3471, Loss: 8.4947\nBatch 148/3471, Loss: 8.3846\nBatch 149/3471, Loss: 7.3536\nBatch 150/3471, Loss: 6.5889\n  M√©dia parcial at√© aqui: 8.4034\nBatch 151/3471, Loss: 9.6211\nBatch 152/3471, Loss: 7.4560\nBatch 153/3471, Loss: 8.7144\nBatch 154/3471, Loss: 9.1355\nBatch 155/3471, Loss: 8.2382\nBatch 156/3471, Loss: 6.0587\nBatch 157/3471, Loss: 8.5102\nBatch 158/3471, Loss: 7.4884\nBatch 159/3471, Loss: 8.9936\nBatch 160/3471, Loss: 9.2172\n  M√©dia parcial at√© aqui: 8.3996\nBatch 161/3471, Loss: 8.3945\nBatch 162/3471, Loss: 9.5062\nBatch 163/3471, Loss: 7.5416\nBatch 164/3471, Loss: 9.3490\nBatch 165/3471, Loss: 9.5054\nBatch 166/3471, Loss: 8.1441\nBatch 167/3471, Loss: 8.8180\nBatch 168/3471, Loss: 9.5975\nBatch 169/3471, Loss: 8.1727\nBatch 170/3471, Loss: 7.4364\n  M√©dia parcial at√© aqui: 8.4141\nBatch 171/3471, Loss: 5.7705\nBatch 172/3471, Loss: 8.6611\nBatch 173/3471, Loss: 8.7324\nBatch 174/3471, Loss: 9.5355\nBatch 175/3471, Loss: 9.3387\nBatch 176/3471, Loss: 7.8356\nBatch 177/3471, Loss: 8.6071\nBatch 178/3471, Loss: 8.6265\nBatch 179/3471, Loss: 6.4637\nBatch 180/3471, Loss: 10.0638\n  M√©dia parcial at√© aqui: 8.4113\nBatch 181/3471, Loss: 7.8748\nBatch 182/3471, Loss: 10.1325\nBatch 183/3471, Loss: 8.8129\nBatch 184/3471, Loss: 9.3813\nBatch 185/3471, Loss: 8.6198\nBatch 186/3471, Loss: 8.8263\nBatch 187/3471, Loss: 10.0572\nBatch 188/3471, Loss: 7.1558\nBatch 189/3471, Loss: 8.9482\nBatch 190/3471, Loss: 8.6216\n  M√©dia parcial at√© aqui: 8.4340\nBatch 191/3471, Loss: 6.5349\nBatch 192/3471, Loss: 9.0539\nBatch 193/3471, Loss: 9.1569\nBatch 194/3471, Loss: 7.6157\nBatch 195/3471, Loss: 8.5294\nBatch 196/3471, Loss: 8.6796\nBatch 197/3471, Loss: 9.4102\nBatch 198/3471, Loss: 8.8722\nBatch 199/3471, Loss: 8.1015\nBatch 200/3471, Loss: 10.1368\n  M√©dia parcial at√© aqui: 8.4428\nBatch 201/3471, Loss: 8.8668\nBatch 202/3471, Loss: 7.9797\nBatch 203/3471, Loss: 8.6022\nBatch 204/3471, Loss: 7.7303\nBatch 205/3471, Loss: 7.7474\nBatch 206/3471, Loss: 7.8186\nBatch 207/3471, Loss: 9.5656\nBatch 208/3471, Loss: 8.6954\nBatch 209/3471, Loss: 9.5041\nBatch 210/3471, Loss: 8.2499\n  M√©dia parcial at√© aqui: 8.4444\nBatch 211/3471, Loss: 8.4301\nBatch 212/3471, Loss: 7.8395\nBatch 213/3471, Loss: 10.0611\nBatch 214/3471, Loss: 8.2942\nBatch 215/3471, Loss: 9.4571\nBatch 216/3471, Loss: 8.7425\nBatch 217/3471, Loss: 7.4125\nBatch 218/3471, Loss: 8.6855\nBatch 219/3471, Loss: 8.6556\nBatch 220/3471, Loss: 8.6166\n  M√©dia parcial at√© aqui: 8.4523\nBatch 221/3471, Loss: 9.6561\nBatch 222/3471, Loss: 9.1867\nBatch 223/3471, Loss: 9.4671\nBatch 224/3471, Loss: 5.7783\nBatch 225/3471, Loss: 9.9449\nBatch 226/3471, Loss: 8.8585\nBatch 227/3471, Loss: 9.5735\nBatch 228/3471, Loss: 8.5271\nBatch 229/3471, Loss: 9.3621\nBatch 230/3471, Loss: 7.8808\n  M√©dia parcial at√© aqui: 8.4685\nBatch 231/3471, Loss: 6.9011\nBatch 232/3471, Loss: 8.6254\nBatch 233/3471, Loss: 8.1010\nBatch 234/3471, Loss: 9.6461\nBatch 235/3471, Loss: 7.8087\nBatch 236/3471, Loss: 9.8686\nBatch 237/3471, Loss: 6.4190\nBatch 238/3471, Loss: 8.5718\nBatch 239/3471, Loss: 9.2233\nBatch 240/3471, Loss: 9.7481\n  M√©dia parcial at√© aqui: 8.4694\nBatch 241/3471, Loss: 8.5491\nBatch 242/3471, Loss: 9.5398\nBatch 243/3471, Loss: 9.5892\nBatch 244/3471, Loss: 7.4112\nBatch 245/3471, Loss: 8.6865\nBatch 246/3471, Loss: 7.6716\nBatch 247/3471, Loss: 9.4695\nBatch 248/3471, Loss: 7.9900\nBatch 249/3471, Loss: 7.7329\nBatch 250/3471, Loss: 8.7738\n  M√©dia parcial at√© aqui: 8.4723\nBatch 251/3471, Loss: 7.6933\nBatch 252/3471, Loss: 9.6798\nBatch 253/3471, Loss: 8.6383\nBatch 254/3471, Loss: 8.0672\nBatch 255/3471, Loss: 7.8845\nBatch 256/3471, Loss: 8.6929\nBatch 257/3471, Loss: 8.2426\nBatch 258/3471, Loss: 9.5132\nBatch 259/3471, Loss: 8.0372\nBatch 260/3471, Loss: 7.4513\n  M√©dia parcial at√© aqui: 8.4691\nBatch 261/3471, Loss: 9.1119\nBatch 262/3471, Loss: 9.6454\nBatch 263/3471, Loss: 7.6524\nBatch 264/3471, Loss: 6.5601\nBatch 265/3471, Loss: 9.6545\nBatch 266/3471, Loss: 7.5947\nBatch 267/3471, Loss: 7.6906\nBatch 268/3471, Loss: 8.3743\nBatch 269/3471, Loss: 8.2690\nBatch 270/3471, Loss: 9.9291\n  M√©dia parcial at√© aqui: 8.4684\nBatch 271/3471, Loss: 7.6146\nBatch 272/3471, Loss: 8.6559\nBatch 273/3471, Loss: 7.6842\nBatch 274/3471, Loss: 8.0229\nBatch 275/3471, Loss: 7.6589\nBatch 276/3471, Loss: 9.5570\nBatch 277/3471, Loss: 9.4427\nBatch 278/3471, Loss: 8.7020\nBatch 279/3471, Loss: 8.5526\nBatch 280/3471, Loss: 9.5840\n  M√©dia parcial at√© aqui: 8.4712\nBatch 281/3471, Loss: 9.7951\nBatch 282/3471, Loss: 8.2444\nBatch 283/3471, Loss: 9.0795\nBatch 284/3471, Loss: 7.6444\nBatch 285/3471, Loss: 8.3411\nBatch 286/3471, Loss: 7.4159\nBatch 287/3471, Loss: 8.0499\nBatch 288/3471, Loss: 8.3550\nBatch 289/3471, Loss: 8.0763\nBatch 290/3471, Loss: 7.9864\n  M√©dia parcial at√© aqui: 8.4652\nBatch 291/3471, Loss: 8.2817\nBatch 292/3471, Loss: 8.6106\nBatch 293/3471, Loss: 8.6316\nBatch 294/3471, Loss: 8.9005\nBatch 295/3471, Loss: 6.4900\nBatch 296/3471, Loss: 9.2839\nBatch 297/3471, Loss: 9.7937\nBatch 298/3471, Loss: 8.5559\nBatch 299/3471, Loss: 9.2401\nBatch 300/3471, Loss: 9.7012\n  M√©dia parcial at√© aqui: 8.4747\nBatch 301/3471, Loss: 4.7133\nBatch 302/3471, Loss: 6.9438\nBatch 303/3471, Loss: 8.5966\nBatch 304/3471, Loss: 8.1180\nBatch 305/3471, Loss: 9.3535\nBatch 306/3471, Loss: 8.9368\nBatch 307/3471, Loss: 8.6219\nBatch 308/3471, Loss: 8.4420\nBatch 309/3471, Loss: 8.7173\nBatch 310/3471, Loss: 8.3832\n  M√©dia parcial at√© aqui: 8.4621\nBatch 311/3471, Loss: 8.5384\nBatch 312/3471, Loss: 8.6995\nBatch 313/3471, Loss: 8.6256\nBatch 314/3471, Loss: 8.6954\nBatch 315/3471, Loss: 7.7122\nBatch 316/3471, Loss: 7.9887\nBatch 317/3471, Loss: 9.3260\nBatch 318/3471, Loss: 8.4494\nBatch 319/3471, Loss: 7.5965\nBatch 320/3471, Loss: 5.6809\n  M√©dia parcial at√© aqui: 8.4517\nBatch 321/3471, Loss: 9.7792\nBatch 322/3471, Loss: 8.6370\nBatch 323/3471, Loss: 9.7499\nBatch 324/3471, Loss: 8.3808\nBatch 325/3471, Loss: 9.4041\nBatch 326/3471, Loss: 9.7365\nBatch 327/3471, Loss: 9.1684\nBatch 328/3471, Loss: 8.4602\nBatch 329/3471, Loss: 9.9200\nBatch 330/3471, Loss: 9.9684\n  M√©dia parcial at√© aqui: 8.4780\nBatch 331/3471, Loss: 8.6447\nBatch 332/3471, Loss: 9.6541\nBatch 333/3471, Loss: 8.3301\nBatch 334/3471, Loss: 7.2684\nBatch 335/3471, Loss: 7.8442\nBatch 336/3471, Loss: 7.3246\nBatch 337/3471, Loss: 9.7022\nBatch 338/3471, Loss: 6.6070\nBatch 339/3471, Loss: 6.9704\nBatch 340/3471, Loss: 8.7799\n  M√©dia parcial at√© aqui: 8.4673\nBatch 341/3471, Loss: 8.7123\nBatch 342/3471, Loss: 8.9822\nBatch 343/3471, Loss: 8.9787\nBatch 344/3471, Loss: 9.5911\nBatch 345/3471, Loss: 8.4218\nBatch 346/3471, Loss: 7.5691\nBatch 347/3471, Loss: 8.1644\nBatch 348/3471, Loss: 7.0691\nBatch 349/3471, Loss: 7.5969\nBatch 350/3471, Loss: 8.6228\n  M√©dia parcial at√© aqui: 8.4645\nBatch 351/3471, Loss: 8.5760\nBatch 352/3471, Loss: 8.1140\nBatch 353/3471, Loss: 8.6207\nBatch 354/3471, Loss: 9.7346\nBatch 355/3471, Loss: 6.6321\nBatch 356/3471, Loss: 6.7697\nBatch 357/3471, Loss: 9.9141\nBatch 358/3471, Loss: 7.8316\nBatch 359/3471, Loss: 8.9801\nBatch 360/3471, Loss: 5.7915\n  M√©dia parcial at√© aqui: 8.4543\nBatch 361/3471, Loss: 10.0024\nBatch 362/3471, Loss: 8.8118\nBatch 363/3471, Loss: 6.2667\nBatch 364/3471, Loss: 7.3799\nBatch 365/3471, Loss: 9.5495\nBatch 366/3471, Loss: 8.7197\nBatch 367/3471, Loss: 7.6081\nBatch 368/3471, Loss: 8.2886\nBatch 369/3471, Loss: 7.3513\nBatch 370/3471, Loss: 9.2482\n  M√©dia parcial at√© aqui: 8.4508\nBatch 371/3471, Loss: 7.5412\nBatch 372/3471, Loss: 9.2021\nBatch 373/3471, Loss: 7.4934\nBatch 374/3471, Loss: 9.9357\nBatch 375/3471, Loss: 9.4807\nBatch 376/3471, Loss: 7.8788\nBatch 377/3471, Loss: 7.5218\nBatch 378/3471, Loss: 7.5710\nBatch 379/3471, Loss: 8.9889\nBatch 380/3471, Loss: 6.4399\n  M√©dia parcial at√© aqui: 8.4443\nBatch 381/3471, Loss: 9.1107\nBatch 382/3471, Loss: 7.8234\nBatch 383/3471, Loss: 9.9028\nBatch 384/3471, Loss: 8.4090\nBatch 385/3471, Loss: 7.5507\nBatch 386/3471, Loss: 8.2553\nBatch 387/3471, Loss: 7.4588\nBatch 388/3471, Loss: 9.4534\nBatch 389/3471, Loss: 7.5128\nBatch 390/3471, Loss: 7.0226\n  M√©dia parcial at√© aqui: 8.4393\nBatch 391/3471, Loss: 8.5800\nBatch 392/3471, Loss: 9.8036\nBatch 393/3471, Loss: 8.9699\nBatch 394/3471, Loss: 9.3406\nBatch 395/3471, Loss: 6.4940\nBatch 396/3471, Loss: 8.8635\nBatch 397/3471, Loss: 8.7878\nBatch 398/3471, Loss: 9.8079\nBatch 399/3471, Loss: 9.4834\nBatch 400/3471, Loss: 7.4944\n  M√©dia parcial at√© aqui: 8.4474\nBatch 401/3471, Loss: 7.9686\nBatch 402/3471, Loss: 8.1399\nBatch 403/3471, Loss: 9.3770\nBatch 404/3471, Loss: 8.6772\nBatch 405/3471, Loss: 8.3164\nBatch 406/3471, Loss: 9.3566\nBatch 407/3471, Loss: 8.6433\nBatch 408/3471, Loss: 8.6900\nBatch 409/3471, Loss: 9.3448\nBatch 410/3471, Loss: 6.7421\n  M√©dia parcial at√© aqui: 8.4493\nBatch 411/3471, Loss: 6.7328\nBatch 412/3471, Loss: 8.9550\nBatch 413/3471, Loss: 8.2348\nBatch 414/3471, Loss: 9.4757\nBatch 415/3471, Loss: 7.4034\nBatch 416/3471, Loss: 8.4552\nBatch 417/3471, Loss: 8.5732\nBatch 418/3471, Loss: 7.7005\nBatch 419/3471, Loss: 7.4334\nBatch 420/3471, Loss: 9.4776\n  M√©dia parcial at√© aqui: 8.4444\nBatch 421/3471, Loss: 9.6585\nBatch 422/3471, Loss: 8.8417\nBatch 423/3471, Loss: 8.3891\nBatch 424/3471, Loss: 9.7795\nBatch 425/3471, Loss: 8.0688\nBatch 426/3471, Loss: 9.3259\nBatch 427/3471, Loss: 7.1923\nBatch 428/3471, Loss: 9.5649\nBatch 429/3471, Loss: 8.7556\nBatch 430/3471, Loss: 9.6230\n  M√©dia parcial at√© aqui: 8.4555\nBatch 431/3471, Loss: 9.6984\nBatch 432/3471, Loss: 5.2537\nBatch 433/3471, Loss: 8.7980\nBatch 434/3471, Loss: 8.0800\nBatch 435/3471, Loss: 7.8821\nBatch 436/3471, Loss: 8.9056\nBatch 437/3471, Loss: 10.0937\nBatch 438/3471, Loss: 6.8077\nBatch 439/3471, Loss: 8.3944\nBatch 440/3471, Loss: 9.3692\n  M√©dia parcial at√© aqui: 8.4526\nBatch 441/3471, Loss: 9.7166\nBatch 442/3471, Loss: 9.9687\nBatch 443/3471, Loss: 8.9124\nBatch 444/3471, Loss: 8.4039\nBatch 445/3471, Loss: 8.3860\nBatch 446/3471, Loss: 9.5238\nBatch 447/3471, Loss: 6.9031\nBatch 448/3471, Loss: 7.4076\nBatch 449/3471, Loss: 8.2416\nBatch 450/3471, Loss: 9.5009\n  M√©dia parcial at√© aqui: 8.4580\nBatch 451/3471, Loss: 9.6560\nBatch 452/3471, Loss: 8.2923\nBatch 453/3471, Loss: 8.5316\nBatch 454/3471, Loss: 10.0707\nBatch 455/3471, Loss: 6.5250\nBatch 456/3471, Loss: 7.2905\nBatch 457/3471, Loss: 7.4545\nBatch 458/3471, Loss: 8.5442\nBatch 459/3471, Loss: 9.9699\nBatch 460/3471, Loss: 8.4516\n  M√©dia parcial at√© aqui: 8.4585\nBatch 461/3471, Loss: 9.0384\nBatch 462/3471, Loss: 8.3223\nBatch 463/3471, Loss: 8.7535\nBatch 464/3471, Loss: 9.8388\nBatch 465/3471, Loss: 10.1967\nBatch 466/3471, Loss: 9.1125\nBatch 467/3471, Loss: 9.5553\nBatch 468/3471, Loss: 8.2248\nBatch 469/3471, Loss: 8.7490\nBatch 470/3471, Loss: 8.9905\n  M√©dia parcial at√© aqui: 8.4716\nBatch 471/3471, Loss: 8.2520\nBatch 472/3471, Loss: 8.6865\nBatch 473/3471, Loss: 7.7642\nBatch 474/3471, Loss: 7.6408\nBatch 475/3471, Loss: 7.4456\nBatch 476/3471, Loss: 8.5926\nBatch 477/3471, Loss: 7.4361\nBatch 478/3471, Loss: 6.9812\nBatch 479/3471, Loss: 8.6218\nBatch 480/3471, Loss: 9.2368\n  M√©dia parcial at√© aqui: 8.4632\nBatch 481/3471, Loss: 9.8779\nBatch 482/3471, Loss: 8.5640\nBatch 483/3471, Loss: 6.2363\nBatch 484/3471, Loss: 9.2302\nBatch 485/3471, Loss: 6.8346\nBatch 486/3471, Loss: 9.0717\nBatch 487/3471, Loss: 6.6159\nBatch 488/3471, Loss: 8.6928\nBatch 489/3471, Loss: 8.8031\nBatch 490/3471, Loss: 8.7464\n  M√©dia parcial at√© aqui: 8.4592\nBatch 491/3471, Loss: 8.2103\nBatch 492/3471, Loss: 8.0471\nBatch 493/3471, Loss: 7.7427\nBatch 494/3471, Loss: 6.6204\nBatch 495/3471, Loss: 9.8663\nBatch 496/3471, Loss: 9.2901\nBatch 497/3471, Loss: 8.9750\nBatch 498/3471, Loss: 7.9357\nBatch 499/3471, Loss: 7.6179\nBatch 500/3471, Loss: 6.6935\n  M√©dia parcial at√© aqui: 8.4520\nBatch 501/3471, Loss: 6.6784\nBatch 502/3471, Loss: 9.7021\nBatch 503/3471, Loss: 10.2687\nBatch 504/3471, Loss: 5.1238\nBatch 505/3471, Loss: 7.5223\nBatch 506/3471, Loss: 7.6400\nBatch 507/3471, Loss: 8.1821\nBatch 508/3471, Loss: 9.1299\nBatch 509/3471, Loss: 9.5208\nBatch 510/3471, Loss: 9.7436\n  M√©dia parcial at√© aqui: 8.4500\nBatch 511/3471, Loss: 8.2950\nBatch 512/3471, Loss: 9.3618\nBatch 513/3471, Loss: 9.3038\nBatch 514/3471, Loss: 9.1794\nBatch 515/3471, Loss: 7.9697\nBatch 516/3471, Loss: 5.4626\nBatch 517/3471, Loss: 6.6549\nBatch 518/3471, Loss: 6.7576\nBatch 519/3471, Loss: 7.7657\nBatch 520/3471, Loss: 9.3091\n  M√©dia parcial at√© aqui: 8.4415\nBatch 521/3471, Loss: 9.6599\nBatch 522/3471, Loss: 7.7660\nBatch 523/3471, Loss: 6.1482\nBatch 524/3471, Loss: 9.3169\nBatch 525/3471, Loss: 8.5627\nBatch 526/3471, Loss: 7.7519\nBatch 527/3471, Loss: 9.6294\nBatch 528/3471, Loss: 8.5631\nBatch 529/3471, Loss: 9.1821\nBatch 530/3471, Loss: 8.6810\n  M√©dia parcial at√© aqui: 8.4431\nBatch 531/3471, Loss: 8.7189\nBatch 532/3471, Loss: 8.4200\nBatch 533/3471, Loss: 8.5670\nBatch 534/3471, Loss: 8.8619\nBatch 535/3471, Loss: 7.8560\nBatch 536/3471, Loss: 7.6923\nBatch 537/3471, Loss: 8.5695\nBatch 538/3471, Loss: 8.9416\nBatch 539/3471, Loss: 7.4269\nBatch 540/3471, Loss: 9.5709\n  M√©dia parcial at√© aqui: 8.4434\nBatch 541/3471, Loss: 8.3867\nBatch 542/3471, Loss: 7.7874\nBatch 543/3471, Loss: 9.5783\nBatch 544/3471, Loss: 7.4804\nBatch 545/3471, Loss: 8.2665\nBatch 546/3471, Loss: 9.7837\nBatch 547/3471, Loss: 7.2421\nBatch 548/3471, Loss: 9.4604\nBatch 549/3471, Loss: 9.6949\nBatch 550/3471, Loss: 8.2181\n  M√©dia parcial at√© aqui: 8.4461\nBatch 551/3471, Loss: 8.5103\nBatch 552/3471, Loss: 8.6414\nBatch 553/3471, Loss: 8.2750\nBatch 554/3471, Loss: 8.5132\nBatch 555/3471, Loss: 8.7195\nBatch 556/3471, Loss: 8.4210\nBatch 557/3471, Loss: 8.6748\nBatch 558/3471, Loss: 8.5733\nBatch 559/3471, Loss: 8.2932\nBatch 560/3471, Loss: 9.4825\n  M√©dia parcial at√© aqui: 8.4490\nBatch 561/3471, Loss: 9.3258\nBatch 562/3471, Loss: 9.4472\nBatch 563/3471, Loss: 7.9070\nBatch 564/3471, Loss: 8.3735\nBatch 565/3471, Loss: 7.2223\nBatch 566/3471, Loss: 7.6373\nBatch 567/3471, Loss: 8.3251\nBatch 568/3471, Loss: 7.2595\nBatch 569/3471, Loss: 10.0399\nBatch 570/3471, Loss: 9.6257\n  M√©dia parcial at√© aqui: 8.4502\nBatch 571/3471, Loss: 7.4459\nBatch 572/3471, Loss: 8.7121\nBatch 573/3471, Loss: 7.9206\nBatch 574/3471, Loss: 7.2788\nBatch 575/3471, Loss: 9.2493\nBatch 576/3471, Loss: 10.1212\nBatch 577/3471, Loss: 9.5555\nBatch 578/3471, Loss: 8.6026\nBatch 579/3471, Loss: 9.3442\nBatch 580/3471, Loss: 8.6616\n  M√©dia parcial at√© aqui: 8.4543\nBatch 581/3471, Loss: 8.6035\nBatch 582/3471, Loss: 7.0335\nBatch 583/3471, Loss: 8.7091\nBatch 584/3471, Loss: 6.9025\nBatch 585/3471, Loss: 7.5763\nBatch 586/3471, Loss: 8.4736\nBatch 587/3471, Loss: 7.5311\nBatch 588/3471, Loss: 7.0658\nBatch 589/3471, Loss: 9.0305\nBatch 590/3471, Loss: 7.5816\n  M√©dia parcial at√© aqui: 8.4441\nBatch 591/3471, Loss: 9.5175\nBatch 592/3471, Loss: 9.0134\nBatch 593/3471, Loss: 7.2733\nBatch 594/3471, Loss: 8.1319\nBatch 595/3471, Loss: 7.6156\nBatch 596/3471, Loss: 9.3137\nBatch 597/3471, Loss: 8.4405\nBatch 598/3471, Loss: 9.2693\nBatch 599/3471, Loss: 7.6796\nBatch 600/3471, Loss: 8.5216\n  M√©dia parcial at√© aqui: 8.4447\nBatch 601/3471, Loss: 8.4419\nBatch 602/3471, Loss: 8.7954\nBatch 603/3471, Loss: 6.7050\nBatch 604/3471, Loss: 8.5890\nBatch 605/3471, Loss: 8.3937\nBatch 606/3471, Loss: 8.2941\nBatch 607/3471, Loss: 9.4354\nBatch 608/3471, Loss: 8.8617\nBatch 609/3471, Loss: 7.2484\nBatch 610/3471, Loss: 7.7310\n  M√©dia parcial at√© aqui: 8.4415\nBatch 611/3471, Loss: 8.6302\nBatch 612/3471, Loss: 9.2757\nBatch 613/3471, Loss: 8.6260\nBatch 614/3471, Loss: 4.9884\nBatch 615/3471, Loss: 8.5239\nBatch 616/3471, Loss: 8.6995\nBatch 617/3471, Loss: 8.6735\nBatch 618/3471, Loss: 9.6824\nBatch 619/3471, Loss: 8.4306\nBatch 620/3471, Loss: 7.9633\n  M√©dia parcial at√© aqui: 8.4400\nBatch 621/3471, Loss: 9.6840\nBatch 622/3471, Loss: 6.6233\nBatch 623/3471, Loss: 9.6765\nBatch 624/3471, Loss: 8.7313\nBatch 625/3471, Loss: 6.5953\nBatch 626/3471, Loss: 9.5797\nBatch 627/3471, Loss: 9.7291\nBatch 628/3471, Loss: 7.5644\nBatch 629/3471, Loss: 8.7342\nBatch 630/3471, Loss: 3.8719\n  M√©dia parcial at√© aqui: 8.4342\nBatch 631/3471, Loss: 7.3364\nBatch 632/3471, Loss: 8.5182\nBatch 633/3471, Loss: 6.5997\nBatch 634/3471, Loss: 8.9754\nBatch 635/3471, Loss: 9.8010\nBatch 636/3471, Loss: 8.9688\nBatch 637/3471, Loss: 4.6936\nBatch 638/3471, Loss: 9.4463\nBatch 639/3471, Loss: 8.4789\nBatch 640/3471, Loss: 8.7796\n  M√©dia parcial at√© aqui: 8.4300\nBatch 641/3471, Loss: 8.3515\nBatch 642/3471, Loss: 8.7285\nBatch 643/3471, Loss: 8.6242\nBatch 644/3471, Loss: 6.6706\nBatch 645/3471, Loss: 8.8024\nBatch 646/3471, Loss: 8.5490\nBatch 647/3471, Loss: 6.4003\nBatch 648/3471, Loss: 8.4961\nBatch 649/3471, Loss: 9.6086\nBatch 650/3471, Loss: 9.8262\n  M√©dia parcial at√© aqui: 8.4296\nBatch 651/3471, Loss: 9.2538\nBatch 652/3471, Loss: 8.7481\nBatch 653/3471, Loss: 9.5338\nBatch 654/3471, Loss: 9.5102\nBatch 655/3471, Loss: 8.4647\nBatch 656/3471, Loss: 7.4631\nBatch 657/3471, Loss: 8.8520\nBatch 658/3471, Loss: 9.0549\nBatch 659/3471, Loss: 8.8798\nBatch 660/3471, Loss: 8.6385\n  M√©dia parcial at√© aqui: 8.4358\nBatch 661/3471, Loss: 8.4252\nBatch 662/3471, Loss: 9.4099\nBatch 663/3471, Loss: 8.3280\nBatch 664/3471, Loss: 8.5500\nBatch 665/3471, Loss: 9.6280\nBatch 666/3471, Loss: 9.3990\nBatch 667/3471, Loss: 7.9457\nBatch 668/3471, Loss: 8.9462\nBatch 669/3471, Loss: 7.6321\nBatch 670/3471, Loss: 8.9828\n  M√©dia parcial at√© aqui: 8.4401\nBatch 671/3471, Loss: 9.5381\nBatch 672/3471, Loss: 8.1786\nBatch 673/3471, Loss: 7.6837\nBatch 674/3471, Loss: 9.6974\nBatch 675/3471, Loss: 8.3444\nBatch 676/3471, Loss: 9.5589\nBatch 677/3471, Loss: 8.0452\nBatch 678/3471, Loss: 8.8989\nBatch 679/3471, Loss: 8.2457\nBatch 680/3471, Loss: 8.5516\n  M√©dia parcial at√© aqui: 8.4436\nBatch 681/3471, Loss: 9.3290\nBatch 682/3471, Loss: 6.9362\nBatch 683/3471, Loss: 7.5631\nBatch 684/3471, Loss: 7.9879\nBatch 685/3471, Loss: 7.6562\nBatch 686/3471, Loss: 8.6919\nBatch 687/3471, Loss: 7.3982\nBatch 688/3471, Loss: 6.5289\nBatch 689/3471, Loss: 9.4583\nBatch 690/3471, Loss: 9.4208\n  M√©dia parcial at√© aqui: 8.4385\nBatch 691/3471, Loss: 9.8759\nBatch 692/3471, Loss: 9.2549\nBatch 693/3471, Loss: 8.9258\nBatch 694/3471, Loss: 8.5757\nBatch 695/3471, Loss: 7.8073\nBatch 696/3471, Loss: 7.9797\nBatch 697/3471, Loss: 8.3179\nBatch 698/3471, Loss: 7.3227\nBatch 699/3471, Loss: 9.4471\nBatch 700/3471, Loss: 6.5563\n  M√©dia parcial at√© aqui: 8.4381\nBatch 701/3471, Loss: 9.5359\nBatch 702/3471, Loss: 9.8296\nBatch 703/3471, Loss: 8.5022\nBatch 704/3471, Loss: 9.3460\nBatch 705/3471, Loss: 9.4590\nBatch 706/3471, Loss: 8.8049\nBatch 707/3471, Loss: 9.6856\nBatch 708/3471, Loss: 8.9172\nBatch 709/3471, Loss: 9.6951\nBatch 710/3471, Loss: 8.4337\n  M√©dia parcial at√© aqui: 8.4491\nBatch 711/3471, Loss: 7.8485\nBatch 712/3471, Loss: 8.1908\nBatch 713/3471, Loss: 8.4557\nBatch 714/3471, Loss: 8.5863\nBatch 715/3471, Loss: 7.8793\nBatch 716/3471, Loss: 9.5797\nBatch 717/3471, Loss: 8.5616\nBatch 718/3471, Loss: 8.3674\nBatch 719/3471, Loss: 8.8494\nBatch 720/3471, Loss: 8.7024\n  M√©dia parcial at√© aqui: 8.4498\nBatch 721/3471, Loss: 8.2707\nBatch 722/3471, Loss: 7.9493\nBatch 723/3471, Loss: 6.8137\nBatch 724/3471, Loss: 7.7582\nBatch 725/3471, Loss: 8.1690\nBatch 726/3471, Loss: 9.3663\nBatch 727/3471, Loss: 8.3449\nBatch 728/3471, Loss: 7.4026\nBatch 729/3471, Loss: 9.3886\nBatch 730/3471, Loss: 10.1234\n  M√©dia parcial at√© aqui: 8.4486\nBatch 731/3471, Loss: 9.1751\nBatch 732/3471, Loss: 9.4925\nBatch 733/3471, Loss: 9.1359\nBatch 734/3471, Loss: 9.4653\nBatch 735/3471, Loss: 9.6999\nBatch 736/3471, Loss: 9.5208\nBatch 737/3471, Loss: 9.5078\nBatch 738/3471, Loss: 9.5318\nBatch 739/3471, Loss: 10.4222\nBatch 740/3471, Loss: 8.8347\n  M√©dia parcial at√© aqui: 8.4625\nBatch 741/3471, Loss: 8.1969\nBatch 742/3471, Loss: 8.1435\nBatch 743/3471, Loss: 7.5948\nBatch 744/3471, Loss: 7.1236\nBatch 745/3471, Loss: 9.6538\nBatch 746/3471, Loss: 9.8451\nBatch 747/3471, Loss: 9.4422\nBatch 748/3471, Loss: 9.8811\nBatch 749/3471, Loss: 10.3787\nBatch 750/3471, Loss: 8.6026\n  M√©dia parcial at√© aqui: 8.4682\nBatch 751/3471, Loss: 8.5161\nBatch 752/3471, Loss: 8.3922\nBatch 753/3471, Loss: 8.7762\nBatch 754/3471, Loss: 8.3998\nBatch 755/3471, Loss: 8.0914\nBatch 756/3471, Loss: 9.7580\nBatch 757/3471, Loss: 8.7658\nBatch 758/3471, Loss: 7.4665\nBatch 759/3471, Loss: 7.5695\nBatch 760/3471, Loss: 7.9186\n  M√©dia parcial at√© aqui: 8.4668\nBatch 761/3471, Loss: 8.7718\nBatch 762/3471, Loss: 8.5534\nBatch 763/3471, Loss: 8.7745\nBatch 764/3471, Loss: 7.7984\nBatch 765/3471, Loss: 6.5613\nBatch 766/3471, Loss: 7.4422\nBatch 767/3471, Loss: 8.8901\nBatch 768/3471, Loss: 8.2887\nBatch 769/3471, Loss: 9.2614\nBatch 770/3471, Loss: 4.6021\n  M√©dia parcial at√© aqui: 8.4594\nBatch 771/3471, Loss: 7.4831\nBatch 772/3471, Loss: 9.3736\nBatch 773/3471, Loss: 7.9498\nBatch 774/3471, Loss: 9.9965\nBatch 775/3471, Loss: 8.1875\nBatch 776/3471, Loss: 7.6056\nBatch 777/3471, Loss: 9.8189\nBatch 778/3471, Loss: 6.9291\nBatch 779/3471, Loss: 8.5938\nBatch 780/3471, Loss: 9.6902\n  M√©dia parcial at√© aqui: 8.4607\nBatch 781/3471, Loss: 9.1964\nBatch 782/3471, Loss: 8.5417\nBatch 783/3471, Loss: 9.4595\nBatch 784/3471, Loss: 10.0095\nBatch 785/3471, Loss: 9.3197\nBatch 786/3471, Loss: 8.8234\nBatch 787/3471, Loss: 8.9247\nBatch 788/3471, Loss: 7.9797\nBatch 789/3471, Loss: 6.5435\nBatch 790/3471, Loss: 6.9075\n  M√©dia parcial at√© aqui: 8.4621\nBatch 791/3471, Loss: 8.5753\nBatch 792/3471, Loss: 9.1845\nBatch 793/3471, Loss: 9.5571\nBatch 794/3471, Loss: 8.2540\nBatch 795/3471, Loss: 8.4677\nBatch 796/3471, Loss: 5.8219\nBatch 797/3471, Loss: 8.6653\nBatch 798/3471, Loss: 8.6707\nBatch 799/3471, Loss: 6.5917\nBatch 800/3471, Loss: 6.7909\n  M√©dia parcial at√© aqui: 8.4570\nBatch 801/3471, Loss: 7.7513\nBatch 802/3471, Loss: 8.9506\nBatch 803/3471, Loss: 9.3220\nBatch 804/3471, Loss: 8.3466\nBatch 805/3471, Loss: 8.4998\nBatch 806/3471, Loss: 8.7085\nBatch 807/3471, Loss: 8.5272\nBatch 808/3471, Loss: 8.2123\nBatch 809/3471, Loss: 9.7374\nBatch 810/3471, Loss: 7.7378\n  M√©dia parcial at√© aqui: 8.4585\nBatch 811/3471, Loss: 7.8675\nBatch 812/3471, Loss: 9.5384\nBatch 813/3471, Loss: 7.5091\nBatch 814/3471, Loss: 8.6530\nBatch 815/3471, Loss: 8.6903\nBatch 816/3471, Loss: 8.7018\nBatch 817/3471, Loss: 8.6856\nBatch 818/3471, Loss: 7.6408\nBatch 819/3471, Loss: 7.5229\nBatch 820/3471, Loss: 7.7852\n  M√©dia parcial at√© aqui: 8.4561\nBatch 821/3471, Loss: 6.3533\nBatch 822/3471, Loss: 9.7702\nBatch 823/3471, Loss: 5.7805\nBatch 824/3471, Loss: 4.6549\nBatch 825/3471, Loss: 7.8108\nBatch 826/3471, Loss: 6.5629\nBatch 827/3471, Loss: 6.6887\nBatch 828/3471, Loss: 6.7833\nBatch 829/3471, Loss: 9.6395\nBatch 830/3471, Loss: 9.7547\n  M√©dia parcial at√© aqui: 8.4432\nBatch 831/3471, Loss: 9.3900\nBatch 832/3471, Loss: 6.8907\nBatch 833/3471, Loss: 8.5677\nBatch 834/3471, Loss: 8.4323\nBatch 835/3471, Loss: 8.7857\nBatch 836/3471, Loss: 9.5594\nBatch 837/3471, Loss: 6.5353\nBatch 838/3471, Loss: 8.3602\nBatch 839/3471, Loss: 8.0937\nBatch 840/3471, Loss: 9.0892\n  M√©dia parcial at√© aqui: 8.4423\nBatch 841/3471, Loss: 7.5602\nBatch 842/3471, Loss: 7.8260\nBatch 843/3471, Loss: 9.9906\nBatch 844/3471, Loss: 7.8202\nBatch 845/3471, Loss: 6.4387\nBatch 846/3471, Loss: 8.4656\nBatch 847/3471, Loss: 8.1333\nBatch 848/3471, Loss: 7.4120\nBatch 849/3471, Loss: 7.8476\nBatch 850/3471, Loss: 8.9863\n  M√©dia parcial at√© aqui: 8.4376\nBatch 851/3471, Loss: 7.6547\nBatch 852/3471, Loss: 7.5539\nBatch 853/3471, Loss: 8.7663\nBatch 854/3471, Loss: 7.6979\nBatch 855/3471, Loss: 8.7955\nBatch 856/3471, Loss: 9.8106\nBatch 857/3471, Loss: 9.8282\nBatch 858/3471, Loss: 8.6396\nBatch 859/3471, Loss: 8.9013\nBatch 860/3471, Loss: 9.4852\n  M√©dia parcial at√© aqui: 8.4409\nBatch 861/3471, Loss: 8.8174\nBatch 862/3471, Loss: 7.8761\nBatch 863/3471, Loss: 7.9857\nBatch 864/3471, Loss: 6.1377\nBatch 865/3471, Loss: 9.6367\nBatch 866/3471, Loss: 7.5439\nBatch 867/3471, Loss: 9.3229\nBatch 868/3471, Loss: 7.0076\nBatch 869/3471, Loss: 9.8589\nBatch 870/3471, Loss: 9.7521\n  M√©dia parcial at√© aqui: 8.4403\nBatch 871/3471, Loss: 5.0552\nBatch 872/3471, Loss: 8.8044\nBatch 873/3471, Loss: 9.5289\nBatch 874/3471, Loss: 9.8051\nBatch 875/3471, Loss: 9.8522\nBatch 876/3471, Loss: 8.3800\nBatch 877/3471, Loss: 8.7058\nBatch 878/3471, Loss: 9.5957\nBatch 879/3471, Loss: 6.9631\nBatch 880/3471, Loss: 8.9577\n  M√©dia parcial at√© aqui: 8.4417\nBatch 881/3471, Loss: 8.6189\nBatch 882/3471, Loss: 8.7036\nBatch 883/3471, Loss: 8.7799\nBatch 884/3471, Loss: 7.5284\nBatch 885/3471, Loss: 9.5490\nBatch 886/3471, Loss: 8.1422\nBatch 887/3471, Loss: 9.6518\nBatch 888/3471, Loss: 8.5160\nBatch 889/3471, Loss: 8.3487\nBatch 890/3471, Loss: 9.8194\n  M√©dia parcial at√© aqui: 8.4454\nBatch 891/3471, Loss: 8.1939\nBatch 892/3471, Loss: 9.0598\nBatch 893/3471, Loss: 8.9829\nBatch 894/3471, Loss: 6.5328\nBatch 895/3471, Loss: 9.4785\nBatch 896/3471, Loss: 8.5271\nBatch 897/3471, Loss: 8.5057\nBatch 898/3471, Loss: 7.9022\nBatch 899/3471, Loss: 5.7105\nBatch 900/3471, Loss: 8.7611\n  M√©dia parcial at√© aqui: 8.4423\nBatch 901/3471, Loss: 8.8192\nBatch 902/3471, Loss: 6.6366\nBatch 903/3471, Loss: 9.3763\nBatch 904/3471, Loss: 8.5796\nBatch 905/3471, Loss: 8.7414\nBatch 906/3471, Loss: 5.9105\nBatch 907/3471, Loss: 7.6369\nBatch 908/3471, Loss: 8.9620\nBatch 909/3471, Loss: 6.6152\nBatch 910/3471, Loss: 9.2499\n  M√©dia parcial at√© aqui: 8.4380\nBatch 911/3471, Loss: 8.5785\nBatch 912/3471, Loss: 8.4526\nBatch 913/3471, Loss: 5.8205\nBatch 914/3471, Loss: 7.3700\nBatch 915/3471, Loss: 8.7572\nBatch 916/3471, Loss: 8.6800\nBatch 917/3471, Loss: 8.2433\nBatch 918/3471, Loss: 7.6388\nBatch 919/3471, Loss: 9.9950\nBatch 920/3471, Loss: 9.3965\n  M√©dia parcial at√© aqui: 8.4364\nBatch 921/3471, Loss: 8.5439\nBatch 922/3471, Loss: 6.5725\nBatch 923/3471, Loss: 8.8597\nBatch 924/3471, Loss: 7.4923\nBatch 925/3471, Loss: 8.1448\nBatch 926/3471, Loss: 8.4683\nBatch 927/3471, Loss: 9.6716\nBatch 928/3471, Loss: 9.3826\nBatch 929/3471, Loss: 8.8018\nBatch 930/3471, Loss: 7.4575\n  M√©dia parcial at√© aqui: 8.4354\nBatch 931/3471, Loss: 8.3700\nBatch 932/3471, Loss: 8.6146\nBatch 933/3471, Loss: 6.1034\nBatch 934/3471, Loss: 7.5694\nBatch 935/3471, Loss: 9.4817\nBatch 936/3471, Loss: 8.4820\nBatch 937/3471, Loss: 9.3257\nBatch 938/3471, Loss: 6.6957\nBatch 939/3471, Loss: 9.1798\nBatch 940/3471, Loss: 8.3170\n  M√©dia parcial at√© aqui: 8.4330\nBatch 941/3471, Loss: 10.0502\nBatch 942/3471, Loss: 7.6119\nBatch 943/3471, Loss: 6.4241\nBatch 944/3471, Loss: 9.6436\nBatch 945/3471, Loss: 10.0726\nBatch 946/3471, Loss: 8.7353\nBatch 947/3471, Loss: 10.1901\nBatch 948/3471, Loss: 9.3607\nBatch 949/3471, Loss: 8.6367\nBatch 950/3471, Loss: 8.7542\n  M√©dia parcial at√© aqui: 8.4384\nBatch 951/3471, Loss: 8.8483\nBatch 952/3471, Loss: 8.3000\nBatch 953/3471, Loss: 9.7327\nBatch 954/3471, Loss: 7.3403\nBatch 955/3471, Loss: 6.5057\nBatch 956/3471, Loss: 8.7091\nBatch 957/3471, Loss: 9.7993\nBatch 958/3471, Loss: 9.1657\nBatch 959/3471, Loss: 9.3262\nBatch 960/3471, Loss: 9.3519\n  M√©dia parcial at√© aqui: 8.4412\nBatch 961/3471, Loss: 10.0284\nBatch 962/3471, Loss: 9.4296\nBatch 963/3471, Loss: 8.8513\nBatch 964/3471, Loss: 9.3995\nBatch 965/3471, Loss: 9.8184\nBatch 966/3471, Loss: 7.4807\nBatch 967/3471, Loss: 9.7892\nBatch 968/3471, Loss: 8.1259\nBatch 969/3471, Loss: 8.5305\nBatch 970/3471, Loss: 9.8945\n  M√©dia parcial at√© aqui: 8.4484\nBatch 971/3471, Loss: 8.8738\nBatch 972/3471, Loss: 9.2477\nBatch 973/3471, Loss: 8.7513\nBatch 974/3471, Loss: 9.9363\nBatch 975/3471, Loss: 8.7158\nBatch 976/3471, Loss: 10.1378\nBatch 977/3471, Loss: 7.5774\nBatch 978/3471, Loss: 7.6050\nBatch 979/3471, Loss: 6.7705\nBatch 980/3471, Loss: 7.5045\n  M√©dia parcial at√© aqui: 8.4490\nBatch 981/3471, Loss: 9.1468\nBatch 982/3471, Loss: 9.7021\nBatch 983/3471, Loss: 8.2985\nBatch 984/3471, Loss: 6.9650\nBatch 985/3471, Loss: 8.6616\nBatch 986/3471, Loss: 9.3215\nBatch 987/3471, Loss: 9.1341\nBatch 988/3471, Loss: 8.5315\nBatch 989/3471, Loss: 9.7370\nBatch 990/3471, Loss: 9.5652\n  M√©dia parcial at√© aqui: 8.4537\nBatch 991/3471, Loss: 7.3902\nBatch 992/3471, Loss: 8.3651\nBatch 993/3471, Loss: 9.7324\nBatch 994/3471, Loss: 9.4249\nBatch 995/3471, Loss: 8.6005\nBatch 996/3471, Loss: 7.8972\nBatch 997/3471, Loss: 8.6470\nBatch 998/3471, Loss: 7.5630\nBatch 999/3471, Loss: 7.0556\nBatch 1000/3471, Loss: 8.8696\n  M√©dia parcial at√© aqui: 8.4527\nBatch 1001/3471, Loss: 7.7857\nBatch 1002/3471, Loss: 6.8542\nBatch 1003/3471, Loss: 5.4583\nBatch 1004/3471, Loss: 10.0010\nBatch 1005/3471, Loss: 8.8923\nBatch 1006/3471, Loss: 9.6508\nBatch 1007/3471, Loss: 8.9693\nBatch 1008/3471, Loss: 8.2746\nBatch 1009/3471, Loss: 8.6270\nBatch 1010/3471, Loss: 8.6737\n  M√©dia parcial at√© aqui: 8.4513\nBatch 1011/3471, Loss: 8.3270\nBatch 1012/3471, Loss: 7.6410\nBatch 1013/3471, Loss: 8.3430\nBatch 1014/3471, Loss: 8.8251\nBatch 1015/3471, Loss: 7.3684\nBatch 1016/3471, Loss: 8.3376\nBatch 1017/3471, Loss: 7.8690\nBatch 1018/3471, Loss: 7.6571\nBatch 1019/3471, Loss: 9.7453\nBatch 1020/3471, Loss: 8.7898\n  M√©dia parcial at√© aqui: 8.4498\nBatch 1021/3471, Loss: 7.7338\nBatch 1022/3471, Loss: 8.5344\nBatch 1023/3471, Loss: 8.8336\nBatch 1024/3471, Loss: 8.5857\nBatch 1025/3471, Loss: 9.2805\nBatch 1026/3471, Loss: 7.5384\nBatch 1027/3471, Loss: 8.8087\nBatch 1028/3471, Loss: 9.0467\nBatch 1029/3471, Loss: 8.4981\nBatch 1030/3471, Loss: 7.3325\n  M√©dia parcial at√© aqui: 8.4495\nBatch 1031/3471, Loss: 8.8057\nBatch 1032/3471, Loss: 8.3085\nBatch 1033/3471, Loss: 8.3685\nBatch 1034/3471, Loss: 7.4081\nBatch 1035/3471, Loss: 9.7375\nBatch 1036/3471, Loss: 6.6478\nBatch 1037/3471, Loss: 7.6162\nBatch 1038/3471, Loss: 9.4448\nBatch 1039/3471, Loss: 8.5008\nBatch 1040/3471, Loss: 7.3699\n  M√©dia parcial at√© aqui: 8.4473\nBatch 1041/3471, Loss: 8.6628\nBatch 1042/3471, Loss: 9.3499\nBatch 1043/3471, Loss: 6.3145\nBatch 1044/3471, Loss: 8.7808\nBatch 1045/3471, Loss: 9.3611\nBatch 1046/3471, Loss: 9.7095\nBatch 1047/3471, Loss: 8.2464\nBatch 1048/3471, Loss: 8.5929\nBatch 1049/3471, Loss: 8.3123\nBatch 1050/3471, Loss: 9.9653\n  M√©dia parcial at√© aqui: 8.4500\nBatch 1051/3471, Loss: 8.6450\nBatch 1052/3471, Loss: 6.6324\nBatch 1053/3471, Loss: 9.4892\nBatch 1054/3471, Loss: 8.4207\nBatch 1055/3471, Loss: 8.6221\nBatch 1056/3471, Loss: 8.6262\nBatch 1057/3471, Loss: 6.4842\nBatch 1058/3471, Loss: 7.5276\nBatch 1059/3471, Loss: 9.3186\nBatch 1060/3471, Loss: 8.8445\n  M√©dia parcial at√© aqui: 8.4482\nBatch 1061/3471, Loss: 9.7755\nBatch 1062/3471, Loss: 9.4192\nBatch 1063/3471, Loss: 8.2404\nBatch 1064/3471, Loss: 8.9690\nBatch 1065/3471, Loss: 8.7700\nBatch 1066/3471, Loss: 8.7923\nBatch 1067/3471, Loss: 8.2907\nBatch 1068/3471, Loss: 6.8725\nBatch 1069/3471, Loss: 8.0225\nBatch 1070/3471, Loss: 6.9097\n  M√©dia parcial at√© aqui: 8.4478\nBatch 1071/3471, Loss: 7.5843\nBatch 1072/3471, Loss: 9.6811\nBatch 1073/3471, Loss: 7.7025\nBatch 1074/3471, Loss: 8.3706\nBatch 1075/3471, Loss: 7.9140\nBatch 1076/3471, Loss: 9.3716\nBatch 1077/3471, Loss: 8.3701\nBatch 1078/3471, Loss: 7.8308\nBatch 1079/3471, Loss: 9.5250\nBatch 1080/3471, Loss: 7.1622\n  M√©dia parcial at√© aqui: 8.4469\nBatch 1081/3471, Loss: 8.6327\nBatch 1082/3471, Loss: 8.7045\nBatch 1083/3471, Loss: 9.2329\nBatch 1084/3471, Loss: 9.7434\nBatch 1085/3471, Loss: 9.4978\nBatch 1086/3471, Loss: 9.0148\nBatch 1087/3471, Loss: 8.5616\nBatch 1088/3471, Loss: 7.7211\nBatch 1089/3471, Loss: 9.4682\nBatch 1090/3471, Loss: 8.5828\n  M√©dia parcial at√© aqui: 8.4512\nBatch 1091/3471, Loss: 7.9930\nBatch 1092/3471, Loss: 9.3218\nBatch 1093/3471, Loss: 7.7678\nBatch 1094/3471, Loss: 8.5183\nBatch 1095/3471, Loss: 6.5332\nBatch 1096/3471, Loss: 8.7857\nBatch 1097/3471, Loss: 8.4834\nBatch 1098/3471, Loss: 7.5391\nBatch 1099/3471, Loss: 9.5605\nBatch 1100/3471, Loss: 9.7384\n  M√©dia parcial at√© aqui: 8.4509\nBatch 1101/3471, Loss: 8.3490\nBatch 1102/3471, Loss: 9.4825\nBatch 1103/3471, Loss: 8.7498\nBatch 1104/3471, Loss: 9.2974\nBatch 1105/3471, Loss: 6.5777\nBatch 1106/3471, Loss: 9.7248\nBatch 1107/3471, Loss: 5.8756\nBatch 1108/3471, Loss: 7.5432\nBatch 1109/3471, Loss: 9.1151\nBatch 1110/3471, Loss: 9.0072\n  M√©dia parcial at√© aqui: 8.4502\nBatch 1111/3471, Loss: 7.3217\nBatch 1112/3471, Loss: 8.7524\nBatch 1113/3471, Loss: 8.8363\nBatch 1114/3471, Loss: 7.5888\nBatch 1115/3471, Loss: 8.2886\nBatch 1116/3471, Loss: 7.5623\nBatch 1117/3471, Loss: 8.2971\nBatch 1118/3471, Loss: 9.1177\nBatch 1119/3471, Loss: 7.6218\nBatch 1120/3471, Loss: 9.3663\n  M√©dia parcial at√© aqui: 8.4487\nBatch 1121/3471, Loss: 7.5758\nBatch 1122/3471, Loss: 6.6533\nBatch 1123/3471, Loss: 6.8841\nBatch 1124/3471, Loss: 7.3977\nBatch 1125/3471, Loss: 9.1121\nBatch 1126/3471, Loss: 6.9995\nBatch 1127/3471, Loss: 7.5430\nBatch 1128/3471, Loss: 8.1115\nBatch 1129/3471, Loss: 9.4069\nBatch 1130/3471, Loss: 7.5281\n  M√©dia parcial at√© aqui: 8.4422\nBatch 1131/3471, Loss: 9.4869\nBatch 1132/3471, Loss: 4.6479\nBatch 1133/3471, Loss: 7.5841\nBatch 1134/3471, Loss: 8.8167\nBatch 1135/3471, Loss: 8.5625\nBatch 1136/3471, Loss: 7.7083\nBatch 1137/3471, Loss: 7.7075\nBatch 1138/3471, Loss: 7.4156\nBatch 1139/3471, Loss: 9.4991\nBatch 1140/3471, Loss: 6.4192\n  M√©dia parcial at√© aqui: 8.4365\nBatch 1141/3471, Loss: 7.1379\nBatch 1142/3471, Loss: 9.6214\nBatch 1143/3471, Loss: 9.6104\nBatch 1144/3471, Loss: 8.4731\nBatch 1145/3471, Loss: 7.9336\nBatch 1146/3471, Loss: 7.6410\nBatch 1147/3471, Loss: 9.8195\nBatch 1148/3471, Loss: 8.7064\nBatch 1149/3471, Loss: 8.0036\nBatch 1150/3471, Loss: 8.8148\n  M√©dia parcial at√© aqui: 8.4377\nBatch 1151/3471, Loss: 9.8737\nBatch 1152/3471, Loss: 8.3414\nBatch 1153/3471, Loss: 8.2543\nBatch 1154/3471, Loss: 9.7883\nBatch 1155/3471, Loss: 9.1501\nBatch 1156/3471, Loss: 10.0159\nBatch 1157/3471, Loss: 7.6393\nBatch 1158/3471, Loss: 9.3948\nBatch 1159/3471, Loss: 9.7685\nBatch 1160/3471, Loss: 9.7423\n  M√©dia parcial at√© aqui: 8.4442\nBatch 1161/3471, Loss: 9.5628\nBatch 1162/3471, Loss: 7.7216\nBatch 1163/3471, Loss: 9.7406\nBatch 1164/3471, Loss: 8.6492\nBatch 1165/3471, Loss: 8.3354\nBatch 1166/3471, Loss: 7.9288\nBatch 1167/3471, Loss: 5.3838\nBatch 1168/3471, Loss: 6.8475\nBatch 1169/3471, Loss: 8.1715\nBatch 1170/3471, Loss: 8.5840\n  M√©dia parcial at√© aqui: 8.4412\nBatch 1171/3471, Loss: 8.7496\nBatch 1172/3471, Loss: 9.8528\nBatch 1173/3471, Loss: 8.3226\nBatch 1174/3471, Loss: 9.3288\nBatch 1175/3471, Loss: 9.5240\nBatch 1176/3471, Loss: 9.7489\nBatch 1177/3471, Loss: 9.4549\nBatch 1178/3471, Loss: 9.7269\nBatch 1179/3471, Loss: 8.8729\nBatch 1180/3471, Loss: 7.6192\n  M√©dia parcial at√© aqui: 8.4470\nBatch 1181/3471, Loss: 8.2433\nBatch 1182/3471, Loss: 8.5547\nBatch 1183/3471, Loss: 9.2383\nBatch 1184/3471, Loss: 8.9195\nBatch 1185/3471, Loss: 8.4254\nBatch 1186/3471, Loss: 9.9562\nBatch 1187/3471, Loss: 8.4376\nBatch 1188/3471, Loss: 9.5214\nBatch 1189/3471, Loss: 7.3655\nBatch 1190/3471, Loss: 8.8292\n  M√©dia parcial at√© aqui: 8.4495\nBatch 1191/3471, Loss: 9.8192\nBatch 1192/3471, Loss: 8.2203\nBatch 1193/3471, Loss: 9.7268\nBatch 1194/3471, Loss: 8.6537\nBatch 1195/3471, Loss: 9.2489\nBatch 1196/3471, Loss: 9.2047\nBatch 1197/3471, Loss: 7.5064\nBatch 1198/3471, Loss: 8.4252\nBatch 1199/3471, Loss: 7.5297\nBatch 1200/3471, Loss: 9.5218\n  M√©dia parcial at√© aqui: 8.4523\nBatch 1201/3471, Loss: 9.1374\nBatch 1202/3471, Loss: 7.7112\nBatch 1203/3471, Loss: 8.4380\nBatch 1204/3471, Loss: 8.7436\nBatch 1205/3471, Loss: 8.7625\nBatch 1206/3471, Loss: 8.5264\nBatch 1207/3471, Loss: 7.9855\nBatch 1208/3471, Loss: 7.4496\nBatch 1209/3471, Loss: 7.3353\nBatch 1210/3471, Loss: 6.9427\n  M√©dia parcial at√© aqui: 8.4494\nBatch 1211/3471, Loss: 8.3898\nBatch 1212/3471, Loss: 6.3669\nBatch 1213/3471, Loss: 8.8576\nBatch 1214/3471, Loss: 8.2277\nBatch 1215/3471, Loss: 9.2721\nBatch 1216/3471, Loss: 9.6005\nBatch 1217/3471, Loss: 6.9511\nBatch 1218/3471, Loss: 8.2528\nBatch 1219/3471, Loss: 7.8070\nBatch 1220/3471, Loss: 8.8128\n  M√©dia parcial at√© aqui: 8.4478\nBatch 1221/3471, Loss: 6.9803\nBatch 1222/3471, Loss: 8.9887\nBatch 1223/3471, Loss: 9.2521\nBatch 1224/3471, Loss: 7.9854\nBatch 1225/3471, Loss: 9.4116\nBatch 1226/3471, Loss: 7.2185\nBatch 1227/3471, Loss: 8.3276\nBatch 1228/3471, Loss: 7.4098\nBatch 1229/3471, Loss: 7.5782\nBatch 1230/3471, Loss: 8.9355\n  M√©dia parcial at√© aqui: 8.4459\nBatch 1231/3471, Loss: 9.9586\nBatch 1232/3471, Loss: 8.6020\nBatch 1233/3471, Loss: 9.5007\nBatch 1234/3471, Loss: 9.4956\nBatch 1235/3471, Loss: 8.2727\nBatch 1236/3471, Loss: 8.2489\nBatch 1237/3471, Loss: 8.8318\nBatch 1238/3471, Loss: 8.5626\nBatch 1239/3471, Loss: 8.8392\nBatch 1240/3471, Loss: 10.3584\n  M√©dia parcial at√© aqui: 8.4509\nBatch 1241/3471, Loss: 8.6644\nBatch 1242/3471, Loss: 7.9545\nBatch 1243/3471, Loss: 10.0084\nBatch 1244/3471, Loss: 8.7475\nBatch 1245/3471, Loss: 9.4074\nBatch 1246/3471, Loss: 8.2974\nBatch 1247/3471, Loss: 7.8806\nBatch 1248/3471, Loss: 9.5229\nBatch 1249/3471, Loss: 5.6595\nBatch 1250/3471, Loss: 7.4784\n  M√©dia parcial at√© aqui: 8.4502\nBatch 1251/3471, Loss: 8.6677\nBatch 1252/3471, Loss: 8.4653\nBatch 1253/3471, Loss: 7.7610\nBatch 1254/3471, Loss: 8.8296\nBatch 1255/3471, Loss: 8.5135\nBatch 1256/3471, Loss: 9.7265\nBatch 1257/3471, Loss: 8.7824\nBatch 1258/3471, Loss: 9.2657\nBatch 1259/3471, Loss: 8.5111\nBatch 1260/3471, Loss: 7.3779\n  M√©dia parcial at√© aqui: 8.4513\nBatch 1261/3471, Loss: 7.7517\nBatch 1262/3471, Loss: 9.5619\nBatch 1263/3471, Loss: 9.3975\nBatch 1264/3471, Loss: 9.4234\nBatch 1265/3471, Loss: 7.3364\nBatch 1266/3471, Loss: 9.0155\nBatch 1267/3471, Loss: 8.5400\nBatch 1268/3471, Loss: 9.5610\nBatch 1269/3471, Loss: 8.5151\nBatch 1270/3471, Loss: 8.4418\n  M√©dia parcial at√© aqui: 8.4537\nBatch 1271/3471, Loss: 8.7815\nBatch 1272/3471, Loss: 7.4083\nBatch 1273/3471, Loss: 8.1809\nBatch 1274/3471, Loss: 8.1312\nBatch 1275/3471, Loss: 9.9017\nBatch 1276/3471, Loss: 7.3162\nBatch 1277/3471, Loss: 8.0819\nBatch 1278/3471, Loss: 9.0855\nBatch 1279/3471, Loss: 8.6725\nBatch 1280/3471, Loss: 7.9771\n  M√©dia parcial at√© aqui: 8.4529\nBatch 1281/3471, Loss: 9.5702\nBatch 1282/3471, Loss: 7.6615\nBatch 1283/3471, Loss: 9.6935\nBatch 1284/3471, Loss: 9.5237\nBatch 1285/3471, Loss: 8.8467\nBatch 1286/3471, Loss: 7.5877\nBatch 1287/3471, Loss: 9.6565\nBatch 1288/3471, Loss: 7.6970\nBatch 1289/3471, Loss: 6.3104\nBatch 1290/3471, Loss: 7.8650\n  M√©dia parcial at√© aqui: 8.4528\nBatch 1291/3471, Loss: 7.4590\nBatch 1292/3471, Loss: 7.2749\nBatch 1293/3471, Loss: 6.6631\nBatch 1294/3471, Loss: 8.7597\nBatch 1295/3471, Loss: 7.0688\nBatch 1296/3471, Loss: 6.5501\nBatch 1297/3471, Loss: 9.3085\nBatch 1298/3471, Loss: 9.4428\nBatch 1299/3471, Loss: 9.4743\nBatch 1300/3471, Loss: 7.8183\n  M√©dia parcial at√© aqui: 8.4492\nBatch 1301/3471, Loss: 7.6609\nBatch 1302/3471, Loss: 9.4946\nBatch 1303/3471, Loss: 9.9771\nBatch 1304/3471, Loss: 9.7146\nBatch 1305/3471, Loss: 9.5697\nBatch 1306/3471, Loss: 7.5282\nBatch 1307/3471, Loss: 7.9582\nBatch 1308/3471, Loss: 9.7553\nBatch 1309/3471, Loss: 8.8292\nBatch 1310/3471, Loss: 8.4601\n  M√©dia parcial at√© aqui: 8.4526\nBatch 1311/3471, Loss: 5.9094\nBatch 1312/3471, Loss: 8.4662\nBatch 1313/3471, Loss: 8.6754\nBatch 1314/3471, Loss: 7.9825\nBatch 1315/3471, Loss: 8.6683\nBatch 1316/3471, Loss: 9.7193\nBatch 1317/3471, Loss: 7.8258\nBatch 1318/3471, Loss: 7.7438\nBatch 1319/3471, Loss: 8.5624\nBatch 1320/3471, Loss: 7.8860\n  M√©dia parcial at√© aqui: 8.4502\nBatch 1321/3471, Loss: 10.0702\nBatch 1322/3471, Loss: 9.9373\nBatch 1323/3471, Loss: 8.8841\nBatch 1324/3471, Loss: 9.0981\nBatch 1325/3471, Loss: 9.4295\nBatch 1326/3471, Loss: 7.5858\nBatch 1327/3471, Loss: 9.6329\nBatch 1328/3471, Loss: 6.5437\nBatch 1329/3471, Loss: 7.3077\nBatch 1330/3471, Loss: 7.8319\n  M√©dia parcial at√© aqui: 8.4516\nBatch 1331/3471, Loss: 9.0299\nBatch 1332/3471, Loss: 9.4512\nBatch 1333/3471, Loss: 8.3589\nBatch 1334/3471, Loss: 7.5284\nBatch 1335/3471, Loss: 8.4537\nBatch 1336/3471, Loss: 8.6450\nBatch 1337/3471, Loss: 8.5490\nBatch 1338/3471, Loss: 8.4226\nBatch 1339/3471, Loss: 9.6800\nBatch 1340/3471, Loss: 9.6234\n  M√©dia parcial at√© aqui: 8.4540\nBatch 1341/3471, Loss: 6.6307\nBatch 1342/3471, Loss: 8.4927\nBatch 1343/3471, Loss: 9.4143\nBatch 1344/3471, Loss: 8.7294\nBatch 1345/3471, Loss: 7.6961\nBatch 1346/3471, Loss: 8.4605\nBatch 1347/3471, Loss: 6.9828\nBatch 1348/3471, Loss: 8.6417\nBatch 1349/3471, Loss: 7.5238\nBatch 1350/3471, Loss: 8.5930\n  M√©dia parcial at√© aqui: 8.4515\nBatch 1351/3471, Loss: 8.6344\nBatch 1352/3471, Loss: 8.3569\nBatch 1353/3471, Loss: 7.0661\nBatch 1354/3471, Loss: 9.5080\nBatch 1355/3471, Loss: 6.9110\nBatch 1356/3471, Loss: 8.1192\nBatch 1357/3471, Loss: 8.8941\nBatch 1358/3471, Loss: 8.7697\nBatch 1359/3471, Loss: 7.7239\nBatch 1360/3471, Loss: 8.7604\n  M√©dia parcial at√© aqui: 8.4502\nBatch 1361/3471, Loss: 9.5687\nBatch 1362/3471, Loss: 9.3241\nBatch 1363/3471, Loss: 8.4708\nBatch 1364/3471, Loss: 9.3880\nBatch 1365/3471, Loss: 9.2561\nBatch 1366/3471, Loss: 9.2906\nBatch 1367/3471, Loss: 8.5385\nBatch 1368/3471, Loss: 9.2866\nBatch 1369/3471, Loss: 9.6056\nBatch 1370/3471, Loss: 8.7543\n  M√©dia parcial at√© aqui: 8.4553\nBatch 1371/3471, Loss: 7.5939\nBatch 1372/3471, Loss: 8.5304\nBatch 1373/3471, Loss: 7.8573\nBatch 1374/3471, Loss: 7.2409\nBatch 1375/3471, Loss: 7.7622\nBatch 1376/3471, Loss: 8.5359\nBatch 1377/3471, Loss: 8.4835\nBatch 1378/3471, Loss: 6.8666\nBatch 1379/3471, Loss: 6.8036\nBatch 1380/3471, Loss: 9.2325\n  M√©dia parcial at√© aqui: 8.4512\nBatch 1381/3471, Loss: 6.8436\nBatch 1382/3471, Loss: 8.3975\nBatch 1383/3471, Loss: 8.3652\nBatch 1384/3471, Loss: 9.6953\nBatch 1385/3471, Loss: 8.4938\nBatch 1386/3471, Loss: 9.0627\nBatch 1387/3471, Loss: 6.3480\nBatch 1388/3471, Loss: 8.4899\nBatch 1389/3471, Loss: 7.5614\nBatch 1390/3471, Loss: 9.1071\n  M√©dia parcial at√© aqui: 8.4497\nBatch 1391/3471, Loss: 7.7625\nBatch 1392/3471, Loss: 8.3036\nBatch 1393/3471, Loss: 8.4666\nBatch 1394/3471, Loss: 7.5416\nBatch 1395/3471, Loss: 7.7878\nBatch 1396/3471, Loss: 10.0448\nBatch 1397/3471, Loss: 8.4536\nBatch 1398/3471, Loss: 9.8239\nBatch 1399/3471, Loss: 9.2384\nBatch 1400/3471, Loss: 9.3936\n  M√©dia parcial at√© aqui: 8.4513\nBatch 1401/3471, Loss: 8.3825\nBatch 1402/3471, Loss: 7.6830\nBatch 1403/3471, Loss: 7.3381\nBatch 1404/3471, Loss: 8.7716\nBatch 1405/3471, Loss: 9.0763\nBatch 1406/3471, Loss: 8.5062\nBatch 1407/3471, Loss: 5.8166\nBatch 1408/3471, Loss: 7.9405\nBatch 1409/3471, Loss: 8.2825\nBatch 1410/3471, Loss: 9.7960\n  M√©dia parcial at√© aqui: 8.4493\nBatch 1411/3471, Loss: 8.9182\nBatch 1412/3471, Loss: 7.6632\nBatch 1413/3471, Loss: 8.6959\nBatch 1414/3471, Loss: 7.7022\nBatch 1415/3471, Loss: 7.8041\nBatch 1416/3471, Loss: 8.3685\nBatch 1417/3471, Loss: 8.4098\nBatch 1418/3471, Loss: 9.6376\nBatch 1419/3471, Loss: 9.6309\nBatch 1420/3471, Loss: 8.3384\n  M√©dia parcial at√© aqui: 8.4497\nBatch 1421/3471, Loss: 7.9205\nBatch 1422/3471, Loss: 7.6290\nBatch 1423/3471, Loss: 8.6980\nBatch 1424/3471, Loss: 9.8621\nBatch 1425/3471, Loss: 10.0730\nBatch 1426/3471, Loss: 8.1486\nBatch 1427/3471, Loss: 10.1793\nBatch 1428/3471, Loss: 8.4615\nBatch 1429/3471, Loss: 8.3056\nBatch 1430/3471, Loss: 8.7249\n  M√©dia parcial at√© aqui: 8.4522\nBatch 1431/3471, Loss: 7.7641\nBatch 1432/3471, Loss: 8.8817\nBatch 1433/3471, Loss: 7.6596\nBatch 1434/3471, Loss: 9.4857\nBatch 1435/3471, Loss: 8.2044\nBatch 1436/3471, Loss: 7.8833\nBatch 1437/3471, Loss: 9.5102\nBatch 1438/3471, Loss: 8.2179\nBatch 1439/3471, Loss: 8.5953\nBatch 1440/3471, Loss: 9.5671\n  M√©dia parcial at√© aqui: 8.4531\nBatch 1441/3471, Loss: 9.6975\nBatch 1442/3471, Loss: 9.5775\nBatch 1443/3471, Loss: 8.6507\nBatch 1444/3471, Loss: 5.7845\nBatch 1445/3471, Loss: 6.5975\nBatch 1446/3471, Loss: 8.7108\nBatch 1447/3471, Loss: 8.6042\nBatch 1448/3471, Loss: 9.2292\nBatch 1449/3471, Loss: 9.2767\nBatch 1450/3471, Loss: 7.6987\n  M√©dia parcial at√© aqui: 8.4526\nBatch 1451/3471, Loss: 8.9106\nBatch 1452/3471, Loss: 7.6122\nBatch 1453/3471, Loss: 9.8280\nBatch 1454/3471, Loss: 9.3409\nBatch 1455/3471, Loss: 7.1418\nBatch 1456/3471, Loss: 8.9407\nBatch 1457/3471, Loss: 8.7031\nBatch 1458/3471, Loss: 8.6538\nBatch 1459/3471, Loss: 10.0592\nBatch 1460/3471, Loss: 9.6663\n  M√©dia parcial at√© aqui: 8.4555\nBatch 1461/3471, Loss: 8.2570\nBatch 1462/3471, Loss: 8.2812\nBatch 1463/3471, Loss: 9.6809\nBatch 1464/3471, Loss: 5.6015\nBatch 1465/3471, Loss: 7.7936\nBatch 1466/3471, Loss: 9.1791\nBatch 1467/3471, Loss: 8.6415\nBatch 1468/3471, Loss: 8.4038\nBatch 1469/3471, Loss: 8.0339\nBatch 1470/3471, Loss: 9.6057\n  M√©dia parcial at√© aqui: 8.4548\nBatch 1471/3471, Loss: 9.7558\nBatch 1472/3471, Loss: 7.9779\nBatch 1473/3471, Loss: 8.2253\nBatch 1474/3471, Loss: 8.7874\nBatch 1475/3471, Loss: 8.9434\nBatch 1476/3471, Loss: 6.5744\nBatch 1477/3471, Loss: 6.7084\nBatch 1478/3471, Loss: 8.2510\nBatch 1479/3471, Loss: 8.2613\nBatch 1480/3471, Loss: 8.6445\n  M√©dia parcial at√© aqui: 8.4532\nBatch 1481/3471, Loss: 7.4814\nBatch 1482/3471, Loss: 8.6029\nBatch 1483/3471, Loss: 8.7425\nBatch 1484/3471, Loss: 9.9150\nBatch 1485/3471, Loss: 6.6593\nBatch 1486/3471, Loss: 8.2850\nBatch 1487/3471, Loss: 8.3909\nBatch 1488/3471, Loss: 8.1187\nBatch 1489/3471, Loss: 6.8493\nBatch 1490/3471, Loss: 6.5788\n  M√©dia parcial at√© aqui: 8.4499\nBatch 1491/3471, Loss: 8.5656\nBatch 1492/3471, Loss: 9.6063\nBatch 1493/3471, Loss: 7.7977\nBatch 1494/3471, Loss: 8.8285\nBatch 1495/3471, Loss: 7.0869\nBatch 1496/3471, Loss: 9.5545\nBatch 1497/3471, Loss: 8.3710\nBatch 1498/3471, Loss: 9.6629\nBatch 1499/3471, Loss: 9.5395\nBatch 1500/3471, Loss: 8.9936\n  M√©dia parcial at√© aqui: 8.4522\nBatch 1501/3471, Loss: 9.5144\nBatch 1502/3471, Loss: 8.7346\nBatch 1503/3471, Loss: 8.6132\nBatch 1504/3471, Loss: 9.0968\nBatch 1505/3471, Loss: 7.7255\nBatch 1506/3471, Loss: 9.6193\nBatch 1507/3471, Loss: 8.6833\nBatch 1508/3471, Loss: 7.8823\nBatch 1509/3471, Loss: 8.6547\nBatch 1510/3471, Loss: 6.6498\n  M√©dia parcial at√© aqui: 8.4526\nBatch 1511/3471, Loss: 9.6219\nBatch 1512/3471, Loss: 10.0380\nBatch 1513/3471, Loss: 6.8728\nBatch 1514/3471, Loss: 8.7157\nBatch 1515/3471, Loss: 7.7193\nBatch 1516/3471, Loss: 9.7463\nBatch 1517/3471, Loss: 6.4565\nBatch 1518/3471, Loss: 6.6589\nBatch 1519/3471, Loss: 9.8224\nBatch 1520/3471, Loss: 7.8248\n  M√©dia parcial at√© aqui: 8.4520\nBatch 1521/3471, Loss: 9.7135\nBatch 1522/3471, Loss: 8.5586\nBatch 1523/3471, Loss: 8.8882\nBatch 1524/3471, Loss: 7.6978\nBatch 1525/3471, Loss: 7.6374\nBatch 1526/3471, Loss: 9.5280\nBatch 1527/3471, Loss: 9.1615\nBatch 1528/3471, Loss: 8.4334\nBatch 1529/3471, Loss: 8.2997\nBatch 1530/3471, Loss: 5.7221\n  M√©dia parcial at√© aqui: 8.4514\nBatch 1531/3471, Loss: 9.5774\nBatch 1532/3471, Loss: 8.6453\nBatch 1533/3471, Loss: 7.7242\nBatch 1534/3471, Loss: 6.7276\nBatch 1535/3471, Loss: 6.5960\nBatch 1536/3471, Loss: 9.3579\nBatch 1537/3471, Loss: 9.2934\nBatch 1538/3471, Loss: 9.1162\nBatch 1539/3471, Loss: 9.7405\nBatch 1540/3471, Loss: 8.5294\n  M√©dia parcial at√© aqui: 8.4519\nBatch 1541/3471, Loss: 8.4598\nBatch 1542/3471, Loss: 9.9727\nBatch 1543/3471, Loss: 9.9541\nBatch 1544/3471, Loss: 7.5932\nBatch 1545/3471, Loss: 7.8545\nBatch 1546/3471, Loss: 8.9359\nBatch 1547/3471, Loss: 8.5616\nBatch 1548/3471, Loss: 9.7855\nBatch 1549/3471, Loss: 8.1576\nBatch 1550/3471, Loss: 8.1020\n  M√©dia parcial at√© aqui: 8.4537\nBatch 1551/3471, Loss: 8.9009\nBatch 1552/3471, Loss: 9.1716\nBatch 1553/3471, Loss: 7.3599\nBatch 1554/3471, Loss: 6.9207\nBatch 1555/3471, Loss: 8.5667\nBatch 1556/3471, Loss: 6.4392\nBatch 1557/3471, Loss: 5.7728\nBatch 1558/3471, Loss: 9.5049\nBatch 1559/3471, Loss: 9.8541\nBatch 1560/3471, Loss: 7.6621\n  M√©dia parcial at√© aqui: 8.4509\nBatch 1561/3471, Loss: 7.4975\nBatch 1562/3471, Loss: 6.1450\nBatch 1563/3471, Loss: 6.7776\nBatch 1564/3471, Loss: 8.4174\nBatch 1565/3471, Loss: 7.5042\nBatch 1566/3471, Loss: 9.7333\nBatch 1567/3471, Loss: 9.2647\nBatch 1568/3471, Loss: 7.5292\nBatch 1569/3471, Loss: 7.7463\nBatch 1570/3471, Loss: 8.6098\n  M√©dia parcial at√© aqui: 8.4476\nBatch 1571/3471, Loss: 9.7263\nBatch 1572/3471, Loss: 6.8051\nBatch 1573/3471, Loss: 9.5092\nBatch 1574/3471, Loss: 8.9867\nBatch 1575/3471, Loss: 8.2996\nBatch 1576/3471, Loss: 9.3638\nBatch 1577/3471, Loss: 7.6760\nBatch 1578/3471, Loss: 9.7133\nBatch 1579/3471, Loss: 7.3285\nBatch 1580/3471, Loss: 8.5264\n  M√©dia parcial at√© aqui: 8.4485\nBatch 1581/3471, Loss: 7.4858\nBatch 1582/3471, Loss: 9.7133\nBatch 1583/3471, Loss: 8.7183\nBatch 1584/3471, Loss: 6.8335\nBatch 1585/3471, Loss: 7.8392\nBatch 1586/3471, Loss: 9.3369\nBatch 1587/3471, Loss: 6.8010\nBatch 1588/3471, Loss: 8.7594\nBatch 1589/3471, Loss: 8.5933\nBatch 1590/3471, Loss: 7.8750\n  M√©dia parcial at√© aqui: 8.4469\nBatch 1591/3471, Loss: 9.9211\nBatch 1592/3471, Loss: 9.1849\nBatch 1593/3471, Loss: 7.7007\nBatch 1594/3471, Loss: 7.7350\nBatch 1595/3471, Loss: 8.9238\nBatch 1596/3471, Loss: 10.4946\nBatch 1597/3471, Loss: 8.5353\nBatch 1598/3471, Loss: 9.8861\nBatch 1599/3471, Loss: 8.4352\nBatch 1600/3471, Loss: 8.5441\n  M√©dia parcial at√© aqui: 8.4500\nBatch 1601/3471, Loss: 8.4490\nBatch 1602/3471, Loss: 8.8042\nBatch 1603/3471, Loss: 9.8557\nBatch 1604/3471, Loss: 6.7292\nBatch 1605/3471, Loss: 9.1433\nBatch 1606/3471, Loss: 9.5914\nBatch 1607/3471, Loss: 9.2266\nBatch 1608/3471, Loss: 8.9550\nBatch 1609/3471, Loss: 8.2804\nBatch 1610/3471, Loss: 8.8723\n  M√©dia parcial at√© aqui: 8.4521\nBatch 1611/3471, Loss: 7.7558\nBatch 1612/3471, Loss: 8.9651\nBatch 1613/3471, Loss: 7.5864\nBatch 1614/3471, Loss: 8.7420\nBatch 1615/3471, Loss: 9.5898\nBatch 1616/3471, Loss: 9.1098\nBatch 1617/3471, Loss: 8.6561\nBatch 1618/3471, Loss: 9.2709\nBatch 1619/3471, Loss: 4.9236\nBatch 1620/3471, Loss: 9.4164\n  M√©dia parcial at√© aqui: 8.4518\nBatch 1621/3471, Loss: 8.7183\nBatch 1622/3471, Loss: 6.8997\nBatch 1623/3471, Loss: 9.4792\nBatch 1624/3471, Loss: 8.2633\nBatch 1625/3471, Loss: 9.6530\nBatch 1626/3471, Loss: 8.3176\nBatch 1627/3471, Loss: 8.4785\nBatch 1628/3471, Loss: 8.5115\nBatch 1629/3471, Loss: 7.1250\nBatch 1630/3471, Loss: 7.6129\n  M√©dia parcial at√© aqui: 8.4509\nBatch 1631/3471, Loss: 8.5779\nBatch 1632/3471, Loss: 9.1140\nBatch 1633/3471, Loss: 5.3933\nBatch 1634/3471, Loss: 8.6705\nBatch 1635/3471, Loss: 7.6684\nBatch 1636/3471, Loss: 9.4779\nBatch 1637/3471, Loss: 8.9071\nBatch 1638/3471, Loss: 6.4776\nBatch 1639/3471, Loss: 8.5407\nBatch 1640/3471, Loss: 8.5400\n  M√©dia parcial at√© aqui: 8.4489\nBatch 1641/3471, Loss: 9.7824\nBatch 1642/3471, Loss: 9.0976\nBatch 1643/3471, Loss: 6.8410\nBatch 1644/3471, Loss: 8.2749\nBatch 1645/3471, Loss: 8.5994\nBatch 1646/3471, Loss: 9.5684\nBatch 1647/3471, Loss: 8.5935\nBatch 1648/3471, Loss: 7.4401\nBatch 1649/3471, Loss: 8.6972\nBatch 1650/3471, Loss: 9.8556\n  M√©dia parcial at√© aqui: 8.4503\nBatch 1651/3471, Loss: 8.8546\nBatch 1652/3471, Loss: 8.2742\nBatch 1653/3471, Loss: 8.1772\nBatch 1654/3471, Loss: 8.1843\nBatch 1655/3471, Loss: 8.3229\nBatch 1656/3471, Loss: 9.8628\nBatch 1657/3471, Loss: 8.7965\nBatch 1658/3471, Loss: 8.9505\nBatch 1659/3471, Loss: 8.9973\nBatch 1660/3471, Loss: 9.7178\n  M√©dia parcial at√© aqui: 8.4525\nBatch 1661/3471, Loss: 9.7581\nBatch 1662/3471, Loss: 7.4399\nBatch 1663/3471, Loss: 7.6666\nBatch 1664/3471, Loss: 9.5765\nBatch 1665/3471, Loss: 7.9099\nBatch 1666/3471, Loss: 9.8826\nBatch 1667/3471, Loss: 8.2841\nBatch 1668/3471, Loss: 9.1325\nBatch 1669/3471, Loss: 7.9105\nBatch 1670/3471, Loss: 8.5931\n  M√©dia parcial at√© aqui: 8.4535\nBatch 1671/3471, Loss: 10.0002\nBatch 1672/3471, Loss: 7.3448\nBatch 1673/3471, Loss: 7.9347\nBatch 1674/3471, Loss: 9.6917\nBatch 1675/3471, Loss: 8.1158\nBatch 1676/3471, Loss: 8.3503\nBatch 1677/3471, Loss: 7.9925\nBatch 1678/3471, Loss: 8.3473\nBatch 1679/3471, Loss: 9.8781\nBatch 1680/3471, Loss: 9.4895\n  M√©dia parcial at√© aqui: 8.4550\nBatch 1681/3471, Loss: 5.6287\nBatch 1682/3471, Loss: 8.7765\nBatch 1683/3471, Loss: 7.7547\nBatch 1684/3471, Loss: 8.3718\nBatch 1685/3471, Loss: 8.4204\nBatch 1686/3471, Loss: 9.4415\nBatch 1687/3471, Loss: 9.4592\nBatch 1688/3471, Loss: 9.5071\nBatch 1689/3471, Loss: 8.6753\nBatch 1690/3471, Loss: 8.5233\n  M√©dia parcial at√© aqui: 8.4550\nBatch 1691/3471, Loss: 8.6453\nBatch 1692/3471, Loss: 7.6958\nBatch 1693/3471, Loss: 8.9273\nBatch 1694/3471, Loss: 7.7462\nBatch 1695/3471, Loss: 8.5811\nBatch 1696/3471, Loss: 7.9670\nBatch 1697/3471, Loss: 7.7576\nBatch 1698/3471, Loss: 6.3083\nBatch 1699/3471, Loss: 8.9597\nBatch 1700/3471, Loss: 9.5851\n  M√©dia parcial at√© aqui: 8.4536\nBatch 1701/3471, Loss: 4.6046\nBatch 1702/3471, Loss: 6.9992\nBatch 1703/3471, Loss: 9.6398\nBatch 1704/3471, Loss: 10.2581\nBatch 1705/3471, Loss: 7.5438\nBatch 1706/3471, Loss: 9.3639\nBatch 1707/3471, Loss: 9.6864\nBatch 1708/3471, Loss: 9.4605\nBatch 1709/3471, Loss: 8.7393\nBatch 1710/3471, Loss: 8.4994\n  M√©dia parcial at√© aqui: 8.4538\nBatch 1711/3471, Loss: 7.6833\nBatch 1712/3471, Loss: 7.0324\nBatch 1713/3471, Loss: 5.9490\nBatch 1714/3471, Loss: 9.4564\nBatch 1715/3471, Loss: 9.5278\nBatch 1716/3471, Loss: 9.3382\nBatch 1717/3471, Loss: 9.5336\nBatch 1718/3471, Loss: 9.1557\nBatch 1719/3471, Loss: 7.7103\nBatch 1720/3471, Loss: 8.6601\n  M√©dia parcial at√© aqui: 8.4535\nBatch 1721/3471, Loss: 8.8118\nBatch 1722/3471, Loss: 8.7697\nBatch 1723/3471, Loss: 7.8400\nBatch 1724/3471, Loss: 6.7960\nBatch 1725/3471, Loss: 6.6524\nBatch 1726/3471, Loss: 8.5374\nBatch 1727/3471, Loss: 8.4916\nBatch 1728/3471, Loss: 9.4677\nBatch 1729/3471, Loss: 7.9277\nBatch 1730/3471, Loss: 7.7612\n  M√©dia parcial at√© aqui: 8.4515\nBatch 1731/3471, Loss: 8.7316\nBatch 1732/3471, Loss: 7.9790\nBatch 1733/3471, Loss: 8.7797\nBatch 1734/3471, Loss: 9.8361\nBatch 1735/3471, Loss: 7.5758\nBatch 1736/3471, Loss: 10.2847\nBatch 1737/3471, Loss: 8.5805\nBatch 1738/3471, Loss: 7.7826\nBatch 1739/3471, Loss: 8.8329\nBatch 1740/3471, Loss: 9.3457\n  M√©dia parcial at√© aqui: 8.4533\nBatch 1741/3471, Loss: 9.5326\nBatch 1742/3471, Loss: 8.8836\nBatch 1743/3471, Loss: 9.7291\nBatch 1744/3471, Loss: 9.2602\nBatch 1745/3471, Loss: 7.6606\nBatch 1746/3471, Loss: 8.3547\nBatch 1747/3471, Loss: 6.1308\nBatch 1748/3471, Loss: 7.3477\nBatch 1749/3471, Loss: 7.0922\nBatch 1750/3471, Loss: 8.4955\n  M√©dia parcial at√© aqui: 8.4522\nBatch 1751/3471, Loss: 7.7523\nBatch 1752/3471, Loss: 8.5438\nBatch 1753/3471, Loss: 6.5101\nBatch 1754/3471, Loss: 8.6967\nBatch 1755/3471, Loss: 8.4609\nBatch 1756/3471, Loss: 8.9903\nBatch 1757/3471, Loss: 9.7288\nBatch 1758/3471, Loss: 7.6946\nBatch 1759/3471, Loss: 6.6319\nBatch 1760/3471, Loss: 7.5904\n  M√©dia parcial at√© aqui: 8.4499\nBatch 1761/3471, Loss: 8.6480\nBatch 1762/3471, Loss: 7.4814\nBatch 1763/3471, Loss: 7.5508\nBatch 1764/3471, Loss: 9.5830\nBatch 1765/3471, Loss: 9.8110\nBatch 1766/3471, Loss: 6.5992\nBatch 1767/3471, Loss: 9.3729\nBatch 1768/3471, Loss: 7.9280\nBatch 1769/3471, Loss: 7.3874\nBatch 1770/3471, Loss: 8.8410\n  M√©dia parcial at√© aqui: 8.4492\nBatch 1771/3471, Loss: 6.5077\nBatch 1772/3471, Loss: 9.9918\nBatch 1773/3471, Loss: 8.5099\nBatch 1774/3471, Loss: 7.9913\nBatch 1775/3471, Loss: 9.7193\nBatch 1776/3471, Loss: 8.5495\nBatch 1777/3471, Loss: 9.8929\nBatch 1778/3471, Loss: 7.3807\nBatch 1779/3471, Loss: 8.5233\nBatch 1780/3471, Loss: 8.2570\n  M√©dia parcial at√© aqui: 8.4497\nBatch 1781/3471, Loss: 7.9294\nBatch 1782/3471, Loss: 7.6514\nBatch 1783/3471, Loss: 7.4552\nBatch 1784/3471, Loss: 8.3610\nBatch 1785/3471, Loss: 8.5717\nBatch 1786/3471, Loss: 8.3940\nBatch 1787/3471, Loss: 7.1155\nBatch 1788/3471, Loss: 7.7594\nBatch 1789/3471, Loss: 7.3762\nBatch 1790/3471, Loss: 9.1293\n  M√©dia parcial at√© aqui: 8.4470\nBatch 1791/3471, Loss: 7.5253\nBatch 1792/3471, Loss: 9.8584\nBatch 1793/3471, Loss: 7.4598\nBatch 1794/3471, Loss: 8.7110\nBatch 1795/3471, Loss: 9.5658\nBatch 1796/3471, Loss: 7.9485\nBatch 1797/3471, Loss: 8.6399\nBatch 1798/3471, Loss: 6.6546\nBatch 1799/3471, Loss: 9.3673\nBatch 1800/3471, Loss: 9.8401\n  M√©dia parcial at√© aqui: 8.4476\nBatch 1801/3471, Loss: 7.6819\nBatch 1802/3471, Loss: 6.7859\nBatch 1803/3471, Loss: 8.9782\nBatch 1804/3471, Loss: 9.5961\nBatch 1805/3471, Loss: 8.8750\nBatch 1806/3471, Loss: 9.9742\nBatch 1807/3471, Loss: 5.7873\nBatch 1808/3471, Loss: 8.5201\nBatch 1809/3471, Loss: 7.5925\nBatch 1810/3471, Loss: 9.3900\n  M√©dia parcial at√© aqui: 8.4469\nBatch 1811/3471, Loss: 8.8229\nBatch 1812/3471, Loss: 8.7618\nBatch 1813/3471, Loss: 9.3553\nBatch 1814/3471, Loss: 7.7760\nBatch 1815/3471, Loss: 8.7889\nBatch 1816/3471, Loss: 8.0108\nBatch 1817/3471, Loss: 6.0334\nBatch 1818/3471, Loss: 7.3966\nBatch 1819/3471, Loss: 9.7677\nBatch 1820/3471, Loss: 9.7601\n  M√©dia parcial at√© aqui: 8.4469\nBatch 1821/3471, Loss: 9.0901\nBatch 1822/3471, Loss: 8.6766\nBatch 1823/3471, Loss: 10.2295\nBatch 1824/3471, Loss: 9.5908\nBatch 1825/3471, Loss: 8.7321\nBatch 1826/3471, Loss: 9.5947\nBatch 1827/3471, Loss: 8.5787\nBatch 1828/3471, Loss: 10.0306\nBatch 1829/3471, Loss: 8.8858\nBatch 1830/3471, Loss: 7.1107\n  M√©dia parcial at√© aqui: 8.4502\nBatch 1831/3471, Loss: 9.5886\nBatch 1832/3471, Loss: 8.9241\nBatch 1833/3471, Loss: 7.5508\nBatch 1834/3471, Loss: 8.2977\nBatch 1835/3471, Loss: 9.5814\nBatch 1836/3471, Loss: 9.8733\nBatch 1837/3471, Loss: 8.4668\nBatch 1838/3471, Loss: 8.4286\nBatch 1839/3471, Loss: 8.6142\nBatch 1840/3471, Loss: 9.6607\n  M√©dia parcial at√© aqui: 8.4527\nBatch 1841/3471, Loss: 8.6308\nBatch 1842/3471, Loss: 6.6544\nBatch 1843/3471, Loss: 8.9273\nBatch 1844/3471, Loss: 9.9421\nBatch 1845/3471, Loss: 8.8322\nBatch 1846/3471, Loss: 8.1540\nBatch 1847/3471, Loss: 8.7733\nBatch 1848/3471, Loss: 8.5911\nBatch 1849/3471, Loss: 9.0431\nBatch 1850/3471, Loss: 9.9243\n  M√©dia parcial at√© aqui: 8.4543\nBatch 1851/3471, Loss: 8.4492\nBatch 1852/3471, Loss: 9.4908\nBatch 1853/3471, Loss: 8.8328\nBatch 1854/3471, Loss: 9.4004\nBatch 1855/3471, Loss: 8.8538\nBatch 1856/3471, Loss: 8.6226\nBatch 1857/3471, Loss: 8.6474\nBatch 1858/3471, Loss: 8.3152\nBatch 1859/3471, Loss: 8.5964\nBatch 1860/3471, Loss: 9.7417\n  M√©dia parcial at√© aqui: 8.4566\nBatch 1861/3471, Loss: 8.1402\nBatch 1862/3471, Loss: 7.6496\nBatch 1863/3471, Loss: 8.8844\nBatch 1864/3471, Loss: 7.8740\nBatch 1865/3471, Loss: 8.3426\nBatch 1866/3471, Loss: 8.5953\nBatch 1867/3471, Loss: 9.4310\nBatch 1868/3471, Loss: 8.0829\nBatch 1869/3471, Loss: 7.5001\nBatch 1870/3471, Loss: 8.5866\n  M√©dia parcial at√© aqui: 8.4558\nBatch 1871/3471, Loss: 9.3420\nBatch 1872/3471, Loss: 6.4860\nBatch 1873/3471, Loss: 7.5229\nBatch 1874/3471, Loss: 9.6823\nBatch 1875/3471, Loss: 7.5440\nBatch 1876/3471, Loss: 8.9886\nBatch 1877/3471, Loss: 9.3648\nBatch 1878/3471, Loss: 8.7631\nBatch 1879/3471, Loss: 7.9073\nBatch 1880/3471, Loss: 7.5656\n  M√©dia parcial at√© aqui: 8.4551\nBatch 1881/3471, Loss: 8.1241\nBatch 1882/3471, Loss: 9.8549\nBatch 1883/3471, Loss: 9.0644\nBatch 1884/3471, Loss: 7.9607\nBatch 1885/3471, Loss: 7.7978\nBatch 1886/3471, Loss: 8.7073\nBatch 1887/3471, Loss: 8.9878\nBatch 1888/3471, Loss: 7.8524\nBatch 1889/3471, Loss: 9.1699\nBatch 1890/3471, Loss: 9.0217\n  M√©dia parcial at√© aqui: 8.4562\nBatch 1891/3471, Loss: 8.2563\nBatch 1892/3471, Loss: 8.8285\nBatch 1893/3471, Loss: 7.5062\nBatch 1894/3471, Loss: 9.7532\nBatch 1895/3471, Loss: 9.8939\nBatch 1896/3471, Loss: 8.7192\nBatch 1897/3471, Loss: 7.6620\nBatch 1898/3471, Loss: 8.3065\nBatch 1899/3471, Loss: 6.7771\nBatch 1900/3471, Loss: 9.3736\n  M√©dia parcial at√© aqui: 8.4564\nBatch 1901/3471, Loss: 8.7453\nBatch 1902/3471, Loss: 7.8912\nBatch 1903/3471, Loss: 7.0251\nBatch 1904/3471, Loss: 8.7789\nBatch 1905/3471, Loss: 7.3999\nBatch 1906/3471, Loss: 9.0635\nBatch 1907/3471, Loss: 9.0422\nBatch 1908/3471, Loss: 8.9496\nBatch 1909/3471, Loss: 8.9289\nBatch 1910/3471, Loss: 7.7387\n  M√©dia parcial at√© aqui: 8.4559\nBatch 1911/3471, Loss: 8.4006\nBatch 1912/3471, Loss: 9.0608\nBatch 1913/3471, Loss: 6.7359\nBatch 1914/3471, Loss: 5.9929\nBatch 1915/3471, Loss: 8.9155\nBatch 1916/3471, Loss: 7.2038\nBatch 1917/3471, Loss: 8.4192\nBatch 1918/3471, Loss: 8.4352\nBatch 1919/3471, Loss: 10.2364\nBatch 1920/3471, Loss: 8.9398\n  M√©dia parcial at√© aqui: 8.4547\nBatch 1921/3471, Loss: 8.8527\nBatch 1922/3471, Loss: 9.5965\nBatch 1923/3471, Loss: 8.8741\nBatch 1924/3471, Loss: 9.5039\nBatch 1925/3471, Loss: 10.0302\nBatch 1926/3471, Loss: 8.3337\nBatch 1927/3471, Loss: 6.6974\nBatch 1928/3471, Loss: 8.9615\nBatch 1929/3471, Loss: 9.0760\nBatch 1930/3471, Loss: 8.3582\n  M√©dia parcial at√© aqui: 8.4567\nBatch 1931/3471, Loss: 9.5698\nBatch 1932/3471, Loss: 8.6465\nBatch 1933/3471, Loss: 9.4358\nBatch 1934/3471, Loss: 7.6565\nBatch 1935/3471, Loss: 9.5434\nBatch 1936/3471, Loss: 7.5797\nBatch 1937/3471, Loss: 8.0845\nBatch 1938/3471, Loss: 8.0030\nBatch 1939/3471, Loss: 8.2867\nBatch 1940/3471, Loss: 7.7576\n  M√©dia parcial at√© aqui: 8.4567\nBatch 1941/3471, Loss: 7.3148\nBatch 1942/3471, Loss: 8.6562\nBatch 1943/3471, Loss: 7.8845\nBatch 1944/3471, Loss: 8.5461\nBatch 1945/3471, Loss: 9.4566\nBatch 1946/3471, Loss: 8.7152\nBatch 1947/3471, Loss: 9.4797\nBatch 1948/3471, Loss: 7.9080\nBatch 1949/3471, Loss: 7.8582\nBatch 1950/3471, Loss: 9.2046\n  M√©dia parcial at√© aqui: 8.4569\nBatch 1951/3471, Loss: 8.7927\nBatch 1952/3471, Loss: 7.1207\nBatch 1953/3471, Loss: 6.5832\nBatch 1954/3471, Loss: 9.1231\nBatch 1955/3471, Loss: 8.8180\nBatch 1956/3471, Loss: 8.4648\nBatch 1957/3471, Loss: 7.5541\nBatch 1958/3471, Loss: 5.9722\nBatch 1959/3471, Loss: 9.2380\nBatch 1960/3471, Loss: 8.2361\n  M√©dia parcial at√© aqui: 8.4545\nBatch 1961/3471, Loss: 8.6679\nBatch 1962/3471, Loss: 8.6128\nBatch 1963/3471, Loss: 8.3509\nBatch 1964/3471, Loss: 9.2900\nBatch 1965/3471, Loss: 10.4061\nBatch 1966/3471, Loss: 6.5064\nBatch 1967/3471, Loss: 7.8058\nBatch 1968/3471, Loss: 10.1822\nBatch 1969/3471, Loss: 7.5541\nBatch 1970/3471, Loss: 8.8128\n  M√©dia parcial at√© aqui: 8.4554\nBatch 1971/3471, Loss: 8.9431\nBatch 1972/3471, Loss: 8.7666\nBatch 1973/3471, Loss: 9.5495\nBatch 1974/3471, Loss: 6.6318\nBatch 1975/3471, Loss: 8.4674\nBatch 1976/3471, Loss: 7.7523\nBatch 1977/3471, Loss: 6.8591\nBatch 1978/3471, Loss: 9.2902\nBatch 1979/3471, Loss: 8.8833\nBatch 1980/3471, Loss: 8.5090\n  M√©dia parcial at√© aqui: 8.4549\nBatch 1981/3471, Loss: 8.1139\nBatch 1982/3471, Loss: 9.4688\nBatch 1983/3471, Loss: 7.4461\nBatch 1984/3471, Loss: 8.0044\nBatch 1985/3471, Loss: 7.8998\nBatch 1986/3471, Loss: 7.0883\nBatch 1987/3471, Loss: 10.0665\nBatch 1988/3471, Loss: 9.5050\nBatch 1989/3471, Loss: 7.8400\nBatch 1990/3471, Loss: 8.4624\n  M√©dia parcial at√© aqui: 8.4546\nBatch 1991/3471, Loss: 8.7890\nBatch 1992/3471, Loss: 8.9163\nBatch 1993/3471, Loss: 9.9205\nBatch 1994/3471, Loss: 9.0374\nBatch 1995/3471, Loss: 7.5212\nBatch 1996/3471, Loss: 7.7940\nBatch 1997/3471, Loss: 9.6864\nBatch 1998/3471, Loss: 8.8038\nBatch 1999/3471, Loss: 7.5275\nBatch 2000/3471, Loss: 9.1137\n  M√©dia parcial at√© aqui: 8.4559\nBatch 2001/3471, Loss: 8.6854\nBatch 2002/3471, Loss: 8.0403\nBatch 2003/3471, Loss: 9.3311\nBatch 2004/3471, Loss: 7.7095\nBatch 2005/3471, Loss: 9.2763\nBatch 2006/3471, Loss: 7.8186\nBatch 2007/3471, Loss: 9.5162\nBatch 2008/3471, Loss: 9.7285\nBatch 2009/3471, Loss: 8.7173\nBatch 2010/3471, Loss: 7.6332\n  M√©dia parcial at√© aqui: 8.4568\nBatch 2011/3471, Loss: 8.7327\nBatch 2012/3471, Loss: 8.6746\nBatch 2013/3471, Loss: 7.4671\nBatch 2014/3471, Loss: 9.0163\nBatch 2015/3471, Loss: 7.7348\nBatch 2016/3471, Loss: 7.6535\nBatch 2017/3471, Loss: 6.7428\nBatch 2018/3471, Loss: 8.2105\nBatch 2019/3471, Loss: 8.3500\nBatch 2020/3471, Loss: 8.9339\n  M√©dia parcial at√© aqui: 8.4553\nBatch 2021/3471, Loss: 7.9794\nBatch 2022/3471, Loss: 8.8627\nBatch 2023/3471, Loss: 9.7624\nBatch 2024/3471, Loss: 8.0960\nBatch 2025/3471, Loss: 10.0571\nBatch 2026/3471, Loss: 7.4544\nBatch 2027/3471, Loss: 8.8299\nBatch 2028/3471, Loss: 9.3180\nBatch 2029/3471, Loss: 8.9098\nBatch 2030/3471, Loss: 9.5016\n  M√©dia parcial at√© aqui: 8.4574\nBatch 2031/3471, Loss: 8.2795\nBatch 2032/3471, Loss: 8.5376\nBatch 2033/3471, Loss: 5.9630\nBatch 2034/3471, Loss: 9.2185\nBatch 2035/3471, Loss: 7.8025\nBatch 2036/3471, Loss: 7.6303\nBatch 2037/3471, Loss: 7.8783\nBatch 2038/3471, Loss: 6.4884\nBatch 2039/3471, Loss: 9.2726\nBatch 2040/3471, Loss: 9.4816\n  M√©dia parcial at√© aqui: 8.4554\nBatch 2041/3471, Loss: 7.8364\nBatch 2042/3471, Loss: 7.6878\nBatch 2043/3471, Loss: 8.3949\nBatch 2044/3471, Loss: 7.8652\nBatch 2045/3471, Loss: 7.4594\nBatch 2046/3471, Loss: 9.4842\nBatch 2047/3471, Loss: 7.7299\nBatch 2048/3471, Loss: 7.7222\nBatch 2049/3471, Loss: 8.4497\nBatch 2050/3471, Loss: 7.5621\n  M√©dia parcial at√© aqui: 8.4533\nBatch 2051/3471, Loss: 8.1789\nBatch 2052/3471, Loss: 8.8469\nBatch 2053/3471, Loss: 9.4414\nBatch 2054/3471, Loss: 8.4487\nBatch 2055/3471, Loss: 7.7775\nBatch 2056/3471, Loss: 8.9273\nBatch 2057/3471, Loss: 8.3734\nBatch 2058/3471, Loss: 8.2563\nBatch 2059/3471, Loss: 9.5035\nBatch 2060/3471, Loss: 9.4766\n  M√©dia parcial at√© aqui: 8.4546\nBatch 2061/3471, Loss: 7.0244\nBatch 2062/3471, Loss: 7.5245\nBatch 2063/3471, Loss: 8.6194\nBatch 2064/3471, Loss: 9.5862\nBatch 2065/3471, Loss: 7.8150\nBatch 2066/3471, Loss: 9.7594\nBatch 2067/3471, Loss: 8.5548\nBatch 2068/3471, Loss: 8.7378\nBatch 2069/3471, Loss: 9.4078\nBatch 2070/3471, Loss: 7.6309\n  M√©dia parcial at√© aqui: 8.4546\nBatch 2071/3471, Loss: 7.7559\nBatch 2072/3471, Loss: 9.4146\nBatch 2073/3471, Loss: 9.4655\nBatch 2074/3471, Loss: 7.6312\nBatch 2075/3471, Loss: 7.8145\nBatch 2076/3471, Loss: 6.6779\nBatch 2077/3471, Loss: 9.3662\nBatch 2078/3471, Loss: 8.9880\nBatch 2079/3471, Loss: 6.8495\nBatch 2080/3471, Loss: 8.3775\n  M√©dia parcial at√© aqui: 8.4536\nBatch 2081/3471, Loss: 9.0470\nBatch 2082/3471, Loss: 9.4870\nBatch 2083/3471, Loss: 8.7811\nBatch 2084/3471, Loss: 8.9988\nBatch 2085/3471, Loss: 6.6864\nBatch 2086/3471, Loss: 8.1032\nBatch 2087/3471, Loss: 8.3357\nBatch 2088/3471, Loss: 9.4233\nBatch 2089/3471, Loss: 8.2551\nBatch 2090/3471, Loss: 6.9325\n  M√©dia parcial at√© aqui: 8.4533\nBatch 2091/3471, Loss: 7.5376\nBatch 2092/3471, Loss: 8.7870\nBatch 2093/3471, Loss: 7.5470\nBatch 2094/3471, Loss: 7.6752\nBatch 2095/3471, Loss: 9.2868\nBatch 2096/3471, Loss: 9.4805\nBatch 2097/3471, Loss: 9.3629\nBatch 2098/3471, Loss: 9.7838\nBatch 2099/3471, Loss: 8.9011\nBatch 2100/3471, Loss: 9.6686\n  M√©dia parcial at√© aqui: 8.4550\nBatch 2101/3471, Loss: 8.8251\nBatch 2102/3471, Loss: 7.7282\nBatch 2103/3471, Loss: 8.3851\nBatch 2104/3471, Loss: 8.1075\nBatch 2105/3471, Loss: 8.5967\nBatch 2106/3471, Loss: 9.2050\nBatch 2107/3471, Loss: 8.6672\nBatch 2108/3471, Loss: 7.8934\nBatch 2109/3471, Loss: 8.7235\nBatch 2110/3471, Loss: 9.5627\n  M√©dia parcial at√© aqui: 8.4556\nBatch 2111/3471, Loss: 8.3354\nBatch 2112/3471, Loss: 9.1910\nBatch 2113/3471, Loss: 8.5401\nBatch 2114/3471, Loss: 9.6785\nBatch 2115/3471, Loss: 6.8375\nBatch 2116/3471, Loss: 9.8657\nBatch 2117/3471, Loss: 9.5588\nBatch 2118/3471, Loss: 7.6667\nBatch 2119/3471, Loss: 9.0104\nBatch 2120/3471, Loss: 7.9198\n  M√©dia parcial at√© aqui: 8.4565\nBatch 2121/3471, Loss: 8.1206\nBatch 2122/3471, Loss: 7.4892\nBatch 2123/3471, Loss: 7.9047\nBatch 2124/3471, Loss: 8.4842\nBatch 2125/3471, Loss: 8.1492\nBatch 2126/3471, Loss: 8.7602\nBatch 2127/3471, Loss: 8.4163\nBatch 2128/3471, Loss: 5.4882\nBatch 2129/3471, Loss: 8.4713\nBatch 2130/3471, Loss: 8.4767\n  M√©dia parcial at√© aqui: 8.4543\nBatch 2131/3471, Loss: 9.0145\nBatch 2132/3471, Loss: 9.7768\nBatch 2133/3471, Loss: 9.2216\nBatch 2134/3471, Loss: 9.5139\nBatch 2135/3471, Loss: 9.3596\nBatch 2136/3471, Loss: 9.9026\nBatch 2137/3471, Loss: 7.8473\nBatch 2138/3471, Loss: 8.6097\nBatch 2139/3471, Loss: 8.5220\nBatch 2140/3471, Loss: 8.5529\n  M√©dia parcial at√© aqui: 8.4570\nBatch 2141/3471, Loss: 8.5141\nBatch 2142/3471, Loss: 7.8160\nBatch 2143/3471, Loss: 7.7241\nBatch 2144/3471, Loss: 8.2323\nBatch 2145/3471, Loss: 7.5032\nBatch 2146/3471, Loss: 9.6662\nBatch 2147/3471, Loss: 10.0183\nBatch 2148/3471, Loss: 7.4703\nBatch 2149/3471, Loss: 6.7076\nBatch 2150/3471, Loss: 10.1478\n  M√©dia parcial at√© aqui: 8.4566\nBatch 2151/3471, Loss: 8.4439\nBatch 2152/3471, Loss: 8.3707\nBatch 2153/3471, Loss: 9.3388\nBatch 2154/3471, Loss: 9.7174\nBatch 2155/3471, Loss: 8.3130\nBatch 2156/3471, Loss: 9.3164\nBatch 2157/3471, Loss: 8.6707\nBatch 2158/3471, Loss: 8.6504\nBatch 2159/3471, Loss: 7.6798\nBatch 2160/3471, Loss: 10.0374\n  M√©dia parcial at√© aqui: 8.4584\nBatch 2161/3471, Loss: 9.7632\nBatch 2162/3471, Loss: 8.4821\nBatch 2163/3471, Loss: 9.5514\nBatch 2164/3471, Loss: 8.5437\nBatch 2165/3471, Loss: 9.4831\nBatch 2166/3471, Loss: 8.8009\nBatch 2167/3471, Loss: 7.6700\nBatch 2168/3471, Loss: 7.7496\nBatch 2169/3471, Loss: 8.7055\nBatch 2170/3471, Loss: 8.1505\n  M√©dia parcial at√© aqui: 8.4595\nBatch 2171/3471, Loss: 9.1369\nBatch 2172/3471, Loss: 7.4868\nBatch 2173/3471, Loss: 8.4710\nBatch 2174/3471, Loss: 9.2742\nBatch 2175/3471, Loss: 9.5277\nBatch 2176/3471, Loss: 7.6192\nBatch 2177/3471, Loss: 9.2729\nBatch 2178/3471, Loss: 10.2423\nBatch 2179/3471, Loss: 8.6234\nBatch 2180/3471, Loss: 7.8746\n  M√©dia parcial at√© aqui: 8.4609\nBatch 2181/3471, Loss: 8.6063\nBatch 2182/3471, Loss: 8.4403\nBatch 2183/3471, Loss: 9.3163\nBatch 2184/3471, Loss: 9.8662\nBatch 2185/3471, Loss: 8.6327\nBatch 2186/3471, Loss: 9.8024\nBatch 2187/3471, Loss: 9.5754\nBatch 2188/3471, Loss: 7.7401\nBatch 2189/3471, Loss: 9.4570\nBatch 2190/3471, Loss: 10.2243\n  M√©dia parcial at√© aqui: 8.4641\nBatch 2191/3471, Loss: 7.5352\nBatch 2192/3471, Loss: 9.7377\nBatch 2193/3471, Loss: 9.9445\nBatch 2194/3471, Loss: 7.9112\nBatch 2195/3471, Loss: 8.8872\nBatch 2196/3471, Loss: 8.8288\nBatch 2197/3471, Loss: 8.5268\nBatch 2198/3471, Loss: 7.6267\nBatch 2199/3471, Loss: 8.2199\nBatch 2200/3471, Loss: 8.4872\n  M√©dia parcial at√© aqui: 8.4646\nBatch 2201/3471, Loss: 8.5731\nBatch 2202/3471, Loss: 9.5381\nBatch 2203/3471, Loss: 6.6348\nBatch 2204/3471, Loss: 8.2553\nBatch 2205/3471, Loss: 7.6592\nBatch 2206/3471, Loss: 9.5521\nBatch 2207/3471, Loss: 6.6329\nBatch 2208/3471, Loss: 8.2023\nBatch 2209/3471, Loss: 9.3246\nBatch 2210/3471, Loss: 7.9323\n  M√©dia parcial at√© aqui: 8.4635\nBatch 2211/3471, Loss: 9.4936\nBatch 2212/3471, Loss: 6.9057\nBatch 2213/3471, Loss: 9.7421\nBatch 2214/3471, Loss: 8.4820\nBatch 2215/3471, Loss: 8.0162\nBatch 2216/3471, Loss: 7.5753\nBatch 2217/3471, Loss: 7.3373\nBatch 2218/3471, Loss: 7.9959\nBatch 2219/3471, Loss: 9.7680\nBatch 2220/3471, Loss: 8.2751\n  M√©dia parcial at√© aqui: 8.4630\nBatch 2221/3471, Loss: 7.5116\nBatch 2222/3471, Loss: 7.6348\nBatch 2223/3471, Loss: 8.4156\nBatch 2224/3471, Loss: 7.6034\nBatch 2225/3471, Loss: 8.9922\nBatch 2226/3471, Loss: 8.4169\nBatch 2227/3471, Loss: 9.0589\nBatch 2228/3471, Loss: 7.6652\nBatch 2229/3471, Loss: 9.9373\nBatch 2230/3471, Loss: 7.3864\n  M√©dia parcial at√© aqui: 8.4621\nBatch 2231/3471, Loss: 8.8274\nBatch 2232/3471, Loss: 7.5382\nBatch 2233/3471, Loss: 9.9132\nBatch 2234/3471, Loss: 6.8849\nBatch 2235/3471, Loss: 8.7495\nBatch 2236/3471, Loss: 8.7524\nBatch 2237/3471, Loss: 9.3966\nBatch 2238/3471, Loss: 7.6580\nBatch 2239/3471, Loss: 9.2153\nBatch 2240/3471, Loss: 9.6694\n  M√©dia parcial at√© aqui: 8.4630\nBatch 2241/3471, Loss: 8.6537\nBatch 2242/3471, Loss: 7.7306\nBatch 2243/3471, Loss: 9.7901\nBatch 2244/3471, Loss: 9.7138\nBatch 2245/3471, Loss: 8.6575\nBatch 2246/3471, Loss: 7.8911\nBatch 2247/3471, Loss: 8.7287\nBatch 2248/3471, Loss: 8.8830\nBatch 2249/3471, Loss: 8.9290\nBatch 2250/3471, Loss: 8.4572\n  M√©dia parcial at√© aqui: 8.4643\nBatch 2251/3471, Loss: 9.9674\nBatch 2252/3471, Loss: 8.7354\nBatch 2253/3471, Loss: 9.3755\nBatch 2254/3471, Loss: 8.7963\nBatch 2255/3471, Loss: 8.6640\nBatch 2256/3471, Loss: 9.1921\nBatch 2257/3471, Loss: 7.7384\nBatch 2258/3471, Loss: 8.8739\nBatch 2259/3471, Loss: 9.7217\nBatch 2260/3471, Loss: 9.4527\n  M√©dia parcial at√© aqui: 8.4669\nBatch 2261/3471, Loss: 8.7860\nBatch 2262/3471, Loss: 9.6029\nBatch 2263/3471, Loss: 7.5827\nBatch 2264/3471, Loss: 7.7059\nBatch 2265/3471, Loss: 6.3511\nBatch 2266/3471, Loss: 6.6163\nBatch 2267/3471, Loss: 7.6836\nBatch 2268/3471, Loss: 7.5235\nBatch 2269/3471, Loss: 8.3202\nBatch 2270/3471, Loss: 9.6060\n  M√©dia parcial at√© aqui: 8.4647\nBatch 2271/3471, Loss: 6.6410\nBatch 2272/3471, Loss: 7.6835\nBatch 2273/3471, Loss: 8.8480\nBatch 2274/3471, Loss: 9.0641\nBatch 2275/3471, Loss: 8.2104\nBatch 2276/3471, Loss: 7.8066\nBatch 2277/3471, Loss: 8.6648\nBatch 2278/3471, Loss: 8.8943\nBatch 2279/3471, Loss: 7.5734\nBatch 2280/3471, Loss: 8.5017\n  M√©dia parcial at√© aqui: 8.4635\nBatch 2281/3471, Loss: 8.1965\nBatch 2282/3471, Loss: 6.8776\nBatch 2283/3471, Loss: 6.4879\nBatch 2284/3471, Loss: 9.7984\nBatch 2285/3471, Loss: 8.5862\nBatch 2286/3471, Loss: 7.4308\nBatch 2287/3471, Loss: 8.7337\nBatch 2288/3471, Loss: 9.1283\nBatch 2289/3471, Loss: 9.6714\nBatch 2290/3471, Loss: 7.1053\n  M√©dia parcial at√© aqui: 8.4624\nBatch 2291/3471, Loss: 8.0024\nBatch 2292/3471, Loss: 8.5111\nBatch 2293/3471, Loss: 9.0547\nBatch 2294/3471, Loss: 9.1518\nBatch 2295/3471, Loss: 8.7388\nBatch 2296/3471, Loss: 9.4296\nBatch 2297/3471, Loss: 8.6597\nBatch 2298/3471, Loss: 9.5919\nBatch 2299/3471, Loss: 7.9668\nBatch 2300/3471, Loss: 9.4101\n  M√©dia parcial at√© aqui: 8.4640\nBatch 2301/3471, Loss: 8.9153\nBatch 2302/3471, Loss: 9.3092\nBatch 2303/3471, Loss: 8.1343\nBatch 2304/3471, Loss: 9.4478\nBatch 2305/3471, Loss: 6.0274\nBatch 2306/3471, Loss: 5.9590\nBatch 2307/3471, Loss: 9.6403\nBatch 2308/3471, Loss: 7.7919\nBatch 2309/3471, Loss: 8.6674\nBatch 2310/3471, Loss: 6.8562\n  M√©dia parcial at√© aqui: 8.4624\nBatch 2311/3471, Loss: 7.9603\nBatch 2312/3471, Loss: 9.6785\nBatch 2313/3471, Loss: 8.2854\nBatch 2314/3471, Loss: 8.6449\nBatch 2315/3471, Loss: 9.1632\nBatch 2316/3471, Loss: 9.3824\nBatch 2317/3471, Loss: 8.8294\nBatch 2318/3471, Loss: 7.8888\nBatch 2319/3471, Loss: 8.6374\nBatch 2320/3471, Loss: 8.5540\n  M√©dia parcial at√© aqui: 8.4634\nBatch 2321/3471, Loss: 6.5364\nBatch 2322/3471, Loss: 8.6748\nBatch 2323/3471, Loss: 9.7539\nBatch 2324/3471, Loss: 7.5190\nBatch 2325/3471, Loss: 7.6521\nBatch 2326/3471, Loss: 7.3251\nBatch 2327/3471, Loss: 8.8110\nBatch 2328/3471, Loss: 9.2696\nBatch 2329/3471, Loss: 8.5826\nBatch 2330/3471, Loss: 9.2254\n  M√©dia parcial at√© aqui: 8.4628\nBatch 2331/3471, Loss: 9.1355\nBatch 2332/3471, Loss: 8.9160\nBatch 2333/3471, Loss: 8.6074\nBatch 2334/3471, Loss: 9.7507\nBatch 2335/3471, Loss: 8.5243\nBatch 2336/3471, Loss: 8.5092\nBatch 2337/3471, Loss: 8.5193\nBatch 2338/3471, Loss: 8.7598\nBatch 2339/3471, Loss: 9.3265\nBatch 2340/3471, Loss: 8.6136\n  M√©dia parcial at√© aqui: 8.4646\nBatch 2341/3471, Loss: 9.9883\nBatch 2342/3471, Loss: 9.8320\nBatch 2343/3471, Loss: 9.4369\nBatch 2344/3471, Loss: 6.4639\nBatch 2345/3471, Loss: 8.7168\nBatch 2346/3471, Loss: 8.4692\nBatch 2347/3471, Loss: 9.2156\nBatch 2348/3471, Loss: 8.9872\nBatch 2349/3471, Loss: 9.3907\nBatch 2350/3471, Loss: 7.6306\n  M√©dia parcial at√© aqui: 8.4661\nBatch 2351/3471, Loss: 7.8192\nBatch 2352/3471, Loss: 8.9761\nBatch 2353/3471, Loss: 8.0555\nBatch 2354/3471, Loss: 9.8038\nBatch 2355/3471, Loss: 8.4999\nBatch 2356/3471, Loss: 8.6703\nBatch 2357/3471, Loss: 7.0073\nBatch 2358/3471, Loss: 6.6482\nBatch 2359/3471, Loss: 7.6595\nBatch 2360/3471, Loss: 6.7596\n  M√©dia parcial at√© aqui: 8.4640\nBatch 2361/3471, Loss: 5.3104\nBatch 2362/3471, Loss: 8.2431\nBatch 2363/3471, Loss: 7.4781\nBatch 2364/3471, Loss: 8.4216\nBatch 2365/3471, Loss: 7.5230\nBatch 2366/3471, Loss: 9.4829\nBatch 2367/3471, Loss: 6.7571\nBatch 2368/3471, Loss: 8.7318\nBatch 2369/3471, Loss: 8.8342\nBatch 2370/3471, Loss: 8.4639\n  M√©dia parcial at√© aqui: 8.4618\nBatch 2371/3471, Loss: 9.5202\nBatch 2372/3471, Loss: 7.7748\nBatch 2373/3471, Loss: 8.6755\nBatch 2374/3471, Loss: 8.8914\nBatch 2375/3471, Loss: 8.7625\nBatch 2376/3471, Loss: 7.6053\nBatch 2377/3471, Loss: 9.2718\nBatch 2378/3471, Loss: 7.8336\nBatch 2379/3471, Loss: 7.6326\nBatch 2380/3471, Loss: 8.9744\n  M√©dia parcial at√© aqui: 8.4619\nBatch 2381/3471, Loss: 7.5173\nBatch 2382/3471, Loss: 9.8779\nBatch 2383/3471, Loss: 7.5029\nBatch 2384/3471, Loss: 10.1454\nBatch 2385/3471, Loss: 8.7318\nBatch 2386/3471, Loss: 8.9300\nBatch 2387/3471, Loss: 9.9793\nBatch 2388/3471, Loss: 8.3120\nBatch 2389/3471, Loss: 9.5752\nBatch 2390/3471, Loss: 9.5580\n  M√©dia parcial at√© aqui: 8.4642\nBatch 2391/3471, Loss: 8.4500\nBatch 2392/3471, Loss: 8.5897\nBatch 2393/3471, Loss: 8.9535\nBatch 2394/3471, Loss: 7.5490\nBatch 2395/3471, Loss: 8.9308\nBatch 2396/3471, Loss: 8.1112\nBatch 2397/3471, Loss: 7.6354\nBatch 2398/3471, Loss: 7.8759\nBatch 2399/3471, Loss: 8.6528\nBatch 2400/3471, Loss: 8.1733\n  M√©dia parcial at√© aqui: 8.4635\nBatch 2401/3471, Loss: 8.3944\nBatch 2402/3471, Loss: 9.2442\nBatch 2403/3471, Loss: 9.4373\nBatch 2404/3471, Loss: 8.7472\nBatch 2405/3471, Loss: 8.4654\nBatch 2406/3471, Loss: 9.8161\nBatch 2407/3471, Loss: 10.0292\nBatch 2408/3471, Loss: 7.6094\nBatch 2409/3471, Loss: 8.8559\nBatch 2410/3471, Loss: 9.6007\n  M√©dia parcial at√© aqui: 8.4658\nBatch 2411/3471, Loss: 9.4829\nBatch 2412/3471, Loss: 8.7533\nBatch 2413/3471, Loss: 8.6535\nBatch 2414/3471, Loss: 8.4256\nBatch 2415/3471, Loss: 5.1969\nBatch 2416/3471, Loss: 8.5765\nBatch 2417/3471, Loss: 7.7825\nBatch 2418/3471, Loss: 9.3272\nBatch 2419/3471, Loss: 8.5039\nBatch 2420/3471, Loss: 8.5604\n  M√©dia parcial at√© aqui: 8.4652\nBatch 2421/3471, Loss: 8.9621\nBatch 2422/3471, Loss: 7.5954\nBatch 2423/3471, Loss: 8.5724\nBatch 2424/3471, Loss: 7.4319\nBatch 2425/3471, Loss: 8.8216\nBatch 2426/3471, Loss: 6.1119\nBatch 2427/3471, Loss: 6.9480\nBatch 2428/3471, Loss: 8.8375\nBatch 2429/3471, Loss: 8.4769\nBatch 2430/3471, Loss: 7.3102\n  M√©dia parcial at√© aqui: 8.4629\nBatch 2431/3471, Loss: 8.1496\nBatch 2432/3471, Loss: 9.9002\nBatch 2433/3471, Loss: 8.5426\nBatch 2434/3471, Loss: 7.2576\nBatch 2435/3471, Loss: 7.9183\nBatch 2436/3471, Loss: 8.5644\nBatch 2437/3471, Loss: 9.4338\nBatch 2438/3471, Loss: 6.4867\nBatch 2439/3471, Loss: 7.5405\nBatch 2440/3471, Loss: 8.5997\n  M√©dia parcial at√© aqui: 8.4620\nBatch 2441/3471, Loss: 7.2800\nBatch 2442/3471, Loss: 8.5479\nBatch 2443/3471, Loss: 9.1410\nBatch 2444/3471, Loss: 8.3972\nBatch 2445/3471, Loss: 9.5799\nBatch 2446/3471, Loss: 9.4781\nBatch 2447/3471, Loss: 8.5759\nBatch 2448/3471, Loss: 8.4297\nBatch 2449/3471, Loss: 8.7929\nBatch 2450/3471, Loss: 8.4341\n  M√©dia parcial at√© aqui: 8.4628\nBatch 2451/3471, Loss: 7.6887\nBatch 2452/3471, Loss: 9.3592\nBatch 2453/3471, Loss: 6.4723\nBatch 2454/3471, Loss: 7.8024\nBatch 2455/3471, Loss: 6.6429\nBatch 2456/3471, Loss: 7.8454\nBatch 2457/3471, Loss: 9.2690\nBatch 2458/3471, Loss: 9.8700\nBatch 2459/3471, Loss: 7.5261\nBatch 2460/3471, Loss: 7.7401\n  M√©dia parcial at√© aqui: 8.4610\nBatch 2461/3471, Loss: 9.1226\nBatch 2462/3471, Loss: 6.4024\nBatch 2463/3471, Loss: 7.6783\nBatch 2464/3471, Loss: 9.3267\nBatch 2465/3471, Loss: 9.2415\nBatch 2466/3471, Loss: 8.6834\nBatch 2467/3471, Loss: 6.7786\nBatch 2468/3471, Loss: 8.6635\nBatch 2469/3471, Loss: 9.9701\nBatch 2470/3471, Loss: 8.9439\n  M√©dia parcial at√© aqui: 8.4611\nBatch 2471/3471, Loss: 8.3347\nBatch 2472/3471, Loss: 8.6279\nBatch 2473/3471, Loss: 9.4061\nBatch 2474/3471, Loss: 8.9117\nBatch 2475/3471, Loss: 8.4177\nBatch 2476/3471, Loss: 7.4636\nBatch 2477/3471, Loss: 7.6744\nBatch 2478/3471, Loss: 9.4360\nBatch 2479/3471, Loss: 8.8064\nBatch 2480/3471, Loss: 6.8370\n  M√©dia parcial at√© aqui: 8.4608\nBatch 2481/3471, Loss: 8.7151\nBatch 2482/3471, Loss: 9.3069\nBatch 2483/3471, Loss: 9.0125\nBatch 2484/3471, Loss: 9.5062\nBatch 2485/3471, Loss: 8.3417\nBatch 2486/3471, Loss: 9.2105\nBatch 2487/3471, Loss: 7.7527\nBatch 2488/3471, Loss: 7.7886\nBatch 2489/3471, Loss: 9.3936\nBatch 2490/3471, Loss: 8.5112\n  M√©dia parcial at√© aqui: 8.4620\nBatch 2491/3471, Loss: 9.1850\nBatch 2492/3471, Loss: 9.4615\nBatch 2493/3471, Loss: 7.5309\nBatch 2494/3471, Loss: 9.0792\nBatch 2495/3471, Loss: 8.6166\nBatch 2496/3471, Loss: 8.9167\nBatch 2497/3471, Loss: 8.3470\nBatch 2498/3471, Loss: 8.7607\nBatch 2499/3471, Loss: 8.3342\nBatch 2500/3471, Loss: 9.9367\n  M√©dia parcial at√© aqui: 8.4634\nBatch 2501/3471, Loss: 9.7344\nBatch 2502/3471, Loss: 9.8703\nBatch 2503/3471, Loss: 7.6912\nBatch 2504/3471, Loss: 7.8425\nBatch 2505/3471, Loss: 9.1816\nBatch 2506/3471, Loss: 7.8015\nBatch 2507/3471, Loss: 8.0594\nBatch 2508/3471, Loss: 8.6888\nBatch 2509/3471, Loss: 8.3793\nBatch 2510/3471, Loss: 8.4488\n  M√©dia parcial at√© aqui: 8.4639\nBatch 2511/3471, Loss: 8.5874\nBatch 2512/3471, Loss: 6.5237\nBatch 2513/3471, Loss: 8.1259\nBatch 2514/3471, Loss: 10.2469\nBatch 2515/3471, Loss: 8.6330\nBatch 2516/3471, Loss: 9.0789\nBatch 2517/3471, Loss: 7.5393\nBatch 2518/3471, Loss: 7.5578\nBatch 2519/3471, Loss: 7.5303\nBatch 2520/3471, Loss: 8.3603\n  M√©dia parcial at√© aqui: 8.4629\nBatch 2521/3471, Loss: 6.8546\nBatch 2522/3471, Loss: 9.8428\nBatch 2523/3471, Loss: 7.6210\nBatch 2524/3471, Loss: 7.1919\nBatch 2525/3471, Loss: 9.9188\nBatch 2526/3471, Loss: 8.9285\nBatch 2527/3471, Loss: 8.5500\nBatch 2528/3471, Loss: 7.6319\nBatch 2529/3471, Loss: 9.2892\nBatch 2530/3471, Loss: 9.6870\n  M√©dia parcial at√© aqui: 8.4632\nBatch 2531/3471, Loss: 8.0708\nBatch 2532/3471, Loss: 9.6644\nBatch 2533/3471, Loss: 8.3258\nBatch 2534/3471, Loss: 9.6679\nBatch 2535/3471, Loss: 9.0264\nBatch 2536/3471, Loss: 7.7400\nBatch 2537/3471, Loss: 7.5738\nBatch 2538/3471, Loss: 8.4256\nBatch 2539/3471, Loss: 7.9510\nBatch 2540/3471, Loss: 8.5172\n  M√©dia parcial at√© aqui: 8.4634\nBatch 2541/3471, Loss: 6.7848\nBatch 2542/3471, Loss: 9.0016\nBatch 2543/3471, Loss: 8.9869\nBatch 2544/3471, Loss: 8.3274\nBatch 2545/3471, Loss: 7.4112\nBatch 2546/3471, Loss: 10.3083\nBatch 2547/3471, Loss: 7.9979\nBatch 2548/3471, Loss: 6.9743\nBatch 2549/3471, Loss: 8.2623\nBatch 2550/3471, Loss: 9.4280\n  M√©dia parcial at√© aqui: 8.4629\nBatch 2551/3471, Loss: 9.8730\nBatch 2552/3471, Loss: 6.6341\nBatch 2553/3471, Loss: 9.8890\nBatch 2554/3471, Loss: 8.1063\nBatch 2555/3471, Loss: 7.5939\nBatch 2556/3471, Loss: 7.6909\nBatch 2557/3471, Loss: 8.2478\nBatch 2558/3471, Loss: 8.3914\nBatch 2559/3471, Loss: 8.2165\nBatch 2560/3471, Loss: 9.1461\n  M√©dia parcial at√© aqui: 8.4626\nBatch 2561/3471, Loss: 8.6093\nBatch 2562/3471, Loss: 9.4498\nBatch 2563/3471, Loss: 6.9340\nBatch 2564/3471, Loss: 8.3644\nBatch 2565/3471, Loss: 6.9512\nBatch 2566/3471, Loss: 9.7215\nBatch 2567/3471, Loss: 8.9421\nBatch 2568/3471, Loss: 8.3319\nBatch 2569/3471, Loss: 7.6717\nBatch 2570/3471, Loss: 8.5110\n  M√©dia parcial at√© aqui: 8.4621\nBatch 2571/3471, Loss: 8.4741\nBatch 2572/3471, Loss: 8.6194\nBatch 2573/3471, Loss: 8.0700\nBatch 2574/3471, Loss: 8.3766\nBatch 2575/3471, Loss: 7.6400\nBatch 2576/3471, Loss: 9.8598\nBatch 2577/3471, Loss: 9.8416\nBatch 2578/3471, Loss: 9.5119\nBatch 2579/3471, Loss: 8.3705\nBatch 2580/3471, Loss: 8.0578\n  M√©dia parcial at√© aqui: 8.4630\nBatch 2581/3471, Loss: 8.2533\nBatch 2582/3471, Loss: 4.5338\nBatch 2583/3471, Loss: 7.7623\nBatch 2584/3471, Loss: 8.3681\nBatch 2585/3471, Loss: 9.1305\nBatch 2586/3471, Loss: 8.8687\nBatch 2587/3471, Loss: 7.9062\nBatch 2588/3471, Loss: 6.6066\nBatch 2589/3471, Loss: 7.6130\nBatch 2590/3471, Loss: 9.6320\n  M√©dia parcial at√© aqui: 8.4607\nBatch 2591/3471, Loss: 8.5299\nBatch 2592/3471, Loss: 8.6264\nBatch 2593/3471, Loss: 8.6524\nBatch 2594/3471, Loss: 6.6438\nBatch 2595/3471, Loss: 9.6267\nBatch 2596/3471, Loss: 7.7324\nBatch 2597/3471, Loss: 8.2476\nBatch 2598/3471, Loss: 8.7380\nBatch 2599/3471, Loss: 8.7358\nBatch 2600/3471, Loss: 8.0953\n  M√©dia parcial at√© aqui: 8.4603\nBatch 2601/3471, Loss: 9.1029\nBatch 2602/3471, Loss: 9.9972\nBatch 2603/3471, Loss: 7.3540\nBatch 2604/3471, Loss: 7.1275\nBatch 2605/3471, Loss: 8.9365\nBatch 2606/3471, Loss: 8.0798\nBatch 2607/3471, Loss: 8.7824\nBatch 2608/3471, Loss: 9.7740\nBatch 2609/3471, Loss: 9.3629\nBatch 2610/3471, Loss: 8.6890\n  M√©dia parcial at√© aqui: 8.4613\nBatch 2611/3471, Loss: 7.5960\nBatch 2612/3471, Loss: 9.5576\nBatch 2613/3471, Loss: 6.4533\nBatch 2614/3471, Loss: 8.8124\nBatch 2615/3471, Loss: 9.4428\nBatch 2616/3471, Loss: 8.8244\nBatch 2617/3471, Loss: 7.6421\nBatch 2618/3471, Loss: 9.5443\nBatch 2619/3471, Loss: 8.7046\nBatch 2620/3471, Loss: 7.6395\n  M√©dia parcial at√© aqui: 8.4612\nBatch 2621/3471, Loss: 8.4359\nBatch 2622/3471, Loss: 9.7071\nBatch 2623/3471, Loss: 7.5502\nBatch 2624/3471, Loss: 8.5643\nBatch 2625/3471, Loss: 9.4472\nBatch 2626/3471, Loss: 8.8092\nBatch 2627/3471, Loss: 7.7953\nBatch 2628/3471, Loss: 9.6454\nBatch 2629/3471, Loss: 7.7684\nBatch 2630/3471, Loss: 8.5375\n  M√©dia parcial at√© aqui: 8.4618\nBatch 2631/3471, Loss: 9.5856\nBatch 2632/3471, Loss: 8.3100\nBatch 2633/3471, Loss: 9.0967\nBatch 2634/3471, Loss: 9.7501\nBatch 2635/3471, Loss: 8.7915\nBatch 2636/3471, Loss: 8.8960\nBatch 2637/3471, Loss: 9.9948\nBatch 2638/3471, Loss: 9.5088\nBatch 2639/3471, Loss: 6.6499\nBatch 2640/3471, Loss: 9.4192\n  M√©dia parcial at√© aqui: 8.4638\nBatch 2641/3471, Loss: 8.7680\nBatch 2642/3471, Loss: 7.8329\nBatch 2643/3471, Loss: 8.7220\nBatch 2644/3471, Loss: 8.4857\nBatch 2645/3471, Loss: 8.7495\nBatch 2646/3471, Loss: 8.6787\nBatch 2647/3471, Loss: 7.6740\nBatch 2648/3471, Loss: 8.5122\nBatch 2649/3471, Loss: 7.6567\nBatch 2650/3471, Loss: 8.5200\n  M√©dia parcial at√© aqui: 8.4634\nBatch 2651/3471, Loss: 8.0173\nBatch 2652/3471, Loss: 8.2155\nBatch 2653/3471, Loss: 8.6089\nBatch 2654/3471, Loss: 9.9525\nBatch 2655/3471, Loss: 8.6655\nBatch 2656/3471, Loss: 9.2963\nBatch 2657/3471, Loss: 9.9694\nBatch 2658/3471, Loss: 7.4337\nBatch 2659/3471, Loss: 8.4938\nBatch 2660/3471, Loss: 9.6324\n  M√©dia parcial at√© aqui: 8.4648\nBatch 2661/3471, Loss: 5.9062\nBatch 2662/3471, Loss: 9.4556\nBatch 2663/3471, Loss: 9.4754\nBatch 2664/3471, Loss: 9.7030\nBatch 2665/3471, Loss: 9.5560\nBatch 2666/3471, Loss: 9.3922\nBatch 2667/3471, Loss: 8.6165\nBatch 2668/3471, Loss: 7.4799\nBatch 2669/3471, Loss: 7.0056\nBatch 2670/3471, Loss: 8.3680\n  M√©dia parcial at√© aqui: 8.4649\nBatch 2671/3471, Loss: 9.0100\nBatch 2672/3471, Loss: 9.3850\nBatch 2673/3471, Loss: 9.8981\nBatch 2674/3471, Loss: 9.8016\nBatch 2675/3471, Loss: 8.6708\nBatch 2676/3471, Loss: 7.1667\nBatch 2677/3471, Loss: 9.3816\nBatch 2678/3471, Loss: 7.5820\nBatch 2679/3471, Loss: 8.7716\nBatch 2680/3471, Loss: 7.8691\n  M√©dia parcial at√© aqui: 8.4660\nBatch 2681/3471, Loss: 7.7718\nBatch 2682/3471, Loss: 10.0312\nBatch 2683/3471, Loss: 10.0432\nBatch 2684/3471, Loss: 8.3887\nBatch 2685/3471, Loss: 7.7217\nBatch 2686/3471, Loss: 7.5367\nBatch 2687/3471, Loss: 8.8040\nBatch 2688/3471, Loss: 8.2505\nBatch 2689/3471, Loss: 8.4964\nBatch 2690/3471, Loss: 7.9338\n  M√©dia parcial at√© aqui: 8.4661\nBatch 2691/3471, Loss: 8.6386\nBatch 2692/3471, Loss: 9.9041\nBatch 2693/3471, Loss: 8.9949\nBatch 2694/3471, Loss: 8.1639\nBatch 2695/3471, Loss: 9.3711\nBatch 2696/3471, Loss: 8.5762\nBatch 2697/3471, Loss: 8.3558\nBatch 2698/3471, Loss: 6.5577\nBatch 2699/3471, Loss: 6.9405\nBatch 2700/3471, Loss: 7.8342\n  M√©dia parcial at√© aqui: 8.4656\nBatch 2701/3471, Loss: 7.9388\nBatch 2702/3471, Loss: 8.5214\nBatch 2703/3471, Loss: 8.6293\nBatch 2704/3471, Loss: 7.4846\nBatch 2705/3471, Loss: 5.6368\nBatch 2706/3471, Loss: 6.6353\nBatch 2707/3471, Loss: 10.0372\nBatch 2708/3471, Loss: 7.4561\nBatch 2709/3471, Loss: 6.7810\nBatch 2710/3471, Loss: 7.7492\n  M√©dia parcial at√© aqui: 8.4628\nBatch 2711/3471, Loss: 8.1447\nBatch 2712/3471, Loss: 8.6419\nBatch 2713/3471, Loss: 7.6447\nBatch 2714/3471, Loss: 9.1274\nBatch 2715/3471, Loss: 9.3462\nBatch 2716/3471, Loss: 7.9896\nBatch 2717/3471, Loss: 5.8423\nBatch 2718/3471, Loss: 7.8360\nBatch 2719/3471, Loss: 8.0041\nBatch 2720/3471, Loss: 7.1810\n  M√©dia parcial at√© aqui: 8.4610\nBatch 2721/3471, Loss: 8.0876\nBatch 2722/3471, Loss: 9.1815\nBatch 2723/3471, Loss: 8.0190\nBatch 2724/3471, Loss: 8.6373\nBatch 2725/3471, Loss: 8.9749\nBatch 2726/3471, Loss: 8.8816\nBatch 2727/3471, Loss: 7.5979\nBatch 2728/3471, Loss: 7.1303\nBatch 2729/3471, Loss: 9.3119\nBatch 2730/3471, Loss: 9.0887\n  M√©dia parcial at√© aqui: 8.4611\nBatch 2731/3471, Loss: 9.3068\nBatch 2732/3471, Loss: 8.5602\nBatch 2733/3471, Loss: 4.8634\nBatch 2734/3471, Loss: 8.0737\nBatch 2735/3471, Loss: 8.5577\nBatch 2736/3471, Loss: 8.9470\nBatch 2737/3471, Loss: 8.5422\nBatch 2738/3471, Loss: 7.4650\nBatch 2739/3471, Loss: 8.4528\nBatch 2740/3471, Loss: 8.8958\n  M√©dia parcial at√© aqui: 8.4600\nBatch 2741/3471, Loss: 9.4601\nBatch 2742/3471, Loss: 8.6640\nBatch 2743/3471, Loss: 8.6154\nBatch 2744/3471, Loss: 8.6851\nBatch 2745/3471, Loss: 7.4982\nBatch 2746/3471, Loss: 8.6493\nBatch 2747/3471, Loss: 7.4840\nBatch 2748/3471, Loss: 7.8070\nBatch 2749/3471, Loss: 7.8541\nBatch 2750/3471, Loss: 8.8417\n  M√©dia parcial at√© aqui: 8.4596\nBatch 2751/3471, Loss: 9.4302\nBatch 2752/3471, Loss: 8.7348\nBatch 2753/3471, Loss: 8.7734\nBatch 2754/3471, Loss: 6.4993\nBatch 2755/3471, Loss: 7.5811\nBatch 2756/3471, Loss: 8.3857\nBatch 2757/3471, Loss: 8.5493\nBatch 2758/3471, Loss: 9.6681\nBatch 2759/3471, Loss: 7.9472\nBatch 2760/3471, Loss: 7.6256\n  M√©dia parcial at√© aqui: 8.4591\nBatch 2761/3471, Loss: 8.6411\nBatch 2762/3471, Loss: 8.2879\nBatch 2763/3471, Loss: 7.6766\nBatch 2764/3471, Loss: 7.5450\nBatch 2765/3471, Loss: 10.0080\nBatch 2766/3471, Loss: 8.5166\nBatch 2767/3471, Loss: 9.3695\nBatch 2768/3471, Loss: 9.4634\nBatch 2769/3471, Loss: 7.3385\nBatch 2770/3471, Loss: 8.4921\n  M√©dia parcial at√© aqui: 8.4594\nBatch 2771/3471, Loss: 7.5607\nBatch 2772/3471, Loss: 9.3681\nBatch 2773/3471, Loss: 9.8745\nBatch 2774/3471, Loss: 7.3748\nBatch 2775/3471, Loss: 8.8760\nBatch 2776/3471, Loss: 8.5575\nBatch 2777/3471, Loss: 7.7250\nBatch 2778/3471, Loss: 7.8122\nBatch 2779/3471, Loss: 9.3666\nBatch 2780/3471, Loss: 9.4772\n  M√©dia parcial at√© aqui: 8.4599\nBatch 2781/3471, Loss: 7.1504\nBatch 2782/3471, Loss: 8.3196\nBatch 2783/3471, Loss: 8.5781\nBatch 2784/3471, Loss: 6.5153\nBatch 2785/3471, Loss: 7.8256\nBatch 2786/3471, Loss: 8.6683\nBatch 2787/3471, Loss: 8.9186\nBatch 2788/3471, Loss: 10.1814\nBatch 2789/3471, Loss: 9.2229\nBatch 2790/3471, Loss: 8.4012\n  M√©dia parcial at√© aqui: 8.4596\nBatch 2791/3471, Loss: 8.4304\nBatch 2792/3471, Loss: 9.2861\nBatch 2793/3471, Loss: 8.5103\nBatch 2794/3471, Loss: 7.6815\nBatch 2795/3471, Loss: 9.2905\nBatch 2796/3471, Loss: 7.4741\nBatch 2797/3471, Loss: 8.2858\nBatch 2798/3471, Loss: 9.3751\nBatch 2799/3471, Loss: 7.4831\nBatch 2800/3471, Loss: 9.6699\n  M√©dia parcial at√© aqui: 8.4599\nBatch 2801/3471, Loss: 8.2890\nBatch 2802/3471, Loss: 9.5043\nBatch 2803/3471, Loss: 5.5309\nBatch 2804/3471, Loss: 7.3675\nBatch 2805/3471, Loss: 6.9536\nBatch 2806/3471, Loss: 10.3917\nBatch 2807/3471, Loss: 9.3334\nBatch 2808/3471, Loss: 9.2791\nBatch 2809/3471, Loss: 8.6126\nBatch 2810/3471, Loss: 7.1015\n  M√©dia parcial at√© aqui: 8.4591\nBatch 2811/3471, Loss: 8.5686\nBatch 2812/3471, Loss: 5.7692\nBatch 2813/3471, Loss: 7.6992\nBatch 2814/3471, Loss: 9.6580\nBatch 2815/3471, Loss: 9.5088\nBatch 2816/3471, Loss: 8.5446\nBatch 2817/3471, Loss: 9.5156\nBatch 2818/3471, Loss: 9.7324\nBatch 2819/3471, Loss: 5.7098\nBatch 2820/3471, Loss: 8.9215\n  M√©dia parcial at√© aqui: 8.4588\nBatch 2821/3471, Loss: 9.7740\nBatch 2822/3471, Loss: 8.8069\nBatch 2823/3471, Loss: 8.4780\nBatch 2824/3471, Loss: 6.8685\nBatch 2825/3471, Loss: 8.6160\nBatch 2826/3471, Loss: 9.4410\nBatch 2827/3471, Loss: 8.7630\nBatch 2828/3471, Loss: 9.8421\nBatch 2829/3471, Loss: 8.6009\nBatch 2830/3471, Loss: 9.1240\n  M√©dia parcial at√© aqui: 8.4601\nBatch 2831/3471, Loss: 9.6400\nBatch 2832/3471, Loss: 7.4295\nBatch 2833/3471, Loss: 9.2569\nBatch 2834/3471, Loss: 8.0554\nBatch 2835/3471, Loss: 7.4827\nBatch 2836/3471, Loss: 7.5058\nBatch 2837/3471, Loss: 8.5503\nBatch 2838/3471, Loss: 7.8904\nBatch 2839/3471, Loss: 7.6117\nBatch 2840/3471, Loss: 8.5866\n  M√©dia parcial at√© aqui: 8.4592\nBatch 2841/3471, Loss: 9.3980\nBatch 2842/3471, Loss: 7.3265\nBatch 2843/3471, Loss: 9.7794\nBatch 2844/3471, Loss: 7.6485\nBatch 2845/3471, Loss: 8.9037\nBatch 2846/3471, Loss: 8.2950\nBatch 2847/3471, Loss: 7.9457\nBatch 2848/3471, Loss: 7.8694\nBatch 2849/3471, Loss: 9.5955\nBatch 2850/3471, Loss: 9.9791\n  M√©dia parcial at√© aqui: 8.4599\nBatch 2851/3471, Loss: 7.9634\nBatch 2852/3471, Loss: 8.7849\nBatch 2853/3471, Loss: 8.0824\nBatch 2854/3471, Loss: 7.5012\nBatch 2855/3471, Loss: 8.2418\nBatch 2856/3471, Loss: 8.5097\nBatch 2857/3471, Loss: 9.3229\nBatch 2858/3471, Loss: 9.0236\nBatch 2859/3471, Loss: 8.1949\nBatch 2860/3471, Loss: 7.6591\n  M√©dia parcial at√© aqui: 8.4595\nBatch 2861/3471, Loss: 6.9704\nBatch 2862/3471, Loss: 7.6086\nBatch 2863/3471, Loss: 5.8520\nBatch 2864/3471, Loss: 8.2875\nBatch 2865/3471, Loss: 5.1451\nBatch 2866/3471, Loss: 8.0125\nBatch 2867/3471, Loss: 9.7294\nBatch 2868/3471, Loss: 9.7426\nBatch 2869/3471, Loss: 5.7324\nBatch 2870/3471, Loss: 8.3960\n  M√©dia parcial at√© aqui: 8.4563\nBatch 2871/3471, Loss: 9.6327\nBatch 2872/3471, Loss: 9.1480\nBatch 2873/3471, Loss: 9.3302\nBatch 2874/3471, Loss: 9.6493\nBatch 2875/3471, Loss: 10.0352\nBatch 2876/3471, Loss: 8.7637\nBatch 2877/3471, Loss: 8.0652\nBatch 2878/3471, Loss: 7.8250\nBatch 2879/3471, Loss: 8.8061\nBatch 2880/3471, Loss: 9.6435\n  M√©dia parcial at√© aqui: 8.4585\nBatch 2881/3471, Loss: 9.8789\nBatch 2882/3471, Loss: 9.5306\nBatch 2883/3471, Loss: 8.6294\nBatch 2884/3471, Loss: 9.4765\nBatch 2885/3471, Loss: 9.6193\nBatch 2886/3471, Loss: 8.5521\nBatch 2887/3471, Loss: 8.4425\nBatch 2888/3471, Loss: 9.6613\nBatch 2889/3471, Loss: 8.6292\nBatch 2890/3471, Loss: 8.5836\n  M√©dia parcial at√© aqui: 8.4607\nBatch 2891/3471, Loss: 8.4637\nBatch 2892/3471, Loss: 8.3648\nBatch 2893/3471, Loss: 7.0475\nBatch 2894/3471, Loss: 6.1419\nBatch 2895/3471, Loss: 8.6474\nBatch 2896/3471, Loss: 9.4062\nBatch 2897/3471, Loss: 7.7668\nBatch 2898/3471, Loss: 8.1323\nBatch 2899/3471, Loss: 7.3042\nBatch 2900/3471, Loss: 8.7951\n  M√©dia parcial at√© aqui: 8.4592\nBatch 2901/3471, Loss: 9.8484\nBatch 2902/3471, Loss: 8.6797\nBatch 2903/3471, Loss: 9.2668\nBatch 2904/3471, Loss: 9.2237\nBatch 2905/3471, Loss: 7.3822\nBatch 2906/3471, Loss: 8.6106\nBatch 2907/3471, Loss: 9.2384\nBatch 2908/3471, Loss: 8.4709\nBatch 2909/3471, Loss: 8.2754\nBatch 2910/3471, Loss: 9.1306\n  M√©dia parcial at√© aqui: 8.4604\nBatch 2911/3471, Loss: 8.9854\nBatch 2912/3471, Loss: 8.2542\nBatch 2913/3471, Loss: 7.5610\nBatch 2914/3471, Loss: 7.7884\nBatch 2915/3471, Loss: 7.4629\nBatch 2916/3471, Loss: 8.7506\nBatch 2917/3471, Loss: 9.5474\nBatch 2918/3471, Loss: 9.7449\nBatch 2919/3471, Loss: 8.3258\nBatch 2920/3471, Loss: 8.2852\n  M√©dia parcial at√© aqui: 8.4604\nBatch 2921/3471, Loss: 8.6448\nBatch 2922/3471, Loss: 8.4491\nBatch 2923/3471, Loss: 9.7481\nBatch 2924/3471, Loss: 5.7059\nBatch 2925/3471, Loss: 7.6955\nBatch 2926/3471, Loss: 7.2983\nBatch 2927/3471, Loss: 9.7253\nBatch 2928/3471, Loss: 6.5966\nBatch 2929/3471, Loss: 9.9834\nBatch 2930/3471, Loss: 7.7377\n  M√©dia parcial at√© aqui: 8.4594\nBatch 2931/3471, Loss: 7.5782\nBatch 2932/3471, Loss: 8.1665\nBatch 2933/3471, Loss: 9.3625\nBatch 2934/3471, Loss: 7.5076\nBatch 2935/3471, Loss: 8.7612\nBatch 2936/3471, Loss: 8.8543\nBatch 2937/3471, Loss: 7.7712\nBatch 2938/3471, Loss: 8.8750\nBatch 2939/3471, Loss: 6.9085\nBatch 2940/3471, Loss: 9.2433\n  M√©dia parcial at√© aqui: 8.4588\nBatch 2941/3471, Loss: 8.4835\nBatch 2942/3471, Loss: 8.3897\nBatch 2943/3471, Loss: 6.6499\nBatch 2944/3471, Loss: 6.7362\nBatch 2945/3471, Loss: 6.0825\nBatch 2946/3471, Loss: 9.6604\nBatch 2947/3471, Loss: 10.0765\nBatch 2948/3471, Loss: 8.0981\nBatch 2949/3471, Loss: 7.6572\nBatch 2950/3471, Loss: 7.8048\n  M√©dia parcial at√© aqui: 8.4572\nBatch 2951/3471, Loss: 5.9105\nBatch 2952/3471, Loss: 9.7633\nBatch 2953/3471, Loss: 8.6987\nBatch 2954/3471, Loss: 9.5856\nBatch 2955/3471, Loss: 5.8512\nBatch 2956/3471, Loss: 9.2833\nBatch 2957/3471, Loss: 9.0157\nBatch 2958/3471, Loss: 8.2611\nBatch 2959/3471, Loss: 7.2586\nBatch 2960/3471, Loss: 8.4553\n  M√©dia parcial at√© aqui: 8.4563\nBatch 2961/3471, Loss: 6.9509\nBatch 2962/3471, Loss: 8.5190\nBatch 2963/3471, Loss: 8.3253\nBatch 2964/3471, Loss: 8.6232\nBatch 2965/3471, Loss: 7.3325\nBatch 2966/3471, Loss: 9.2234\nBatch 2967/3471, Loss: 9.0787\nBatch 2968/3471, Loss: 7.5840\nBatch 2969/3471, Loss: 8.7476\nBatch 2970/3471, Loss: 6.4899\n  M√©dia parcial at√© aqui: 8.4551\nBatch 2971/3471, Loss: 8.9957\nBatch 2972/3471, Loss: 9.5236\nBatch 2973/3471, Loss: 7.8651\nBatch 2974/3471, Loss: 8.6067\nBatch 2975/3471, Loss: 6.8411\nBatch 2976/3471, Loss: 9.4198\nBatch 2977/3471, Loss: 7.5752\nBatch 2978/3471, Loss: 7.8904\nBatch 2979/3471, Loss: 8.8103\nBatch 2980/3471, Loss: 7.5340\n  M√©dia parcial at√© aqui: 8.4546\nBatch 2981/3471, Loss: 8.8166\nBatch 2982/3471, Loss: 8.8550\nBatch 2983/3471, Loss: 7.3256\nBatch 2984/3471, Loss: 8.5599\nBatch 2985/3471, Loss: 7.3796\nBatch 2986/3471, Loss: 9.1173\nBatch 2987/3471, Loss: 8.0440\nBatch 2988/3471, Loss: 8.6967\nBatch 2989/3471, Loss: 8.3581\nBatch 2990/3471, Loss: 9.2418\n  M√©dia parcial at√© aqui: 8.4545\nBatch 2991/3471, Loss: 9.3859\nBatch 2992/3471, Loss: 9.8032\nBatch 2993/3471, Loss: 8.0793\nBatch 2994/3471, Loss: 7.7176\nBatch 2995/3471, Loss: 8.2909\nBatch 2996/3471, Loss: 8.7193\nBatch 2997/3471, Loss: 9.5656\nBatch 2998/3471, Loss: 9.3959\nBatch 2999/3471, Loss: 7.6430\nBatch 3000/3471, Loss: 9.0757\n  M√©dia parcial at√© aqui: 8.4556\nBatch 3001/3471, Loss: 7.6584\nBatch 3002/3471, Loss: 7.7796\nBatch 3003/3471, Loss: 6.9295\nBatch 3004/3471, Loss: 8.4682\nBatch 3005/3471, Loss: 8.7902\nBatch 3006/3471, Loss: 8.6760\nBatch 3007/3471, Loss: 7.9873\nBatch 3008/3471, Loss: 9.6704\nBatch 3009/3471, Loss: 10.2213\nBatch 3010/3471, Loss: 10.1739\n  M√©dia parcial at√© aqui: 8.4562\nBatch 3011/3471, Loss: 9.5261\nBatch 3012/3471, Loss: 8.6942\nBatch 3013/3471, Loss: 8.0389\nBatch 3014/3471, Loss: 8.7477\nBatch 3015/3471, Loss: 8.4392\nBatch 3016/3471, Loss: 8.8119\nBatch 3017/3471, Loss: 9.6109\nBatch 3018/3471, Loss: 6.0759\nBatch 3019/3471, Loss: 9.6230\nBatch 3020/3471, Loss: 9.3459\n  M√©dia parcial at√© aqui: 8.4570\nBatch 3021/3471, Loss: 7.8058\nBatch 3022/3471, Loss: 8.5596\nBatch 3023/3471, Loss: 7.5563\nBatch 3024/3471, Loss: 9.7671\nBatch 3025/3471, Loss: 9.3120\nBatch 3026/3471, Loss: 8.8351\nBatch 3027/3471, Loss: 9.4538\nBatch 3028/3471, Loss: 10.1426\nBatch 3029/3471, Loss: 7.4106\nBatch 3030/3471, Loss: 9.3564\n  M√©dia parcial at√© aqui: 8.4582\nBatch 3031/3471, Loss: 8.7463\nBatch 3032/3471, Loss: 8.6113\nBatch 3033/3471, Loss: 8.8771\nBatch 3034/3471, Loss: 8.3887\nBatch 3035/3471, Loss: 9.3694\nBatch 3036/3471, Loss: 8.6249\nBatch 3037/3471, Loss: 8.4774\nBatch 3038/3471, Loss: 8.5841\nBatch 3039/3471, Loss: 9.1634\nBatch 3040/3471, Loss: 6.6519\n  M√©dia parcial at√© aqui: 8.4585\nBatch 3041/3471, Loss: 8.3780\nBatch 3042/3471, Loss: 7.6093\nBatch 3043/3471, Loss: 8.5907\nBatch 3044/3471, Loss: 8.4892\nBatch 3045/3471, Loss: 7.7944\nBatch 3046/3471, Loss: 7.6405\nBatch 3047/3471, Loss: 7.6506\nBatch 3048/3471, Loss: 8.2205\nBatch 3049/3471, Loss: 6.6551\nBatch 3050/3471, Loss: 8.5149\n  M√©dia parcial at√© aqui: 8.4568\nBatch 3051/3471, Loss: 9.6918\nBatch 3052/3471, Loss: 9.7109\nBatch 3053/3471, Loss: 8.6449\nBatch 3054/3471, Loss: 8.6687\nBatch 3055/3471, Loss: 9.4907\nBatch 3056/3471, Loss: 8.6615\nBatch 3057/3471, Loss: 8.9807\nBatch 3058/3471, Loss: 8.7751\nBatch 3059/3471, Loss: 6.7865\nBatch 3060/3471, Loss: 8.2082\n  M√©dia parcial at√© aqui: 8.4578\nBatch 3061/3471, Loss: 9.4601\nBatch 3062/3471, Loss: 7.8684\nBatch 3063/3471, Loss: 8.3579\nBatch 3064/3471, Loss: 7.4236\nBatch 3065/3471, Loss: 7.5863\nBatch 3066/3471, Loss: 8.1748\nBatch 3067/3471, Loss: 9.9553\nBatch 3068/3471, Loss: 9.5720\nBatch 3069/3471, Loss: 6.8679\nBatch 3070/3471, Loss: 6.7685\n  M√©dia parcial at√© aqui: 8.4570\nBatch 3071/3471, Loss: 7.4490\nBatch 3072/3471, Loss: 8.7318\nBatch 3073/3471, Loss: 8.7997\nBatch 3074/3471, Loss: 9.2508\nBatch 3075/3471, Loss: 8.3154\nBatch 3076/3471, Loss: 8.7837\nBatch 3077/3471, Loss: 9.7969\nBatch 3078/3471, Loss: 9.5784\nBatch 3079/3471, Loss: 8.1098\nBatch 3080/3471, Loss: 9.2432\n  M√©dia parcial at√© aqui: 8.4581\nBatch 3081/3471, Loss: 8.5036\nBatch 3082/3471, Loss: 8.4633\nBatch 3083/3471, Loss: 8.7512\nBatch 3084/3471, Loss: 7.8703\nBatch 3085/3471, Loss: 8.8647\nBatch 3086/3471, Loss: 8.5588\nBatch 3087/3471, Loss: 7.5076\nBatch 3088/3471, Loss: 8.6740\nBatch 3089/3471, Loss: 8.6311\nBatch 3090/3471, Loss: 9.6625\n  M√©dia parcial at√© aqui: 8.4584\nBatch 3091/3471, Loss: 8.4926\nBatch 3092/3471, Loss: 8.5256\nBatch 3093/3471, Loss: 8.9434\nBatch 3094/3471, Loss: 7.4590\nBatch 3095/3471, Loss: 8.8720\nBatch 3096/3471, Loss: 6.5934\nBatch 3097/3471, Loss: 9.8380\nBatch 3098/3471, Loss: 7.8723\nBatch 3099/3471, Loss: 9.6698\nBatch 3100/3471, Loss: 9.5751\n  M√©dia parcial at√© aqui: 8.4588\nBatch 3101/3471, Loss: 8.4604\nBatch 3102/3471, Loss: 6.9265\nBatch 3103/3471, Loss: 8.5523\nBatch 3104/3471, Loss: 9.4803\nBatch 3105/3471, Loss: 7.7718\nBatch 3106/3471, Loss: 8.5227\nBatch 3107/3471, Loss: 8.1064\nBatch 3108/3471, Loss: 9.4532\nBatch 3109/3471, Loss: 8.3439\nBatch 3110/3471, Loss: 9.7419\n  M√©dia parcial at√© aqui: 8.4591\nBatch 3111/3471, Loss: 7.4464\nBatch 3112/3471, Loss: 8.7299\nBatch 3113/3471, Loss: 7.4659\nBatch 3114/3471, Loss: 7.5756\nBatch 3115/3471, Loss: 7.3787\nBatch 3116/3471, Loss: 8.5563\nBatch 3117/3471, Loss: 8.5789\nBatch 3118/3471, Loss: 9.6335\nBatch 3119/3471, Loss: 8.8013\nBatch 3120/3471, Loss: 6.5300\n  M√©dia parcial at√© aqui: 8.4578\nBatch 3121/3471, Loss: 7.5470\nBatch 3122/3471, Loss: 8.8570\nBatch 3123/3471, Loss: 8.1876\nBatch 3124/3471, Loss: 10.0384\nBatch 3125/3471, Loss: 7.8181\nBatch 3126/3471, Loss: 9.1128\nBatch 3127/3471, Loss: 9.6742\nBatch 3128/3471, Loss: 7.5896\nBatch 3129/3471, Loss: 7.9983\nBatch 3130/3471, Loss: 8.4077\n  M√©dia parcial at√© aqui: 8.4580\nBatch 3131/3471, Loss: 9.7844\nBatch 3132/3471, Loss: 8.3990\nBatch 3133/3471, Loss: 7.9681\nBatch 3134/3471, Loss: 6.5122\nBatch 3135/3471, Loss: 7.0436\nBatch 3136/3471, Loss: 8.7660\nBatch 3137/3471, Loss: 8.3140\nBatch 3138/3471, Loss: 7.2314\nBatch 3139/3471, Loss: 9.2110\nBatch 3140/3471, Loss: 8.6483\n  M√©dia parcial at√© aqui: 8.4572\nBatch 3141/3471, Loss: 9.1137\nBatch 3142/3471, Loss: 9.8891\nBatch 3143/3471, Loss: 8.5916\nBatch 3144/3471, Loss: 9.2950\nBatch 3145/3471, Loss: 7.2411\nBatch 3146/3471, Loss: 9.0243\nBatch 3147/3471, Loss: 8.6169\nBatch 3148/3471, Loss: 10.2164\nBatch 3149/3471, Loss: 7.2395\nBatch 3150/3471, Loss: 9.3204\n  M√©dia parcial at√© aqui: 8.4584\nBatch 3151/3471, Loss: 8.7825\nBatch 3152/3471, Loss: 8.4913\nBatch 3153/3471, Loss: 6.6628\nBatch 3154/3471, Loss: 8.9240\nBatch 3155/3471, Loss: 8.8800\nBatch 3156/3471, Loss: 7.6993\nBatch 3157/3471, Loss: 9.6016\nBatch 3158/3471, Loss: 8.7301\nBatch 3159/3471, Loss: 9.3817\nBatch 3160/3471, Loss: 8.4302\n  M√©dia parcial at√© aqui: 8.4587\nBatch 3161/3471, Loss: 7.6211\nBatch 3162/3471, Loss: 10.0849\nBatch 3163/3471, Loss: 6.8477\nBatch 3164/3471, Loss: 7.4852\nBatch 3165/3471, Loss: 9.6568\nBatch 3166/3471, Loss: 8.2502\nBatch 3167/3471, Loss: 7.0294\nBatch 3168/3471, Loss: 8.8594\nBatch 3169/3471, Loss: 7.7420\nBatch 3170/3471, Loss: 9.6705\n  M√©dia parcial at√© aqui: 8.4583\nBatch 3171/3471, Loss: 7.6571\nBatch 3172/3471, Loss: 8.4819\nBatch 3173/3471, Loss: 8.6819\nBatch 3174/3471, Loss: 8.4852\nBatch 3175/3471, Loss: 8.6103\nBatch 3176/3471, Loss: 10.0199\nBatch 3177/3471, Loss: 5.8216\nBatch 3178/3471, Loss: 9.3733\nBatch 3179/3471, Loss: 8.6531\nBatch 3180/3471, Loss: 8.5033\n  M√©dia parcial at√© aqui: 8.4582\nBatch 3181/3471, Loss: 6.4894\nBatch 3182/3471, Loss: 6.4039\nBatch 3183/3471, Loss: 9.7072\nBatch 3184/3471, Loss: 8.7890\nBatch 3185/3471, Loss: 8.8642\nBatch 3186/3471, Loss: 9.3951\nBatch 3187/3471, Loss: 7.1002\nBatch 3188/3471, Loss: 9.6471\nBatch 3189/3471, Loss: 7.6552\nBatch 3190/3471, Loss: 7.7838\n  M√©dia parcial at√© aqui: 8.4574\nBatch 3191/3471, Loss: 8.5891\nBatch 3192/3471, Loss: 8.5727\nBatch 3193/3471, Loss: 8.4711\nBatch 3194/3471, Loss: 9.2634\nBatch 3195/3471, Loss: 8.4173\nBatch 3196/3471, Loss: 10.0045\nBatch 3197/3471, Loss: 8.6166\nBatch 3198/3471, Loss: 8.4566\nBatch 3199/3471, Loss: 7.4401\nBatch 3200/3471, Loss: 8.4836\n  M√©dia parcial at√© aqui: 8.4579\nBatch 3201/3471, Loss: 9.5289\nBatch 3202/3471, Loss: 9.6767\nBatch 3203/3471, Loss: 8.0572\nBatch 3204/3471, Loss: 9.4786\nBatch 3205/3471, Loss: 9.2050\nBatch 3206/3471, Loss: 9.9605\nBatch 3207/3471, Loss: 9.6213\nBatch 3208/3471, Loss: 8.8839\nBatch 3209/3471, Loss: 9.0230\nBatch 3210/3471, Loss: 9.0543\n  M√©dia parcial at√© aqui: 8.4604\nBatch 3211/3471, Loss: 9.4312\nBatch 3212/3471, Loss: 8.3617\nBatch 3213/3471, Loss: 5.8559\nBatch 3214/3471, Loss: 9.7835\nBatch 3215/3471, Loss: 9.5592\nBatch 3216/3471, Loss: 8.6701\nBatch 3217/3471, Loss: 8.5160\nBatch 3218/3471, Loss: 9.8024\nBatch 3219/3471, Loss: 8.4532\nBatch 3220/3471, Loss: 5.5217\n  M√©dia parcial at√© aqui: 8.4602\nBatch 3221/3471, Loss: 6.8939\nBatch 3222/3471, Loss: 9.6158\nBatch 3223/3471, Loss: 8.9243\nBatch 3224/3471, Loss: 8.7704\nBatch 3225/3471, Loss: 8.3322\nBatch 3226/3471, Loss: 9.5684\nBatch 3227/3471, Loss: 8.5773\nBatch 3228/3471, Loss: 7.7870\nBatch 3229/3471, Loss: 9.4689\nBatch 3230/3471, Loss: 8.4953\n  M√©dia parcial at√© aqui: 8.4607\nBatch 3231/3471, Loss: 9.4030\nBatch 3232/3471, Loss: 9.7363\nBatch 3233/3471, Loss: 6.8558\nBatch 3234/3471, Loss: 8.4645\nBatch 3235/3471, Loss: 8.0065\nBatch 3236/3471, Loss: 7.7446\nBatch 3237/3471, Loss: 8.7917\nBatch 3238/3471, Loss: 8.7406\nBatch 3239/3471, Loss: 8.2045\nBatch 3240/3471, Loss: 10.1923\n  M√©dia parcial at√© aqui: 8.4612\nBatch 3241/3471, Loss: 6.7750\nBatch 3242/3471, Loss: 9.8405\nBatch 3243/3471, Loss: 8.6897\nBatch 3244/3471, Loss: 7.6201\nBatch 3245/3471, Loss: 9.1423\nBatch 3246/3471, Loss: 7.8147\nBatch 3247/3471, Loss: 7.8160\nBatch 3248/3471, Loss: 9.7479\nBatch 3249/3471, Loss: 8.4613\nBatch 3250/3471, Loss: 8.9275\n  M√©dia parcial at√© aqui: 8.4613\nBatch 3251/3471, Loss: 8.5778\nBatch 3252/3471, Loss: 8.0358\nBatch 3253/3471, Loss: 8.1629\nBatch 3254/3471, Loss: 9.6679\nBatch 3255/3471, Loss: 6.8627\nBatch 3256/3471, Loss: 8.6484\nBatch 3257/3471, Loss: 9.5557\nBatch 3258/3471, Loss: 9.4828\nBatch 3259/3471, Loss: 6.7342\nBatch 3260/3471, Loss: 9.8364\n  M√©dia parcial at√© aqui: 8.4616\nBatch 3261/3471, Loss: 8.4989\nBatch 3262/3471, Loss: 9.0450\nBatch 3263/3471, Loss: 8.7334\nBatch 3264/3471, Loss: 7.7077\nBatch 3265/3471, Loss: 9.8280\nBatch 3266/3471, Loss: 5.8063\nBatch 3267/3471, Loss: 7.7482\nBatch 3268/3471, Loss: 9.3423\nBatch 3269/3471, Loss: 7.4745\nBatch 3270/3471, Loss: 8.1983\n  M√©dia parcial at√© aqui: 8.4609\nBatch 3271/3471, Loss: 9.6325\nBatch 3272/3471, Loss: 7.6336\nBatch 3273/3471, Loss: 9.6075\nBatch 3274/3471, Loss: 8.3677\nBatch 3275/3471, Loss: 7.1611\nBatch 3276/3471, Loss: 8.9073\nBatch 3277/3471, Loss: 8.3277\nBatch 3278/3471, Loss: 9.9186\nBatch 3279/3471, Loss: 8.8011\nBatch 3280/3471, Loss: 9.5024\n  M√©dia parcial at√© aqui: 8.4619\nBatch 3281/3471, Loss: 8.5841\nBatch 3282/3471, Loss: 7.6937\nBatch 3283/3471, Loss: 9.6618\nBatch 3284/3471, Loss: 8.5961\nBatch 3285/3471, Loss: 7.2641\nBatch 3286/3471, Loss: 8.9118\nBatch 3287/3471, Loss: 7.6566\nBatch 3288/3471, Loss: 8.5865\nBatch 3289/3471, Loss: 8.6761\nBatch 3290/3471, Loss: 7.4649\n  M√©dia parcial at√© aqui: 8.4614\nBatch 3291/3471, Loss: 6.6726\nBatch 3292/3471, Loss: 8.4767\nBatch 3293/3471, Loss: 7.7671\nBatch 3294/3471, Loss: 8.0047\nBatch 3295/3471, Loss: 9.6272\nBatch 3296/3471, Loss: 7.6340\nBatch 3297/3471, Loss: 8.7235\nBatch 3298/3471, Loss: 8.0236\nBatch 3299/3471, Loss: 6.4903\nBatch 3300/3471, Loss: 9.1470\n  M√©dia parcial at√© aqui: 8.4602\nBatch 3301/3471, Loss: 8.5895\nBatch 3302/3471, Loss: 7.8208\nBatch 3303/3471, Loss: 6.9671\nBatch 3304/3471, Loss: 8.1845\nBatch 3305/3471, Loss: 8.9753\nBatch 3306/3471, Loss: 9.3234\nBatch 3307/3471, Loss: 8.9506\nBatch 3308/3471, Loss: 7.7377\nBatch 3309/3471, Loss: 8.5044\nBatch 3310/3471, Loss: 9.6443\n  M√©dia parcial at√© aqui: 8.4602\nBatch 3311/3471, Loss: 6.4372\nBatch 3312/3471, Loss: 7.6039\nBatch 3313/3471, Loss: 6.7294\nBatch 3314/3471, Loss: 10.3090\nBatch 3315/3471, Loss: 9.8291\nBatch 3316/3471, Loss: 7.7626\nBatch 3317/3471, Loss: 7.7578\nBatch 3318/3471, Loss: 8.6132\nBatch 3319/3471, Loss: 9.8598\nBatch 3320/3471, Loss: 8.0059\n  M√©dia parcial at√© aqui: 8.4597\nBatch 3321/3471, Loss: 8.6833\nBatch 3322/3471, Loss: 8.5942\nBatch 3323/3471, Loss: 8.8010\nBatch 3324/3471, Loss: 9.2430\nBatch 3325/3471, Loss: 7.4408\nBatch 3326/3471, Loss: 8.0478\nBatch 3327/3471, Loss: 8.6890\nBatch 3328/3471, Loss: 9.5176\nBatch 3329/3471, Loss: 9.2305\nBatch 3330/3471, Loss: 6.7583\n  M√©dia parcial at√© aqui: 8.4598\nBatch 3331/3471, Loss: 8.4151\nBatch 3332/3471, Loss: 7.7828\nBatch 3333/3471, Loss: 8.4634\nBatch 3334/3471, Loss: 6.9942\nBatch 3335/3471, Loss: 8.5010\nBatch 3336/3471, Loss: 9.2290\nBatch 3337/3471, Loss: 7.6703\nBatch 3338/3471, Loss: 7.3765\nBatch 3339/3471, Loss: 8.4924\nBatch 3340/3471, Loss: 6.5373\n  M√©dia parcial at√© aqui: 8.4583\nBatch 3341/3471, Loss: 9.8667\nBatch 3342/3471, Loss: 7.7548\nBatch 3343/3471, Loss: 10.1016\nBatch 3344/3471, Loss: 8.6469\nBatch 3345/3471, Loss: 9.8638\nBatch 3346/3471, Loss: 9.6875\nBatch 3347/3471, Loss: 7.2794\nBatch 3348/3471, Loss: 8.4187\nBatch 3349/3471, Loss: 8.6605\nBatch 3350/3471, Loss: 8.4898\n  M√©dia parcial at√© aqui: 8.4595\nBatch 3351/3471, Loss: 8.8909\nBatch 3352/3471, Loss: 9.6358\nBatch 3353/3471, Loss: 9.4245\nBatch 3354/3471, Loss: 8.4899\nBatch 3355/3471, Loss: 6.7190\nBatch 3356/3471, Loss: 9.4364\nBatch 3357/3471, Loss: 8.3534\nBatch 3358/3471, Loss: 6.5252\nBatch 3359/3471, Loss: 8.7781\nBatch 3360/3471, Loss: 6.5475\n  M√©dia parcial at√© aqui: 8.4590\nBatch 3361/3471, Loss: 8.5488\nBatch 3362/3471, Loss: 8.6788\nBatch 3363/3471, Loss: 9.5868\nBatch 3364/3471, Loss: 8.9859\nBatch 3365/3471, Loss: 7.0974\nBatch 3366/3471, Loss: 8.1982\nBatch 3367/3471, Loss: 8.8442\nBatch 3368/3471, Loss: 7.4174\nBatch 3369/3471, Loss: 8.6060\nBatch 3370/3471, Loss: 6.8269\n  M√©dia parcial at√© aqui: 8.4585\nBatch 3371/3471, Loss: 8.5224\nBatch 3372/3471, Loss: 10.0794\nBatch 3373/3471, Loss: 7.6477\nBatch 3374/3471, Loss: 8.7353\nBatch 3375/3471, Loss: 7.3213\nBatch 3376/3471, Loss: 9.3011\nBatch 3377/3471, Loss: 8.0216\nBatch 3378/3471, Loss: 7.6940\nBatch 3379/3471, Loss: 10.0357\nBatch 3380/3471, Loss: 9.2358\n  M√©dia parcial at√© aqui: 8.4591\nBatch 3381/3471, Loss: 7.5428\nBatch 3382/3471, Loss: 7.4713\nBatch 3383/3471, Loss: 9.3145\nBatch 3384/3471, Loss: 8.2729\nBatch 3385/3471, Loss: 5.3910\nBatch 3386/3471, Loss: 7.9511\nBatch 3387/3471, Loss: 8.5101\nBatch 3388/3471, Loss: 9.6588\nBatch 3389/3471, Loss: 9.7946\nBatch 3390/3471, Loss: 8.5190\n  M√©dia parcial at√© aqui: 8.4584\nBatch 3391/3471, Loss: 7.7110\nBatch 3392/3471, Loss: 8.3670\nBatch 3393/3471, Loss: 9.3450\nBatch 3394/3471, Loss: 6.3020\nBatch 3395/3471, Loss: 7.5136\nBatch 3396/3471, Loss: 9.6424\nBatch 3397/3471, Loss: 6.7122\nBatch 3398/3471, Loss: 7.2985\nBatch 3399/3471, Loss: 8.6718\nBatch 3400/3471, Loss: 9.0829\n  M√©dia parcial at√© aqui: 8.4573\nBatch 3401/3471, Loss: 8.6266\nBatch 3402/3471, Loss: 7.2932\nBatch 3403/3471, Loss: 8.9325\nBatch 3404/3471, Loss: 9.0908\nBatch 3405/3471, Loss: 8.6313\nBatch 3406/3471, Loss: 8.6639\nBatch 3407/3471, Loss: 7.7586\nBatch 3408/3471, Loss: 8.9280\nBatch 3409/3471, Loss: 8.9426\nBatch 3410/3471, Loss: 8.7923\n  M√©dia parcial at√© aqui: 8.4576\nBatch 3411/3471, Loss: 9.1078\nBatch 3412/3471, Loss: 7.9570\nBatch 3413/3471, Loss: 8.7202\nBatch 3414/3471, Loss: 7.8624\nBatch 3415/3471, Loss: 9.9732\nBatch 3416/3471, Loss: 7.5184\nBatch 3417/3471, Loss: 9.4793\nBatch 3418/3471, Loss: 9.7131\nBatch 3419/3471, Loss: 7.5023\nBatch 3420/3471, Loss: 9.7091\n  M√©dia parcial at√© aqui: 8.4584\nBatch 3421/3471, Loss: 9.6385\nBatch 3422/3471, Loss: 9.3188\nBatch 3423/3471, Loss: 8.3855\nBatch 3424/3471, Loss: 8.4688\nBatch 3425/3471, Loss: 8.5704\nBatch 3426/3471, Loss: 8.5047\nBatch 3427/3471, Loss: 8.5452\nBatch 3428/3471, Loss: 9.8860\nBatch 3429/3471, Loss: 7.5355\nBatch 3430/3471, Loss: 9.4361\n  M√©dia parcial at√© aqui: 8.4595\nBatch 3431/3471, Loss: 9.6538\nBatch 3432/3471, Loss: 8.7850\nBatch 3433/3471, Loss: 6.8478\nBatch 3434/3471, Loss: 9.5020\nBatch 3435/3471, Loss: 6.5661\nBatch 3436/3471, Loss: 9.5866\nBatch 3437/3471, Loss: 7.8488\nBatch 3438/3471, Loss: 8.9109\nBatch 3439/3471, Loss: 9.1702\nBatch 3440/3471, Loss: 7.7404\n  M√©dia parcial at√© aqui: 8.4595\nBatch 3441/3471, Loss: 9.3284\nBatch 3442/3471, Loss: 6.4690\nBatch 3443/3471, Loss: 8.3959\nBatch 3444/3471, Loss: 8.9225\nBatch 3445/3471, Loss: 7.7905\nBatch 3446/3471, Loss: 9.2746\nBatch 3447/3471, Loss: 6.7916\nBatch 3448/3471, Loss: 8.5562\nBatch 3449/3471, Loss: 8.6645\nBatch 3450/3471, Loss: 10.0343\n  M√©dia parcial at√© aqui: 8.4594\nBatch 3451/3471, Loss: 9.5517\nBatch 3452/3471, Loss: 9.7184\nBatch 3453/3471, Loss: 7.3592\nBatch 3454/3471, Loss: 8.7711\nBatch 3455/3471, Loss: 7.7106\nBatch 3456/3471, Loss: 8.5660\nBatch 3457/3471, Loss: 9.6273\nBatch 3458/3471, Loss: 6.4961\nBatch 3459/3471, Loss: 8.8500\nBatch 3460/3471, Loss: 8.4352\n  M√©dia parcial at√© aqui: 8.4596\nBatch 3461/3471, Loss: 9.7250\nBatch 3462/3471, Loss: 9.6703\nBatch 3463/3471, Loss: 8.0999\nBatch 3464/3471, Loss: 7.3674\nBatch 3465/3471, Loss: 7.6096\nBatch 3466/3471, Loss: 9.2739\nBatch 3467/3471, Loss: 9.5947\nBatch 3468/3471, Loss: 6.6579\nBatch 3469/3471, Loss: 8.5872\nBatch 3470/3471, Loss: 7.9503\n  M√©dia parcial at√© aqui: 8.4596\nBatch 3471/3471, Loss: 7.6587\nLoss m√©dia final no dataset de teste: 8.4593\nPerplexidade calculada: 4718.85\n","output_type":"stream"}],"execution_count":52},{"cell_type":"markdown","source":"### Gr√°ficos do WanDB (Weights & Biases)","metadata":{}},{"cell_type":"markdown","source":"### Prompts","metadata":{}},{"cell_type":"code","source":"generate_and_print_sample(model, tokenizer, device=device, start_context=\"nem tudo o que dura dura muito tempo\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T08:36:39.115168Z","iopub.execute_input":"2025-09-03T08:36:39.115487Z","iopub.status.idle":"2025-09-03T08:36:39.485917Z","shell.execute_reply.started":"2025-09-03T08:36:39.115464Z","shell.execute_reply":"2025-09-03T08:36:39.484857Z"}},"outputs":[{"name":"stdout","text":"nem tudo o que dura dura muito tempo. Esta segunda parte n√£o acha mais que o mesmo vento de que o mesmo vento de que √© feito, e a morte.  --Vem commigo, n√£o creio que o melhor √© que lhe deu... Voc√™ j√° reparou me convem\n","output_type":"stream"}],"execution_count":87},{"cell_type":"code","source":"generate_and_print_sample(model, tokenizer, device=device, start_context=\"Crie um poema\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T08:40:01.619483Z","iopub.execute_input":"2025-09-03T08:40:01.620050Z","iopub.status.idle":"2025-09-03T08:40:01.987930Z","shell.execute_reply.started":"2025-09-03T08:40:01.620027Z","shell.execute_reply":"2025-09-03T08:40:01.987304Z"}},"outputs":[{"name":"stdout","text":"Crie um poema e outro poema.  Chegai, folgai, cantai. √â esta, √© esta De Lindoya, que a voz suave e forte Do vate celebrou, a alegre festa.  Al√©m do amavel, gracioso porte\n","output_type":"stream"}],"execution_count":94},{"cell_type":"code","source":"generate_and_print_sample(model, tokenizer, device=device, start_context=\"lembran√ßa da juventude\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T08:28:28.836675Z","iopub.execute_input":"2025-09-03T08:28:28.837457Z","iopub.status.idle":"2025-09-03T08:28:29.209940Z","shell.execute_reply.started":"2025-09-03T08:28:28.837432Z","shell.execute_reply":"2025-09-03T08:28:29.209313Z"}},"outputs":[{"name":"stdout","text":"lembran√ßa da juventude, mescla de Os esquecidos e terror dos proprietarios De amor, ao sol e a vida E √° solid√£o do gozo perdido Pelos robustico de constancia eterna; Ou quando, emfim, tua adorada\n","output_type":"stream"}],"execution_count":78},{"cell_type":"code","source":"generate_and_print_sample(model, tokenizer, device=device, start_context=\"Entre paix√£o e del√≠rio\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T08:29:32.921267Z","iopub.execute_input":"2025-09-03T08:29:32.921814Z","iopub.status.idle":"2025-09-03T08:29:33.281937Z","shell.execute_reply.started":"2025-09-03T08:29:32.921793Z","shell.execute_reply":"2025-09-03T08:29:33.281369Z"}},"outputs":[{"name":"stdout","text":"Entre paix√£o e del√≠rio, nem saindo de terra, ou ainda uma vez que n√£o digo. De caminho, mas por lhe parecer-me que a fiz no cerebro. N√£o me cedeu, a muitas as muitas que ficavam entre si. Ezequ\n","output_type":"stream"}],"execution_count":80},{"cell_type":"code","source":"generate_and_print_sample(model, tokenizer, device=device, start_context=\"campo de batatas e duas tribos famintas\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T08:30:25.035060Z","iopub.execute_input":"2025-09-03T08:30:25.035678Z","iopub.status.idle":"2025-09-03T08:30:25.403671Z","shell.execute_reply.started":"2025-09-03T08:30:25.035654Z","shell.execute_reply":"2025-09-03T08:30:25.402780Z"}},"outputs":[{"name":"stdout","text":"campo de batatas e duas tribos famintas na Caim-se vida publica. Ao cabo, o dia seguinte, o sol de um grande e da vida a espaldancia daquella occasi√£o tocava quando elle espreita t√£o escarninho de saudades e Capit\n","output_type":"stream"}],"execution_count":81},{"cell_type":"code","source":"generate_and_print_sample(model, tokenizer, device=device, start_context=\"chorei\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T08:35:39.212049Z","iopub.execute_input":"2025-09-03T08:35:39.212325Z","iopub.status.idle":"2025-09-03T08:35:39.571361Z","shell.execute_reply.started":"2025-09-03T08:35:39.212303Z","shell.execute_reply":"2025-09-03T08:35:39.570633Z"}},"outputs":[{"name":"stdout","text":"chorei muita vez as apparencias ruidos da outra vez a compara√ß√£o, os annos, com as que perdem, como os desfiaram um pouco turbilh√£o, que me enfeixou a alguem. Tudo isso que\n","output_type":"stream"}],"execution_count":86},{"cell_type":"markdown","source":"## Tabela Comparativa (Baseline e Variantes)\n","metadata":{"id":"1Y2ADuKYSAbu"}},{"cell_type":"code","source":"elapsed = end_time - start_time\ntokens_per_sec = tokens_processed / elapsed\nmax_memory = torch.cuda.max_memory_allocated(device) / (1024**2)  # MB\n\nprint(\"\\nDESEMPENHO:\")\nprint(f\"Tempo total: {elapsed:.2f} s\")\nprint(f\"Tokens/s: {tokens_per_sec:.2f}\")\nprint(f\"Mem√≥ria m√°xima: {max_memory:.2f} MB\")\n\nprint(f\"\\nTempo total de treino: {total_train_time:.2f} s ({total_train_time/60:.2f} min)\")\nprint(f\"\\nPerplexidade: {perplexity_test}\")\n\nmodelos = [\"Original\\n(GPT-2)\"]\nperplexidade = [perplexity_test]\ntokens_s = [tokens_per_sec]\nmemoria = [max_memory]\n\nx = np.arange(len(modelos))\nlargura = 0.25\n\nfig, ax = plt.subplots(figsize=(8,5))\n\nax.bar(x - largura, perplexidade, largura, label=\"Perplexidade (‚Üì)\", color=\"#4C72B0\")\nax.bar(x, tokens_s, largura, label=\"Tokens/s (‚Üë)\", color=\"#55A868\")\nax.bar(x + largura, memoria, largura, label=\"Mem√≥ria pico (MB)\", color=\"#C44E52\")\n\nax.set_xlabel(\"**Modelos**\")\nax.set_ylabel(\"**Valores**\")\nax.set_title(\"Compara√ß√£o de desempenho: Baseline vs. Variantes\")\nax.set_xticks(x)\nax.set_xticklabels(modelos)\n\nax.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0.)\n\nplt.tight_layout()\nplt.show()","metadata":{"id":"8UkuBdyHSJG4","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T08:13:25.933478Z","iopub.execute_input":"2025-09-03T08:13:25.934093Z","iopub.status.idle":"2025-09-03T08:13:26.086729Z","shell.execute_reply.started":"2025-09-03T08:13:25.934070Z","shell.execute_reply":"2025-09-03T08:13:26.085946Z"}},"outputs":[{"name":"stdout","text":"\nDESEMPENHO:\nTempo total: 18131.60 s\nTokens/s: 3969.73\nMem√≥ria m√°xima: 11157.89 MB\n\nTempo total de treino: 18095.66 s (301.59 min)\n\nPerplexidade: 4718.85302734375\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x500 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":72},{"cell_type":"markdown","source":"# **Conclus√µes üé®**","metadata":{"id":"-LvvBQ80v9LL"}},{"cell_type":"markdown","source":"O experimento de treinar um GPT-2 do zero com aproximadamente 128 milh√µes de par√¢metros exclusivamente sobre sete obras de Machado de Assis revelou resultados interessantes, mas tamb√©m limita√ß√µes claras. O modelo mostrou capacidade de absorver o vocabul√°rio caracter√≠stico do autor, reproduzindo palavras arcaicas e pouco usuais, al√©m de imitar estruturas longas, com pontua√ß√£o t√≠pica da √©poca e at√© ensaiar produ√ß√µes po√©ticas quando solicitado. Isso indica que ele de fato internalizou parte do estilo liter√°rio de Machado, capturando seu l√©xico e ritmo textual. No entanto, a perplexidade extremamente alta e a presen√ßa de repeti√ß√µes, incoer√™ncias e frases quebradas revelam que o aprendizado foi superficial e insuficiente para garantir coer√™ncia narrativa ou estabilidade sint√°tica.\n\nEsses resultados decorrem de alguns fatores principais: o corpus restrito (apenas sete livros, o que limita a diversidade de tokens e contextos), o tamanho expressivo do modelo, que exige bilh√µes de tokens para convergir de forma adequada, e o n√∫mero reduzido de √©pocas, j√° que foi realizado apenas um ciclo de treino devido ao tempo elevado de processamento. De fato, com uma GPU P100 de 16 GB, o treinamento levou cerca de cinco horas para pouco mais de 28 mil batches para uma √©poca, o que inviabiliza explorar dezenas de √©pocas sem comprometer os recursos dispon√≠veis.\n\nEm s√≠ntese, o modelo aprendeu a ‚Äúfalar como Machado de Assis‚Äù, reproduzindo seu estilo, mas n√£o conseguiu estruturar narrativas longas ou desenvolver ideias de forma consistente. Isso confirma que treinar do zero, com dados t√£o limitados e recursos computacionais modestos, resulta em um modelo estil√≠stico, mas com pouco poder de generaliza√ß√£o. Para atingir perplexidades mais baixas e qualidade textual superior, seria necess√°rio ampliar massivamente o corpus, aumentar o n√∫mero de √©pocas ou adotar a estrat√©gia mais eficiente de realizar fine-tuning em um GPT-2 j√° pr√©-treinado, de forma a preservar o estilo machadiano sem perder coer√™ncia e riqueza lingu√≠stica.","metadata":{}}]}